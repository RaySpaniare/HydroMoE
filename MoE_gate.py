"""
MoEé—¨æ§è·¯ç”±å™¨ - ä¸“å®¶ç½‘ç»œé€‰æ‹©å’Œè´Ÿè½½å‡è¡¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
import math


class MoEGate(nn.Module):
    """MoEé—¨æ§ç½‘ç»œ - è·¯ç”±è¾“å…¥åˆ°ä¸åŒçš„ä¸“å®¶ç½‘ç»œ"""
    
    def __init__(self,
                 input_dim: int,
                 num_experts: int,
                 top_k: int = 2,
                 capacity_factor: float = 1.25,
                 dropout: float = 0.1,
                 noisy_gating: bool = True,
                 noise_epsilon: float = 1e-2):
        super().__init__()
        
        self.input_dim = input_dim
        self.num_experts = num_experts
        self.top_k = min(top_k, num_experts)  # ç¡®ä¿top_kä¸è¶…è¿‡ä¸“å®¶æ•°é‡
        self.capacity_factor = capacity_factor
        self.noisy_gating = noisy_gating
        self.noise_epsilon = noise_epsilon
        
        # é—¨æ§ç½‘ç»œ
        self.gate = nn.Linear(input_dim, num_experts, bias=False)
        
        # å™ªå£°ç½‘ç»œï¼ˆç”¨äºæ¢ç´¢ï¼‰
        if noisy_gating:
            self.noise_gate = nn.Linear(input_dim, num_experts, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
        # è´Ÿè½½å‡è¡¡å‚æ•°
        self.register_buffer('expert_usage', torch.zeros(num_experts))
        self.register_buffer('total_tokens', torch.tensor(0.0))
        
        self._init_weights()
    
    def _init_weights(self):
        """ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–"""
        nn.init.xavier_uniform_(self.gate.weight, gain=0.1)
        if self.noisy_gating:
            nn.init.zeros_(self.noise_gate.weight)
    
    def forward(self, x: torch.Tensor, training: bool = True) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ - è®¡ç®—ä¸“å®¶é€‰æ‹©æ¦‚ç‡å’Œè·¯ç”±ä¿¡æ¯
        
        Args:
            x: [batch_size, seq_len, input_dim] æˆ– [batch_size, input_dim]
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        Returns:
            DictåŒ…å«:
                - gate_weights: [batch_size, num_experts] é—¨æ§æƒé‡
                - top_k_indices: [batch_size, top_k] é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
                - top_k_weights: [batch_size, top_k] å¯¹åº”çš„æƒé‡
                - load_balancing_loss: è´Ÿè½½å‡è¡¡æŸå¤±
                - capacity_info: å®¹é‡ä¿¡æ¯
        """
        # å¤„ç†è¾“å…¥ç»´åº¦
        original_shape = x.shape
        if len(x.shape) == 3:
            batch_size, seq_len, input_dim = x.shape
            x = x.view(-1, input_dim)  # [batch_size * seq_len, input_dim]
        else:
            batch_size, seq_len = x.shape[0], 1
        
        # 1. è®¡ç®—é—¨æ§åˆ†æ•°
        gate_logits = self.gate(x)  # [tokens, num_experts]
        
        # 2. æ·»åŠ å™ªå£°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.noisy_gating and training:
            noise_logits = self.noise_gate(x)
            noise = torch.randn_like(noise_logits) * F.softplus(noise_logits) * self.noise_epsilon
            gate_logits += noise
        
        # 3. è®¡ç®—é—¨æ§æƒé‡
        gate_weights = F.softmax(gate_logits, dim=-1)  # [tokens, num_experts]
        
        # 4. é€‰æ‹©top-kä¸“å®¶
        top_k_weights, top_k_indices = torch.topk(gate_weights, self.top_k, dim=-1)
        
        # 5. é‡æ–°å½’ä¸€åŒ–top-kæƒé‡
        top_k_weights = top_k_weights / (top_k_weights.sum(dim=-1, keepdim=True) + 1e-8)
        
        # 6. è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        load_balancing_loss = self._compute_load_balancing_loss(gate_weights)
        
        # 7. æ›´æ–°ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        if training:
            self._update_expert_usage(gate_weights)
        
        # 8. è®¡ç®—å®¹é‡ä¿¡æ¯
        capacity_info = self._compute_capacity_info(top_k_indices, gate_weights.shape[0])
        
        # æ¢å¤å½¢çŠ¶ä¿¡æ¯
        if len(original_shape) == 3:
            gate_weights = gate_weights.view(batch_size, seq_len, self.num_experts)
            top_k_indices = top_k_indices.view(batch_size, seq_len, self.top_k)
            top_k_weights = top_k_weights.view(batch_size, seq_len, self.top_k)
        
        return {
            'gate_weights': gate_weights,
            'top_k_indices': top_k_indices,
            'top_k_weights': top_k_weights,
            'load_balancing_loss': load_balancing_loss,
            'capacity_info': capacity_info,
            'expert_usage': self.expert_usage.clone()
        }
    
    def _compute_load_balancing_loss(self, gate_weights: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤± - ä¼˜åŒ–ç‰ˆæœ¬"""
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨æ›´ç®€å•çš„è®¡ç®—ï¼Œå‡å°‘æ“ä½œ
        expert_usage = gate_weights.mean(dim=0)  # [num_experts]
        
        # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥è®¡ç®—ä¸ç†æƒ³åˆ†å¸ƒçš„åå·®
        ideal_usage = 1.0 / self.num_experts
        load_balancing_loss = ((expert_usage - ideal_usage) ** 2).mean()
        
        return load_balancing_loss
    
    def _update_expert_usage(self, gate_weights: torch.Tensor):
        """æ›´æ–°ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡"""
        with torch.no_grad():
            current_usage = gate_weights.sum(dim=0)  # [num_experts]
            self.expert_usage = 0.99 * self.expert_usage + 0.01 * current_usage
            self.total_tokens += gate_weights.shape[0]
    
    def _compute_capacity_info(self, top_k_indices: torch.Tensor, num_tokens: int) -> Dict[str, float]:
        """è®¡ç®—å®¹é‡ä¿¡æ¯ç”¨äºç›‘æ§"""
        with torch.no_grad():
            # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„æ¬¡æ•°
            expert_counts = torch.zeros(self.num_experts, device=top_k_indices.device)
            for i in range(self.num_experts):
                expert_counts[i] = (top_k_indices == i).sum().float()
            
            # è®¡ç®—å®¹é‡åˆ©ç”¨ç‡
            capacity_per_expert = num_tokens * self.capacity_factor / self.num_experts
            capacity_utilization = expert_counts / capacity_per_expert
            
            return {
                'capacity_utilization_mean': capacity_utilization.mean().item(),
                'capacity_utilization_std': capacity_utilization.std().item(),
                'expert_usage_entropy': self._compute_entropy(expert_counts / expert_counts.sum()),
                'overflow_rate': (capacity_utilization > 1.0).float().mean().item()
            }
    
    def _compute_entropy(self, probs: torch.Tensor) -> float:
        """è®¡ç®—ç†µ"""
        probs = probs + 1e-8  # é¿å…log(0)
        entropy = -(probs * torch.log(probs)).sum()
        return entropy.item()


class ExpertDispatcher(nn.Module):
    """ä¸“å®¶åˆ†å‘å™¨ - å°†è¾“å…¥åˆ†å‘ç»™é€‰ä¸­çš„ä¸“å®¶"""
    
    def __init__(self, num_experts: int, capacity_factor: float = 1.25):
        super().__init__()
        self.num_experts = num_experts
        self.capacity_factor = capacity_factor
    
    def forward(self, 
                inputs: torch.Tensor,
                gate_info: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        åˆ†å‘è¾“å…¥åˆ°ä¸“å®¶
        
        Args:
            inputs: [batch_size, seq_len, input_dim]
            gate_info: é—¨æ§ä¿¡æ¯
            
        Returns:
            åˆ†å‘ç»“æœå­—å…¸
        """
        batch_size, seq_len, input_dim = inputs.shape
        top_k_indices = gate_info['top_k_indices']  # [batch_size, seq_len, top_k]
        top_k_weights = gate_info['top_k_weights']  # [batch_size, seq_len, top_k]
        
        # å±•å¹³è¾“å…¥ç”¨äºåˆ†å‘
        flat_inputs = inputs.view(-1, input_dim)  # [batch_size * seq_len, input_dim]
        flat_indices = top_k_indices.view(-1, top_k_indices.shape[-1])  # [batch_size * seq_len, top_k]
        flat_weights = top_k_weights.view(-1, top_k_weights.shape[-1])  # [batch_size * seq_len, top_k]
        
        # ä¸ºæ¯ä¸ªä¸“å®¶å‡†å¤‡è¾“å…¥
        expert_inputs = {}
        expert_weights = {}
        
        for expert_id in range(self.num_experts):
            # æ‰¾åˆ°åˆ†é…ç»™è¿™ä¸ªä¸“å®¶çš„æ‰€æœ‰token
            mask = (flat_indices == expert_id)  # [batch_size * seq_len, top_k]
            
            if mask.any():
                # è·å–tokenç´¢å¼•å’Œå¯¹åº”çš„æƒé‡
                token_indices, k_indices = torch.where(mask)
                selected_inputs = flat_inputs[token_indices]  # [num_selected, input_dim]
                selected_weights = flat_weights[token_indices, k_indices]  # [num_selected]
                
                expert_inputs[expert_id] = {
                    'inputs': selected_inputs,
                    'token_indices': token_indices,
                    'weights': selected_weights,
                    'original_shape': (batch_size, seq_len, input_dim)
                }
        
        return expert_inputs


class ExpertCombiner(nn.Module):
    """ä¸“å®¶è¾“å‡ºç»„åˆå™¨"""
    
    def __init__(self):
        super().__init__()
    
    def forward(self,
                expert_outputs: Dict[int, torch.Tensor],
                expert_inputs: Dict[int, Dict],
                original_shape: Tuple[int, int, int]) -> torch.Tensor:
        """
        ç»„åˆä¸“å®¶è¾“å‡º
        
        Args:
            expert_outputs: {expert_id: output_tensor}
            expert_inputs: ä¸“å®¶è¾“å…¥ä¿¡æ¯ï¼ˆåŒ…å«æƒé‡å’Œç´¢å¼•ï¼‰
            original_shape: åŸå§‹è¾“å…¥å½¢çŠ¶
            
        Returns:
            combined_output: [batch_size, seq_len, output_dim]
        """
        batch_size, seq_len, _ = original_shape
        output_dim = None
        
        # åˆå§‹åŒ–è¾“å‡ºå¼ é‡
        for expert_id, output in expert_outputs.items():
            if output_dim is None:
                output_dim = output.shape[-1]
                break
        
        if output_dim is None:
            raise ValueError("No expert outputs provided")
        
        # åˆ›å»ºè¾“å‡ºå¼ é‡
        flat_outputs = torch.zeros(batch_size * seq_len, output_dim, 
                                 device=next(iter(expert_outputs.values())).device,
                                 dtype=next(iter(expert_outputs.values())).dtype)
        
        # ç»„åˆä¸“å®¶è¾“å‡º
        for expert_id, output in expert_outputs.items():
            if expert_id in expert_inputs:
                info = expert_inputs[expert_id]
                token_indices = info['token_indices']
                weights = info['weights'].unsqueeze(-1)  # [num_tokens, 1]
                
                # åŠ æƒç´¯åŠ 
                weighted_output = output * weights
                flat_outputs.index_add_(0, token_indices, weighted_output)
        
        # æ¢å¤åŸå§‹å½¢çŠ¶
        combined_output = flat_outputs.view(batch_size, seq_len, output_dim)

        return combined_output


class ContextAwareMoEGate(nn.Module):
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥MoEé—¨æ§ - è€ƒè™‘å†å²ä¿¡æ¯å’Œæµé‡çŠ¶æ€"""

    def __init__(self,
                 input_dim: int,
                 num_experts: int,
                 top_k: int = 2,
                 context_window: int = 7,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°
                 dropout: float = 0.1):
        super().__init__()

        self.input_dim = input_dim
        self.num_experts = num_experts
        self.top_k = min(top_k, num_experts)
        self.context_window = context_window

        # ä¸Šä¸‹æ–‡ç¼–ç å™¨
        self.context_encoder = nn.LSTM(
            input_size=input_dim,
            hidden_size=input_dim // 2,
            num_layers=1,
            batch_first=True,
            dropout=dropout if context_window > 1 else 0
        )

        # æµé‡çŠ¶æ€æ£€æµ‹å™¨
        self.flow_detector = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, 4),  # 4ä¸ªæµé‡çº§åˆ«
            nn.Softmax(dim=-1)
        )

        # ä¸»é—¨æ§ç½‘ç»œ
        self.main_gate = nn.Sequential(
            nn.Linear(input_dim + input_dim // 2 + 4, input_dim),  # è¾“å…¥+ä¸Šä¸‹æ–‡+æµé‡çŠ¶æ€
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim, num_experts)
        )

        # ä¸“å®¶ç‰¹åŒ–æƒé‡
        self.expert_specialization = nn.Parameter(torch.randn(num_experts, 4))  # ä¸“å®¶å¯¹æµé‡çº§åˆ«çš„åå¥½

        self.dropout = nn.Dropout(dropout)
        self._init_weights()

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

        # ä¸“å®¶ç‰¹åŒ–æƒé‡åˆå§‹åŒ–
        nn.init.normal_(self.expert_specialization, mean=0, std=0.1)

    def forward(self, x: torch.Tensor, context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [batch_size, input_dim] å½“å‰è¾“å…¥
            context: [batch_size, context_window, input_dim] å†å²ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            é—¨æ§ç»“æœå­—å…¸
        """
        batch_size = x.shape[0]

        # 1. ä¸Šä¸‹æ–‡ç¼–ç 
        if context is not None and context.shape[1] > 0:
            context_encoded, _ = self.context_encoder(context)
            context_feature = context_encoded[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        else:
            context_feature = torch.zeros(batch_size, self.input_dim // 2, device=x.device)

        # 2. æµé‡çŠ¶æ€æ£€æµ‹
        flow_state = self.flow_detector(x)  # [batch_size, 4]

        # 3. ç‰¹å¾èåˆ
        combined_features = torch.cat([x, context_feature, flow_state], dim=-1)

        # 4. ä¸»é—¨æ§è®¡ç®—
        gate_logits = self.main_gate(combined_features)  # [batch_size, num_experts]

        # 5. ä¸“å®¶ç‰¹åŒ–è°ƒåˆ¶
        specialization_scores = torch.matmul(flow_state, self.expert_specialization.T)  # [batch_size, num_experts]
        adjusted_logits = gate_logits + 0.1 * specialization_scores

        # 6. Top-ké€‰æ‹©
        gate_probs = F.softmax(adjusted_logits, dim=-1)
        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)

        # 7. é‡æ–°å½’ä¸€åŒ–
        top_k_probs = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-8)

        # 8. è´Ÿè½½å‡è¡¡æŸå¤±
        expert_usage = gate_probs.mean(dim=0)
        uniform_distribution = torch.ones_like(expert_usage) / self.num_experts
        load_balancing_loss = F.kl_div(
            expert_usage.log(), uniform_distribution, reduction='batchmean'
        )

        return {
            'expert_weights': top_k_probs,
            'expert_indices': top_k_indices,
            'load_balancing_loss': load_balancing_loss,
            'flow_state': flow_state,
            'expert_usage': expert_usage,
            'gate_logits': adjusted_logits
        }