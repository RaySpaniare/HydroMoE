"""
ä½å€¼ç«™ç‚¹å¢å¼ºæ¨¡å—ï¼ˆæ–°å»ºï¼‰
ç›®æ ‡ï¼š
1) ä» cmaes_optimal_params.json è¯»å–ç«™ç‚¹ best_r2ï¼Œæ ‡è®° RÂ² < 0.2 çš„ä½å€¼ç«™ç‚¹
2) åŸºäºåŸå§‹é•¿è¡¨ CSV ç¦»çº¿ç”Ÿæˆâ€œæ»å/æ»šåŠ¨â€çš„å¾„æµç‰¹å¾ï¼ˆä¸ä½¿ç”¨å½“æ—¥ y(t)ï¼‰
3) æä¾›ç‹¬ç«‹ç®¡é“ï¼šç”Ÿæˆå¢å¼ºCSV â†’ æ„é€ æ•°æ®åŠ è½½å™¨ â†’ è®­ç»ƒ/è¯„ä¼°ï¼ˆå¯é€‰ï¼‰

æ³¨æ„ï¼šä¸ä¿®æ”¹ç°æœ‰å¤§æ–‡ä»¶ï¼›é€šè¿‡æ–°æ¨¡å—ç‹¬ç«‹è¿è¡Œæˆ–è¢«ä¸»ç¨‹åºè°ƒç”¨ã€‚
"""

import os
import json
from pathlib import Path
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Set, Optional


def compute_low_r2_stations(cmaes_json_path: str, threshold: float = 0.2) -> Set[str]:
    """
    ä» cmaes_optimal_params.json ä¸­ç­›é€‰ä½RÂ²ç«™ç‚¹ã€‚

    å…¼å®¹ä¸¤ç§ç»“æ„ï¼š
      1) æ—§ç‰ˆ: { 'camels_XXXX': { 'best_r2': ... }, ... }
      2) æ–°ç‰ˆ: { 'optimization_summary': {...}, 'station_results': { sid: {'best_r2': ...}, ... } }
    """
    low_set: Set[str] = set()
    # ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    env_override = os.getenv('CMAES_PARAMS_FILE', '').strip()
    if env_override:
        cmaes_json_path = env_override
    # è§£æå€™é€‰è·¯å¾„ï¼šç»å¯¹è·¯å¾„ä¼˜å…ˆï¼Œå…¶æ¬¡ CWDï¼Œç›¸å¯¹æ¨¡å—ç›®å½•
    candidates = []
    p = Path(cmaes_json_path)
    if p.is_absolute():
        candidates.append(p)
    else:
        candidates.append(Path.cwd() / cmaes_json_path)
        candidates.append(Path(__file__).resolve().parent / cmaes_json_path)
    file_to_open = None
    for cand in candidates:
        if cand.exists():
            file_to_open = cand
            break
    if file_to_open is None:
        print(f"âš ï¸ æœªæ‰¾åˆ°CMA-ESå‚æ•°æ–‡ä»¶: {cmaes_json_path} (å·¥ä½œç›®å½•: {os.getcwd()})")
        return low_set

    try:
        with open(file_to_open, 'r', encoding='utf-8') as f:
            raw = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–CMA-ESæ–‡ä»¶å¤±è´¥: {e}")
        return low_set

    # æ–°ç‰ˆç»“æ„
    if isinstance(raw, dict) and 'station_results' in raw:
        station_dict = raw.get('station_results', {}) or {}
        for sid, rec in station_dict.items():
            try:
                r2 = float(rec.get('best_r2', rec.get('r2', rec.get('R2', 0.0))))
            except Exception:
                r2 = 0.0
            if r2 < threshold:
                low_set.add(str(sid))
        return low_set

    # æ—§ç‰ˆç»“æ„
    if isinstance(raw, dict):
        for sid, rec in raw.items():
            if not isinstance(rec, dict):
                continue
            try:
                r2 = float(rec.get('best_r2', rec.get('r2', rec.get('R2', 0.0))))
            except Exception:
                r2 = 0.0
            if r2 < threshold:
                low_set.add(str(sid))
    return low_set


def _build_runoff_lag_features(group: pd.DataFrame,
                               lags: List[int],
                               roll_windows: List[int]) -> pd.DataFrame:
    """
    é’ˆå¯¹å•ä¸ªç«™ç‚¹åˆ†ç»„ç”Ÿæˆæ»åä¸æ»šåŠ¨ç‰¹å¾ï¼ˆåªä½¿ç”¨å†å²ä¿¡æ¯ï¼‰ã€‚
    - åŸå§‹åˆ—è¦æ±‚ï¼š['date','runoff'] è‡³å°‘ï¼›å…¶ä½™åŸæ ·ä¿ç•™
    - æ»åç‰¹å¾ç©ºå€¼å¡«å……ï¼šä»…é¦–æ—¥ä¸ºç©ºï¼Œå¡«0.0ä»¥é¿å…NaN
    - æ»šåŠ¨ç»Ÿè®¡ä½¿ç”¨ runoff.shift(1) çš„å†å²å€¼ï¼Œmin_periods=1 ä¿è¯æ— NaN
    """
    grp = group.sort_values('date').copy()

    # åŸºç¡€ï¼šå‰ä¸€æ—¥å¾„æµï¼Œä¾›æ»šåŠ¨ç»Ÿè®¡ä½¿ç”¨ï¼ˆåªå«å†å²ï¼‰
    prev = grp['runoff'].shift(1)

    # æ»åç‰¹å¾
    max_lag = max(lags) if lags else 0
    for k in lags:
        col = f'runoff_lag_{k}d'
        grp[col] = grp['runoff'].shift(k)
        # ä»…å¼€å¤´ä¸è¶³ k å¤©çš„æ ·æœ¬ä¼šæ˜¯NaNï¼›ç”¨0.0å¡«å……ä»¥é¿å…ä¸‹æ¸¸ScaleræŠ¥é”™
        grp[col] = grp[col].fillna(0.0)

    # æ»šåŠ¨ç»Ÿè®¡ï¼ˆä½¿ç”¨ prevï¼Œç¡®ä¿åªä¾èµ–å†å²ï¼‰
    for w in roll_windows:
        grp[f'runoff_mean_{w}d'] = prev.rolling(window=w, min_periods=1).mean()
        grp[f'runoff_std_{w}d'] = prev.rolling(window=w, min_periods=1).std().fillna(0.0)

    return grp


def augment_csv_with_runoff_lags(src_csv_path: str,
                                 dst_csv_path: str,
                                 lags: List[int] = [1, 3, 7, 14, 30],
                                 roll_windows: List[int] = [7, 30],
                                 station_col: str = 'station_id',
                                 date_col: str = 'date') -> Tuple[str, List[str]]:
    """
    è¯»å–åŸCSVï¼Œåˆ†ç«™ç‚¹ç”Ÿæˆå¾„æµçš„æ»å/æ»šåŠ¨ç‰¹å¾ï¼Œå†™å‡ºå¢å¼ºCSVã€‚

    Returns:
        (è¾“å‡ºè·¯å¾„, æ–°å¢ç‰¹å¾åˆ—åˆ—è¡¨)
    """
    if not os.path.exists(src_csv_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºCSV: {src_csv_path}")

    df = pd.read_csv(src_csv_path)
    if date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col])
    else:
        raise ValueError(f"CSVç¼ºå°‘æ—¥æœŸåˆ—: {date_col}")

    required = {station_col, date_col, 'runoff'}
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"CSVç¼ºå°‘å¿…è¦åˆ—: {missing}")

    # åˆ†ç«™ç‚¹ç”Ÿæˆ
    groups = []
    for sid, g in df.groupby(station_col, sort=False):
        groups.append(_build_runoff_lag_features(g, lags=lags, roll_windows=roll_windows))

    out = pd.concat(groups, axis=0, ignore_index=True)
    out = out.sort_values([station_col, date_col])

    new_cols = [f'runoff_lag_{k}d' for k in lags]
    for w in roll_windows:
        new_cols += [f'runoff_mean_{w}d', f'runoff_std_{w}d']

    # ç¡®ä¿æ²¡æœ‰NaNï¼ˆä»…æ¥è‡ªé¦–æ—¥stdï¼‰ï¼Œç»Ÿä¸€å¡«0
    out[new_cols] = out[new_cols].fillna(0.0)

    # å†™å‡º
    os.makedirs(os.path.dirname(dst_csv_path) or '.', exist_ok=True)
    out.to_csv(dst_csv_path, index=False)
    print(f"âœ… å·²ç”Ÿæˆå¢å¼ºCSV: {dst_csv_path}  (æ–°å¢åˆ—: {len(new_cols)})")
    return dst_csv_path, new_cols


def export_low_station_list(cmaes_json_path: str, threshold: float, save_path: str) -> Set[str]:
    """
    å¯¼å‡ºä½RÂ²ç«™ç‚¹æ¸…å•CSVï¼ŒåŒ…å« station_id ä¸ flagã€‚
    è¿”å›ä½RÂ²ç«™ç‚¹é›†åˆã€‚
    """
    low_set = compute_low_r2_stations(cmaes_json_path, threshold)
    rows = [{'station_id': sid, 'low_r2_flag': 1} for sid in sorted(low_set)]
    df = pd.DataFrame(rows)
    os.makedirs(os.path.dirname(save_path) or '.', exist_ok=True)
    df.to_csv(save_path, index=False)
    print(f"ğŸ’¾ å·²å¯¼å‡ºä½RÂ²ç«™ç‚¹æ¸…å•({len(low_set)}): {save_path}")
    return low_set


def build_augmented_feature_cols(base_cols: Optional[List[str]], new_cols: List[str], max_cols: int = 20) -> List[str]:
    """
    ç”Ÿæˆæœ€ç»ˆç”¨äºæ¨¡å‹çš„ç‰¹å¾åˆ—æ¸…å•ã€‚
    - base_cols: åŸç‰¹å¾åˆ—ï¼ˆå¦‚ ['pet','precip','temp']ï¼‰
    - new_cols: æ–°å¢å¾„æµæ»å/æ»šåŠ¨ç‰¹å¾
    - max_cols: æ§åˆ¶è§„æ¨¡ï¼Œé»˜è®¤åŠ å…¥è¾ƒç²¾ç®€çš„ä¸€éƒ¨åˆ†ä»¥ç¨³å¥èµ·æ­¥
    """
    base_cols = base_cols or ['pet', 'precip', 'temp']
    # ç²¾ç®€æŒ‘é€‰ï¼šlag(1,3,7) + mean/std(7,30)
    preferred = []
    for k in [1, 3, 7]:
        col = f'runoff_lag_{k}d'
        if col in new_cols:
            preferred.append(col)
    for w in [7, 30]:
        for suf in ['mean', 'std']:
            col = f'runoff_{suf}_{w}d'
            if col in new_cols:
                preferred.append(col)

    final = base_cols + preferred
    if len(final) > max_cols:
        final = final[:max_cols]
    print(f"ğŸ§© æœ€ç»ˆç‰¹å¾åˆ—æ•°: {len(final)} -> {final}")
    return final


def run_pipeline(
    src_csv: str,
    cmaes_json: str = 'cmaes_optimal_params.json',
    out_dir: str = './outputs/augmented',
    r2_threshold: float = 0.2,
    lags: List[int] = [1, 3, 7, 14, 30],
    roll_windows: List[int] = [7, 30],
) -> Dict[str, str]:
    """
    ä¸€é”®ç¦»çº¿å¢å¼ºï¼šç”Ÿæˆå¸¦æ»å/æ»šåŠ¨çš„CSV + å¯¼å‡ºä½RÂ²ç«™ç‚¹æ¸…å•ã€‚
    ä¸ç›´æ¥è®­ç»ƒï¼Œåªå‡†å¤‡æ•°æ®ä¸æ¸…å•ã€‚è¿”å›è·¯å¾„å­—å…¸ã€‚
    """
    os.makedirs(out_dir, exist_ok=True)
    dst_csv = os.path.join(out_dir, 'ç‰¹å¾åˆå¹¶é•¿è¡¨_with_lags.csv')
    low_csv = os.path.join(out_dir, 'low_r2_stations.csv')

    dst_csv, new_cols = augment_csv_with_runoff_lags(
        src_csv_path=src_csv,
        dst_csv_path=dst_csv,
        lags=lags,
        roll_windows=roll_windows,
    )
    low_set = export_low_station_list(cmaes_json, threshold=r2_threshold, save_path=low_csv)

    # åŒæ­¥è¿”å›æ¨èçš„ feature_colsï¼ˆç»™ä¸»ç¨‹åº/é…ç½®ä½¿ç”¨ï¼‰
    feature_cols = build_augmented_feature_cols(['pet', 'precip', 'temp'], new_cols)

    meta = {
        'augmented_csv': dst_csv,
        'low_r2_list': low_csv,
        'recommended_feature_cols': ','.join(feature_cols),
        'low_r2_count': str(len(low_set))
    }
    print(f"âœ… å¢å¼ºå‡†å¤‡å®Œæˆ: {meta}")
    return meta


if __name__ == '__main__':
    # ç¤ºä¾‹ç‹¬ç«‹è¿è¡Œï¼š
    # python MoE_lowflow_augment.py
    # å°†åœ¨ ./outputs/augmented ä¸‹ç”Ÿæˆå¢å¼ºCSVä¸ä½RÂ²æ¸…å•ï¼Œå¹¶æ‰“å°æ¨èç‰¹å¾åˆ—
    default_src = r"D:\Science Research\ä¸­ç§‘é™¢åœ°ç†æ‰€\PBM+ML\æ•°æ®\ç¾å›½å·²å¤„ç†\ç‰¹å¾åˆå¹¶é•¿è¡¨.csv"
    try:
        run_pipeline(src_csv=default_src,
                     cmaes_json='cmaes_optimal_params.json',
                     out_dir='./outputs/augmented',
                     r2_threshold=0.2)
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")


