"""
å¢å¼ºè®­ç»ƒå™¨æ¨¡å— - åŒ…å«æ‰€æœ‰è®­ç»ƒç›¸å…³åŠŸèƒ½
1) é—¨æ§ç†µæ­£åˆ™ï¼ˆé˜²å¡Œç¼©ï¼‰
2) åŸºäºç«™ç‚¹RÂ²çš„æ ·æœ¬åŠ æƒè®­ç»ƒï¼ˆå¯é€‰ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple
import os
import pandas as pd

from MoE_losses import StationR2Loss, create_loss_function
from MoE_metrics import compute_all_metrics
from MoE_advanced_normalization import apply_gradient_clipping

logger = logging.getLogger(__name__)


class EnhancedTrainer:
    """å¢å¼ºè®­ç»ƒå™¨ç±»"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.to(device)
        # æ­£åˆ™ä¸åŠ æƒé…ç½®ï¼ˆç¯å¢ƒå˜é‡å¯è¦†ç›–ï¼‰
        # ğŸš€ å®Œå…¨ç¦ç”¨ç†µæ­£åˆ™ï¼Œå…è®¸ä¸“å®¶å®Œå…¨ä¸“ä¸šåŒ–
        self.gate_entropy_w = float(os.getenv('GATE_ENTROPY_W', '0.0'))  # å®Œå…¨ç¦ç”¨
        self.regime_entropy_w = float(os.getenv('REGIME_ENTROPY_W', '0.0'))  # å®Œå…¨ç¦ç”¨
        self.enable_station_weighting = os.getenv('STATION_WEIGHTING', '1').lower() in ['1','true','yes']
        self.station_weight_lambda = float(os.getenv('STATION_WEIGHT_LAMBDA', '0.5'))
        self.station_weights_csv = os.getenv('STATION_WEIGHTS_CSV', 'outputs/enhanced_real_runoff_predictions/station_performance_real_runoff.csv')
        self._station_weight_map = self._load_station_weights(self.station_weights_csv) if self.enable_station_weighting else {}

    def _load_station_weights(self, csv_path: str) -> Dict[str, float]:
        """ä»å†å²è¯„ä¼°CSVåŠ è½½ç«™ç‚¹æƒé‡: w = 1 + Î» * clamp(0.5 - R2, 0, 1)"""
        weight_map: Dict[str, float] = {}
        try:
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path)
                if 'station_id' in df.columns and 'R2' in df.columns:
                    for _, row in df.iterrows():
                        sid = str(row['station_id'])
                        r2 = row['R2']
                        if pd.notna(r2):
                            delta = max(0.0, min(1.0, 0.5 - float(r2)))
                            w = 1.0 + self.station_weight_lambda * delta
                        else:
                            w = 1.0
                        weight_map[sid] = float(w)
                logging.info(f"ğŸ“¦ å·²åŠ è½½ç«™ç‚¹æƒé‡: {len(weight_map)}")
        except Exception as e:
            logging.warning(f"âš ï¸ åŠ è½½ç«™ç‚¹æƒé‡å¤±è´¥: {e}")
        return weight_map

    def _compute_gate_entropy_loss(self, gate_info: Dict) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—é—¨æ§ç†µæ­£åˆ™: æ¨¡å—(PBM/NN) + Regime(ä½/å¹³/æ´ª)"""
        module_entropy = torch.tensor(0.0, device=self.device)
        regime_entropy = torch.tensor(0.0, device=self.device)
        count_m = 0
        if gate_info is not None and isinstance(gate_info, dict):
            modules = gate_info.get('module_gates', {})
            for mname, minfo in modules.items():
                if isinstance(minfo, dict) and 'effective_gate' in minfo:
                    p = minfo['effective_gate']  # [B,2]
                    if isinstance(p, torch.Tensor):
                        ent = (p * (p + 1e-8).log()).sum(dim=-1).mean()  # sum p log p (<=0)
                        module_entropy = module_entropy + ent
                        count_m += 1
            if count_m > 0:
                module_entropy = module_entropy / count_m
            # Regime æƒé‡
            rw = gate_info.get('regime', {}).get('weights', None)
            if rw is None:
                rw = gate_info.get('regime_weights', None)
            if isinstance(rw, torch.Tensor):
                regime_entropy = (rw * (rw + 1e-8).log()).sum(dim=-1).mean()
        return module_entropy, regime_entropy
    
    def train_step(self, batch, criterion, optimizer):
        """å•æ­¥è®­ç»ƒ - ä¼˜åŒ–ç‰ˆæœ¬"""
        self.model.train()
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(batch, return_gate_info=True)
        predictions = outputs['runoff']
        
        # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥è®¡ç®—åŠ æƒMSEï¼Œé¿å…åˆ›å»ºä¸­é—´å¼ é‡
        targets = batch['targets']
        
        # ç«™ç‚¹åŠ æƒï¼ˆå¦‚å¯ç”¨ï¼‰
        if self.enable_station_weighting and self._station_weight_map:
            if 'station_id' in batch:
                sids = batch['station_id']
                if isinstance(sids, (list, tuple)):
                    w_list = [self._station_weight_map.get(str(s), 1.0) for s in sids]
                    w = torch.tensor(w_list, device=predictions.device, dtype=predictions.dtype)
                else:
                    w = None
            else:
                w = None
        else:
            w = None
        
        # ğŸš€ ä¼˜åŒ–ï¼šæ ¹æ®æ˜¯å¦æœ‰æƒé‡ï¼Œä½¿ç”¨ä¸åŒçš„æŸå¤±è®¡ç®—
        if w is not None:
            base_vec = F.mse_loss(predictions, targets, reduction='none')
            loss_main = (base_vec * w).mean()
        else:
            loss_main = F.mse_loss(predictions, targets)

        # é—¨æ§ç†µæ­£åˆ™ï¼ˆé˜²å¡Œç¼©ï¼Œæœ€å¤§åŒ–ç†µ => æœ€å°åŒ– sum p log pï¼‰
        gate_info = outputs.get('gate_info', {}) if isinstance(outputs, dict) else {}
        mod_ent, reg_ent = self._compute_gate_entropy_loss(gate_info)
        loss_reg = self.gate_entropy_w * mod_ent + self.regime_entropy_w * reg_ent

        # è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆæ¥è‡ªæ¨¡å‹ gate_infoï¼‰ï¼Œé»˜è®¤å…³é—­ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡å¼€å¯
        # å½»åº•å…³é—­è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¿å…å‡è¡¡åŒ–å€¾å‘
        lb_w = float(os.getenv('LOAD_BALANCE_W', '0.0'))
        lb_loss = torch.tensor(0.0, device=self.device)
        try:
            if isinstance(gate_info, dict) and 'load_balancing_loss' in gate_info and lb_w > 0.0:
                raw_lb = gate_info['load_balancing_loss']
                if isinstance(raw_lb, torch.Tensor):
                    lb_loss = raw_lb
        except Exception:
            pass

        loss = loss_main + loss_reg + lb_w * lb_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        grad_norm = apply_gradient_clipping(self.model, max_norm=1.0)
        
        optimizer.step()
        
        return {
            'loss': loss.item(),
            'loss_main': float(loss_main.detach().cpu().item()) if torch.is_tensor(loss_main) else float(loss_main),
            'loss_reg': float(loss_reg.detach().cpu().item()) if torch.is_tensor(loss_reg) else float(loss_reg),
            'loss_lb': float(lb_loss.detach().cpu().item()) if torch.is_tensor(lb_loss) else float(lb_loss),
            'grad_norm': grad_norm,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'accumulated': False
        }
    
    def validate(self, val_loader, criterion):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in val_loader:
                outputs = self.model(batch)
                predictions = outputs['runoff']
                
                if hasattr(criterion, '__call__') and 'station_idx' in str(criterion.__class__):
                    loss = criterion(predictions, batch['targets'], batch.get('station_idx'))
                else:
                    loss = criterion(predictions, batch['targets'])
                
                total_loss += loss.item()
                all_preds.extend(predictions.cpu().numpy())
                all_targets.extend(batch['targets'].cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = compute_all_metrics(all_targets, all_preds)
        
        return total_loss / len(val_loader), metrics


def create_enhanced_trainer(model, strategy="conservative"):
    """åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨"""
    return EnhancedTrainer(model)


def validate_model(model, val_loader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            outputs = model(batch)
            predictions = outputs['runoff']
            
            if isinstance(criterion, StationR2Loss):
                loss = criterion(predictions, batch['targets'], batch.get('station_idx'))
            else:
                loss = criterion(predictions, batch['targets'])
            
            val_losses.append(loss.item())
    
    return np.mean(val_losses)


def quick_validation_metrics(model, val_loader, device, dataset):
    """å¿«é€ŸéªŒè¯æŒ‡æ ‡è®¡ç®—ï¼ˆå°†åˆ—è¡¨å®‰å…¨æ‹¼æ¥ä¸ºNumPyæ•°ç»„åå†è®¡ç®—ï¼‰"""
    model.eval()
    preds_list = []
    targets_list = []

    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            outputs = model(batch)
            predictions = outputs['runoff']  # Tensor [B] or [B,1]
            targets = batch['targets']       # Tensor [B] or [B,1]

            # å§‹ç»ˆå±•å¹³ä¸ºä¸€ç»´å¹¶è½¬ä¸ºnumpy
            preds_np = predictions.detach().cpu().numpy().reshape(-1)
            targets_np = targets.detach().cpu().numpy().reshape(-1)

            preds_list.append(preds_np)
            targets_list.append(targets_np)

    if len(preds_list) == 0:
        return {'R2': 0.0, 'KGE': 0.0, 'RMSE': float('inf')}

    # å®‰å…¨æ‹¼æ¥
    y_pred = np.concatenate(preds_list, axis=0)
    y_true = np.concatenate(targets_list, axis=0)

    return compute_all_metrics(y_true, y_pred)


def enhanced_training_loop(model, train_loader, val_loader, device, epochs=50, patience: int = 5):
    """å¢å¼ºè®­ç»ƒå¾ªç¯ - ä¼˜åŒ–ç‰ˆæœ¬"""

    print("\nğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ...")
    
    # ğŸš€ æ˜¾å­˜ç›‘æ§
    if device.type == 'cuda':
        print(f"ğŸ’¾ åˆå§‹æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**2:.1f} MB")
        print(f"ğŸ’¾ ä¿ç•™æ˜¾å­˜: {torch.cuda.memory_reserved()/1024**2:.1f} MB")

    # 1. ä½¿ç”¨ä¿å®ˆé…ç½®åˆ›å»ºè®­ç»ƒå™¨
    enhanced_trainer = create_enhanced_trainer(model, strategy="conservative")

    # 2. ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    print("âœ… ä½¿ç”¨MSEæŸå¤±å‡½æ•°ï¼ˆæ ‡å‡†åšæ³•ï¼‰")
    print("  ğŸ“Š è®­ç»ƒé…ç½®: ä¿å®ˆæ¨¡å¼ - LR=1e-4, GradClip=1.0")

    # 3. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-5,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=20, T_mult=2, eta_min=1e-6
    )
    
    # 4. è®­ç»ƒçŠ¶æ€
    best_val_loss = float('inf')
    best_val_r2 = -1e9
    patience_counter = 0
    patience = int(patience)
    
    # 5. è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        should_log = True
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        train_grad_norms = []
        if should_log:
            print(f"\nğŸ“… Epoch {epoch+1}/{epochs}")
            print("-" * 50)

        for batch_idx, batch in enumerate(train_loader):
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

            stats = enhanced_trainer.train_step(batch, criterion, optimizer)

            if not stats.get('accumulated', False):
                train_losses.append(stats['loss'])
                train_grad_norms.append(stats['grad_norm'])

        # ğŸš€ ä¼˜åŒ–ï¼šè®­ç»ƒé˜¶æ®µç»“æŸåå†ç»Ÿä¸€æ¸…ç†æ˜¾å­˜ï¼Œå‡å°‘é¢å¤–åŒæ­¥å¼€é”€
        if device.type == 'cuda':
            torch.cuda.empty_cache()

        # éªŒè¯é˜¶æ®µ
        if True:
            val_loss = validate_model(model, val_loader, criterion, device)
            # è¯¦ç»†æŒ‡æ ‡
            val_metrics = quick_validation_metrics(model, val_loader, device, val_loader.dataset)
            val_r2 = float(val_metrics.get('R2', 0.0))

            # å¯é€‰ï¼šç»Ÿè®¡Regimeé—¨æ§
            if should_log:
                try:
                    with torch.no_grad():
                        reg_weights = []
                        for vbatch in val_loader:
                            vbatch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in vbatch.items()}
                            voutputs = model(vbatch)
                            if 'regime_weights' in voutputs:
                                reg_weights.append(voutputs['regime_weights'].cpu().numpy())
                        
                        if reg_weights:
                            reg_weights = np.concatenate(reg_weights, axis=0)
                            regime_means = np.mean(reg_weights, axis=0)
                            regime_labels = ['ä½', 'å¹³', 'æ´ª']
                            regime_str = ', '.join([f"{label}={mean:.3f}" for label, mean in zip(regime_labels, regime_means)])
                            print(f"  ğŸ§­ Regimeé—¨æ§å‡å€¼: {regime_str}")
                except Exception as e:
                    pass  # å¿½ç•¥é—¨æ§ç»Ÿè®¡é”™è¯¯

            # è¾“å‡ºè®­ç»ƒä¿¡æ¯
            if should_log:
                print(f"  ğŸ“Š è®­ç»ƒæŸå¤±: {np.mean(train_losses):.4f}")
                try:
                    print(f"    â”œâ”€ ä¸»æŸå¤±: {stats.get('loss_main', float('nan')):.4f}")
                    print(f"    â”œâ”€ æ­£åˆ™(é—¨æ§+Regime): {stats.get('loss_reg', float('nan')):.4f}")
                    print(f"    â””â”€ è´Ÿè½½å‡è¡¡: {stats.get('loss_lb', float('nan')):.6f}")
                except Exception:
                    pass
                print(f"  ğŸ“ˆ éªŒè¯æŸå¤±: {val_loss:.4f}")
                print(f"  ğŸ¯ å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(train_grad_norms):.4f}")
                print(f"  ğŸ“‹ éªŒè¯æŒ‡æ ‡ (mm/day):")
                print(f"    ğŸ¯ RÂ²: {val_r2:.4f} (ä¸»è¦ç›®æ ‡)")
                print(f"    ğŸ¯ KGE: {val_metrics.get('KGE', 0.0):.4f} (ä¸»è¦ç›®æ ‡)")
                print(f"    ğŸ“Š RMSE: {val_metrics.get('RMSE', 0.0):.4f}")
                print(f"    ğŸ“Š Bias: {val_metrics.get('bias', 0.0):.4f}")

            # æ—©åœé€»è¾‘
            if val_r2 > best_val_r2:
                best_val_r2 = val_r2
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰
                os.makedirs("outputs", exist_ok=True)
                model_path = "outputs/enhanced_hydromoe_best.pth"
                torch.save(model.state_dict(), model_path)
                print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
                if should_log:
                    print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹ (RÂ²={val_r2:.4f})")
            else:
                patience_counter += 1
                if should_log:
                    print(f"  â³ æœªæ”¹è¿›è®¡æ•°: {patience_counter}/{patience}")

            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœRÂ²æ˜æ˜¾å›è½ï¼Œç»™äºˆæ›´å¤šè€å¿ƒ
            if should_log and epoch > 25 and val_r2 < best_val_r2 - 0.05 and patience_counter < patience:
                print(f"  âš ï¸ éªŒè¯RÂ²è¾ƒæ˜æ˜¾å›è½ (å½“å‰={val_r2:.4f} vs æœ€ä½³={best_val_r2:.4f})ï¼Œç»§ç»­è§‚å¯Ÿï¼Œä¸æ—©åœ")

            if patience_counter >= patience:
                print(f"  ğŸ›‘ æ—©åœè§¦å‘ (patience={patience})")
                break
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
    
    print("\nâœ… å¢å¼ºè®­ç»ƒå®Œæˆ")
    return enhanced_trainer


def evaluate_enhanced_model(model, test_loader, device):
    """è¯„ä¼°å¢å¼ºæ¨¡å‹"""

    print("\nğŸ“Š è¯„ä¼°å¢å¼ºæ¨¡å‹...")

    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for batch in test_loader:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            outputs = model(batch)
            predictions = outputs['runoff']

            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(batch['targets'].cpu().numpy())

    if len(all_predictions) > 0:
        metrics = compute_all_metrics(all_targets, all_predictions)

        print("ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"  RÂ²: {metrics.get('R2', 0):.4f}")
        print(f"  KGE: {metrics.get('KGE', 0):.4f}")
        print(f"  RMSE: {metrics.get('RMSE', 0):.4f}")
        print(f"  Bias: {metrics.get('bias', 0):.4f}")

        return metrics
    else:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
        return {'R2': 0.0, 'KGE': 0.0, 'RMSE': float('inf')}
