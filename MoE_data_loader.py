"""
ä¿®å¤ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å™¨ - æ­£ç¡®å¤„ç†æ•°æ®æ ‡å‡†åŒ–ï¼ŒåŒ…å«å…¨å±€ç¼“å­˜ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple, Optional
import os
from pathlib import Path
import logging
import pickle
from dataclasses import dataclass
from datetime import datetime
import time
import gc  # ğŸ”¥ æ·»åŠ åƒåœ¾å›æ”¶æ¨¡å—

# å¯¼å…¥ç‰¹å¾å·¥ç¨‹æ¨¡å—
try:
    from MoE_feature_engineering import HydroFeatureEngineer, AdaptiveFeatureSelector
    FEATURE_ENGINEERING_AVAILABLE = True
except ImportError:
    FEATURE_ENGINEERING_AVAILABLE = False
    print("è­¦å‘Šï¼šç‰¹å¾å·¥ç¨‹æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€ç‰¹å¾")

from MoE_data_utils import (
    build_cache_path,
    read_table_auto,
    read_from_cache,
    write_cache,
    has_parquet_support,
)


# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸš€ å¢å¼ºå…¨å±€æ•°æ®ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½å¤§æ–‡ä»¶å’Œé‡å¤åºåˆ—åˆ›å»º
_GLOBAL_DATA_CACHE = {
    'raw_data': None,           # åŸå§‹å®Œæ•´æ•°æ®
    'filtered_data': None,      # ç­›é€‰åæ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼‰
    'cache_params': None,       # ç¼“å­˜å‚æ•°ï¼ˆæ–‡ä»¶è·¯å¾„ã€å¿«é€Ÿæµ‹è¯•é…ç½®ç­‰ï¼‰
    'load_time': None,          # åŠ è½½æ—¶é—´æˆ³
    # ğŸš€ æ–°å¢ï¼šåºåˆ—ç¼“å­˜
    'sequences_cache': {},      # åˆ†splitç¼“å­˜åºåˆ—ï¼š{'train': [...], 'val': [...], 'test': [...]}
    'scalers_cache': None,      # æ ‡å‡†åŒ–å™¨ç¼“å­˜
    'grouped_data_cache': None, # åˆ†ç»„æ•°æ®ç¼“å­˜ï¼ˆæŒ‰ç«™ç‚¹åˆ†ç»„çš„ç»“æœï¼‰
    'sequence_cache_params': None  # åºåˆ—ç¼“å­˜å‚æ•°
}


@dataclass
class FixedDataConfig:
    """ä¿®å¤ç‰ˆæ•°æ®é…ç½®"""
    # æ–‡ä»¶è·¯å¾„
    csv_path: str = r"D:\Science Research\ä¸­ç§‘é™¢åœ°ç†æ‰€\PBM+ML\æ•°æ®\ç¾å›½å·²å¤„ç†\ç‰¹å¾åˆå¹¶é•¿è¡¨.csv"
    
    # åºåˆ—é…ç½®
    sequence_length: int = 96  # åºåˆ—é•¿åº¦ï¼ˆæ—¶é—´æ­¥ï¼‰
    sequence_stride: int = 16  # åºåˆ—æ»‘åŠ¨æ­¥é•¿
    
    # ç‰¹å¾åˆ—
    feature_cols: List[str] = None
    target_col: str = "runoff"
    
    # æ—¶é—´åˆ’åˆ† - ä½¿ç”¨å…·ä½“æ—¥æœŸè€Œä¸æ˜¯æ¯”ä¾‹
    train_start: str = '1980-01-01'
    train_end: str = '1999-12-31'
    val_start: str = '2000-01-01'
    val_end: str = '2007-12-31'
    test_start: str = '2008-01-01'
    test_end: str = '2014-09-30'
    
    # æ•°æ®åˆ†å‰²ï¼ˆä¿ç•™ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
    use_date_split: bool = True  # æ˜¯å¦ä½¿ç”¨å…·ä½“æ—¥æœŸåˆ†å‰²
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    test_ratio: float = 0.15
    
    # æ•°æ®å¤„ç†
    normalize_features: bool = True
    normalize_targets: bool = True
    
    # å…¨ç«™ç‚¹è®­ç»ƒé…ç½®
    use_all_stations: bool = True   # å¯ç”¨å…¨éƒ¨50ä¸ªç«™ç‚¹è®­ç»ƒ
    quick_test: bool = False
    quick_test_stations: int = 10   # å¿«é€Ÿæµ‹è¯•æ—¶çš„ç«™ç‚¹æ•°
    
    # ç«™ç‚¹åˆ†æ‰¹å¤„ç†è§„æ¨¡ï¼ˆåˆ›å»ºåºåˆ—æ—¶ä¸€æ¬¡å¤„ç†çš„ç«™ç‚¹æ•°ï¼‰
    station_batch_size: int = 100
    
    # ğŸš€ æ€§èƒ½ä¼˜åŒ–å‚æ•°
    use_sequence_cache: bool = True      # æ˜¯å¦ä½¿ç”¨åºåˆ—ç¼“å­˜
    parallel_sequence_creation: bool = False  # æ˜¯å¦å¯ç”¨å¹¶è¡Œåºåˆ—åˆ›å»ºï¼ˆå®éªŒæ€§ï¼‰
    max_sequence_workers: int = 4        # å¹¶è¡Œåˆ›å»ºåºåˆ—çš„æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°

    # å¯é€‰ï¼šä»…è®­ç»ƒæŒ‡å®šç«™ç‚¹ï¼ˆç”¨äºè¯»å–ä¸Šä¸€è½®ä½RÂ²ç«™ç‚¹åŠ é€Ÿè®­ç»ƒï¼‰
    filter_station_ids: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.feature_cols is None:
            # æ ¹æ®æ¶æ„è®¾è®¡ï¼šè¾“å…¥æ˜¯è’¸æ•£å‘ã€é™æ°´ã€æ¸©åº¦ï¼Œè¾“å‡ºæ˜¯å¾„æµ
            self.feature_cols = ["pet", "precip", "temp"]
        
        # éªŒè¯åˆ†å‰²æ¯”ä¾‹
        assert abs(self.train_ratio + self.val_ratio + self.test_ratio - 1.0) < 1e-6


class FixedHydroDataset(Dataset):
    """ä¿®å¤ç‰ˆæ°´æ–‡æ•°æ®é›†ç±» - æ­£ç¡®å¤„ç†æ ‡å‡†åŒ–"""
    
    def __init__(self, config: FixedDataConfig, split: str = "train", scalers: Dict = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            config: æ•°æ®é…ç½®
            split: æ•°æ®åˆ†å‰² ("train", "val", "test")
            scalers: æ ‡å‡†åŒ–å™¨å­—å…¸ï¼ˆä»è®­ç»ƒé›†ä¼ é€’ç»™éªŒè¯/æµ‹è¯•é›†ï¼‰
        """
        self.config = config
        self.split = split
        self.scalers = scalers or {}
        
        # åŠ è½½å¹¶å¤„ç†æ•°æ®
        self._load_data()
        self._split_by_time()
        self._normalize_data()
        
        # ğŸš€ æ£€æŸ¥åºåˆ—ç¼“å­˜
        if self._check_sequence_cache():
            logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜åºåˆ—ï¼Œè·³è¿‡åˆ›å»ºè¿‡ç¨‹")
        else:
            self._create_sequences()
            self._cache_sequences()  # ç¼“å­˜æ–°åˆ›å»ºçš„åºåˆ—
        
        logger.info(f"{split.upper()} æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
        logger.info(f"  - åºåˆ—æ•°é‡: {len(self.sequences)}")
        logger.info(f"  - åºåˆ—é•¿åº¦: {self.config.sequence_length}")
        logger.info(f"  - ç‰¹å¾ç»´åº¦: {len(self.config.feature_cols)}")
        logger.info(f"  - ç«™ç‚¹æ•°é‡: {len(self.station_list)}")
    
    def _load_data(self):
        """åŠ è½½åˆå¹¶åçš„é•¿è¡¨æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å…¨å±€ç¼“å­˜"""
        global _GLOBAL_DATA_CACHE
        
        # ç”Ÿæˆå½“å‰é…ç½®çš„ç¼“å­˜é”®
        cache_key = {
            'csv_path': self.config.csv_path,
            'use_all_stations': getattr(self.config, 'use_all_stations', False),
            'quick_test': self.config.quick_test,
            'quick_test_stations': self.config.quick_test_stations if self.config.quick_test else None,
            'filter_station_ids': tuple(sorted(str(s) for s in self.config.filter_station_ids)) if getattr(self.config, 'filter_station_ids', None) else None
        }
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å¯ç”¨
        if (_GLOBAL_DATA_CACHE['cache_params'] == cache_key and 
            _GLOBAL_DATA_CACHE['filtered_data'] is not None):
            
            # ä½¿ç”¨ç¼“å­˜æ•°æ® - é¿å…æ˜‚è´µçš„copyæ“ä½œ
            self.data = _GLOBAL_DATA_CACHE['filtered_data']  # ç›´æ¥å¼•ç”¨ï¼Œä¸å¤åˆ¶
            logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜æ•°æ®: {self.data.shape} (é¿å…äº†{_GLOBAL_DATA_CACHE['load_time']:.2f}ç§’çš„é‡æ–°åŠ è½½)")
            logger.info(f"ç¼“å­˜å‘½ä¸­: ç«™ç‚¹æ•°={self.data['station_id'].nunique()}")
        
        else:
            # éœ€è¦é‡æ–°åŠ è½½æ•°æ®
            logger.info(f"ğŸ’¾ æ­£åœ¨åŠ è½½æ•°æ®: {self.config.csv_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.config.csv_path):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {self.config.csv_path}")
            
            # è®°å½•åŠ è½½å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            raw_path = self.config.csv_path
            wanted = ['station_id','date','lon','lat'] + list(set(self.config.feature_cols + [self.config.target_col]))
            cache_path = build_cache_path(raw_path)
            raw_data = None

            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
            if cache_path and cache_path.exists():
                cached = read_from_cache(cache_path, columns=wanted if cache_path.suffix.lower() == '.parquet' else None)
                if cached is not None:
                    raw_data = cached
                    logger.info(f"âš¡ ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_path}")

            if raw_data is None:
                try:
                    raw_data = read_table_auto(raw_path, usecols=wanted, parse_dates=['date'])
                except Exception:
                    raw_data = read_table_auto(raw_path, parse_dates=['date'])

                # å¼‚æ­¥å†™å…¥ç¼“å­˜
                if cache_path and not cache_path.exists():
                    write_cache(raw_data, cache_path)
            
            # è‹¥åŸå§‹æ˜¯CSVä¸”æ£€æµ‹åˆ°å¯ç”¨åˆ—å¼æ”¯æŒï¼Œè‡ªåŠ¨åœ¨åŒç›®å½•ç”ŸæˆParquetï¼Œåç»­å³è¯»Parquet
            try:
                lower = raw_path.lower()
                if raw_data is not None and (lower.endswith('.csv') or lower.endswith('.txt')) and has_parquet_support():
                    p = Path(raw_path)
                    dst = p.with_suffix('.parquet')
                    # ä»…åœ¨ä¸å­˜åœ¨æ—¶è½¬æ¢ï¼Œé¿å…æ¯æ¬¡å†™ç›˜
                    if not dst.exists():
                        raw_data.to_parquet(str(dst), index=False)
                        logger.info(f"ğŸ§­ å·²è‡ªåŠ¨è½¬æ¢ä¸ºParquet: {dst}")
                        # æ›´æ–°è·¯å¾„ä¾›åç»­è¿è¡Œä½¿ç”¨
                        self.config.csv_path = str(dst)
            except Exception as e:
                logger.warning(f"åˆ—å¼è‡ªåŠ¨è½¬æ¢å¤±è´¥ï¼ˆå¿½ç•¥å¹¶ç»§ç»­CSVï¼‰: {e}")
            logger.info(f"åŸå§‹æ•°æ®å½¢çŠ¶: {raw_data.shape}")
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if 'date' in raw_data.columns and not pd.api.types.is_datetime64_any_dtype(raw_data['date']):
                raw_data['date'] = pd.to_datetime(raw_data['date'])
            
            # ç«™ç‚¹ç­›é€‰é€»è¾‘
            if getattr(self.config, 'filter_station_ids', None):
                # ä»…ä½¿ç”¨æŒ‡å®šç«™ç‚¹
                sid_set = set(str(s) for s in self.config.filter_station_ids)
                filtered_data = raw_data[raw_data['station_id'].astype(str).isin(sid_set)]
                logger.info(f"æŒ‰è¿‡æ»¤ç«™ç‚¹è®­ç»ƒ: ä½¿ç”¨ {filtered_data['station_id'].nunique()} ä¸ªç«™ç‚¹ (æ¥è‡ªå…ˆéªŒåˆ—è¡¨)")
                logger.info(f"æ•°æ®å½¢çŠ¶: {filtered_data.shape}")
            elif hasattr(self.config, 'use_all_stations') and self.config.use_all_stations:
                # å…¨ç«™ç‚¹è®­ç»ƒæ¨¡å¼
                filtered_data = raw_data
                unique_stations = raw_data['station_id'].unique()
                logger.info(f"å…¨ç«™ç‚¹è®­ç»ƒæ¨¡å¼: ä½¿ç”¨æ‰€æœ‰ {len(unique_stations)} ä¸ªç«™ç‚¹")
                logger.info(f"æ•°æ®å½¢çŠ¶: {filtered_data.shape}")
            elif self.config.quick_test:
                # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
                unique_stations = raw_data['station_id'].unique()
                selected_stations = unique_stations[:self.config.quick_test_stations]
                filtered_data = raw_data[raw_data['station_id'].isin(selected_stations)]
                logger.info(f"å¿«é€Ÿæµ‹è¯•æ¨¡å¼: é€‰æ‹© {len(selected_stations)} ä¸ªç«™ç‚¹")
                logger.info(f"ç­›é€‰åæ•°æ®å½¢çŠ¶: {filtered_data.shape}")
            else:
                # é»˜è®¤ä½¿ç”¨å…¨éƒ¨æ•°æ®
                filtered_data = raw_data
            
            # è®°å½•åŠ è½½æ—¶é—´
            load_time = time.time() - start_time
            
            # æ›´æ–°å…¨å±€ç¼“å­˜ - é¿å…å¤åˆ¶ï¼Œç›´æ¥å­˜å‚¨å¼•ç”¨
            _GLOBAL_DATA_CACHE.update({
                'raw_data': raw_data,
                'filtered_data': filtered_data,  # ç›´æ¥å­˜å‚¨ï¼Œä¸å¤åˆ¶
                'cache_params': cache_key,
                'load_time': load_time
            })
            
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            
            # ä½¿ç”¨ç­›é€‰åçš„æ•°æ®
            self.data = filtered_data
        
        # è·å–ç«™ç‚¹åˆ—è¡¨
        self.station_list = sorted(self.data['station_id'].unique())
        logger.info(f"ç«™ç‚¹æ•°é‡: {len(self.station_list)}")
    
    def _split_by_time(self):
        """æŒ‰æ—¶é—´åˆ†å‰²è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›† - æ”¯æŒå…·ä½“æ—¥æœŸåˆ†å‰²"""
        if self.config.use_date_split:
            # ä½¿ç”¨å…·ä½“æ—¥æœŸåˆ†å‰²
            logger.info("ä½¿ç”¨å…·ä½“æ—¥æœŸåˆ†å‰²æ•°æ®")
            
            # æ ¹æ®å½“å‰åˆ†å‰²é€‰æ‹©å¯¹åº”æ—¶é—´æ®µçš„æ•°æ®
            if self.split == "train":
                start_date = pd.to_datetime(self.config.train_start)
                end_date = pd.to_datetime(self.config.train_end)
            elif self.split == "val":
                start_date = pd.to_datetime(self.config.val_start)
                end_date = pd.to_datetime(self.config.val_end)
            else:  # test
                # ğŸ”¥ ä¿®å¤ï¼šæµ‹è¯•é›†éœ€è¦å‘å‰æ‰©å±•sequence_lengthå¤©ä»¥ç¡®ä¿ä»test_startå°±èƒ½æœ‰é¢„æµ‹
                test_start = pd.to_datetime(self.config.test_start)
                start_date = test_start - pd.Timedelta(days=self.config.sequence_length-1)
                end_date = pd.to_datetime(self.config.test_end)
                print(f"ğŸ“… æµ‹è¯•é›†æ•°æ®èŒƒå›´æ‰©å±•: {start_date.date()} ~ {end_date.date()}")
                print(f"   å®é™…é¢„æµ‹èŒƒå›´: {test_start.date()} ~ {end_date.date()}")
            
            # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®
            mask = (self.data['date'] >= start_date) & (self.data['date'] <= end_date)
            self.data = self.data.loc[mask].copy()
            
            logger.info(f"{self.split.upper()} æ—¶é—´æ®µ: {start_date.date()} ~ {end_date.date()}")
            logger.info(f"{self.split.upper()} åˆ†å‰²åæ•°æ®å½¢çŠ¶: {self.data.shape}")
            logger.info(f"{self.split.upper()} åŒ…å«ç«™ç‚¹æ•°: {self.data['station_id'].nunique()}")
            
        else:
            # ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
            logger.info("ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²æ•°æ®")
            
            # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸå¹¶æ’åº
            unique_dates = sorted(self.data['date'].unique())
            n_dates = len(unique_dates)
            
            # è®¡ç®—åˆ†å‰²ç‚¹
            n_train = int(n_dates * self.config.train_ratio)
            n_val = int(n_dates * self.config.val_ratio)
            
            # åˆ†å‰²æ—¥æœŸ
            train_dates = unique_dates[:n_train]
            val_dates = unique_dates[n_train:n_train + n_val]
            test_dates = unique_dates[n_train + n_val:]
            
            logger.info(f"æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®:")
            logger.info(f"  - è®­ç»ƒæœŸé—´: {train_dates[0]} ~ {train_dates[-1]} ({len(train_dates)}å¤©)")
            logger.info(f"  - éªŒè¯æœŸé—´: {val_dates[0]} ~ {val_dates[-1]} ({len(val_dates)}å¤©)")
            logger.info(f"  - æµ‹è¯•æœŸé—´: {test_dates[0]} ~ {test_dates[-1]} ({len(test_dates)}å¤©)")
            
            # æ ¹æ®å½“å‰åˆ†å‰²é€‰æ‹©å¯¹åº”æ—¶é—´æ®µçš„æ•°æ®
            if self.split == "train":
                selected_dates = train_dates
            elif self.split == "val":
                selected_dates = val_dates
            else:  # test
                selected_dates = test_dates
            
            self.data = self.data.loc[self.data['date'].isin(selected_dates)].copy()
            
            logger.info(f"{self.split.upper()} åˆ†å‰²åæ•°æ®å½¢çŠ¶: {self.data.shape}")
            logger.info(f"{self.split.upper()} åŒ…å«ç«™ç‚¹æ•°: {self.data['station_id'].nunique()}")
    
    def _normalize_data(self):
        """æ•°æ®æ ‡å‡†åŒ– - æ”¹è¿›ç‰ˆæœ¬ï¼Œé’ˆå¯¹å¾„æµæ•°æ®ä¼˜åŒ–"""
        from MoE_advanced_normalization import HydroLogNormalizer
        from sklearn.preprocessing import RobustScaler

        if self.split == "train":
            # è®­ç»ƒé›†ï¼šåˆ›å»ºå¹¶åº”ç”¨æ ‡å‡†åŒ–å™¨
            if self.config.normalize_features:
                # å¯¹ç‰¹å¾ä½¿ç”¨é²æ£’æ ‡å‡†åŒ–
                self.feature_scaler = RobustScaler()
                # ğŸš€ ä¼˜åŒ–ï¼šé¿å…å¤šæ¬¡ç´¢å¼•ï¼Œç›´æ¥ä¿®æ”¹æ•°ç»„
                feature_data = self.data[self.config.feature_cols].values
                normalized = self.feature_scaler.fit_transform(feature_data)
                self.data.loc[:, self.config.feature_cols] = normalized
                logger.info("è®­ç»ƒé›†ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨RobustScalerï¼‰")

            if self.config.normalize_targets:
                # å¯¹å¾„æµç›®æ ‡å˜é‡ä½¿ç”¨ä¸“é—¨çš„å¯¹æ•°æ­£æ€å½’ä¸€åŒ–å™¨
                self.target_scaler = HydroLogNormalizer(add_constant=0.01, use_robust=True)

                # ğŸš€ ä¼˜åŒ–ï¼šé¿å…é‡å¤å–å€¼
                runoff_data = self.data[self.config.target_col].values
                runoff_min, runoff_max = runoff_data.min(), runoff_data.max()

                # åº”ç”¨å¯¹æ•°æ­£æ€å½’ä¸€åŒ–
                normalized_runoff = self.target_scaler.fit_transform(runoff_data)
                self.data.loc[:, self.config.target_col] = normalized_runoff

                logger.info("è®­ç»ƒé›†å¾„æµç›®æ ‡å˜é‡æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨HydroLogNormalizerï¼‰")
                logger.info(f"  åŸå§‹å¾„æµèŒƒå›´: {runoff_min:.3f} ~ {runoff_max:.3f} mm/day")
                logger.info(f"  æ ‡å‡†åŒ–åèŒƒå›´: {normalized_runoff.min():.3f} ~ {normalized_runoff.max():.3f}")

                # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°ä¾›éªŒè¯/æµ‹è¯•é›†ä½¿ç”¨
                self.scalers = {
                    'feature_scaler': self.feature_scaler if self.config.normalize_features else None,
                    'target_scaler': self.target_scaler if self.config.normalize_targets else None
                }

        else:
            # éªŒè¯/æµ‹è¯•é›†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„æ ‡å‡†åŒ–å™¨
            if self.scalers is None:
                logger.warning(f"{self.split.upper()} é›†æ²¡æœ‰æä¾›æ ‡å‡†åŒ–å™¨ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                return

            if self.config.normalize_features and 'feature_scaler' in self.scalers and self.scalers['feature_scaler'] is not None:
                # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥æ“ä½œæ•°ç»„é¿å…å¤šæ¬¡ç´¢å¼•
                feature_data = self.data[self.config.feature_cols].values
                normalized = self.scalers['feature_scaler'].transform(feature_data)
                self.data.loc[:, self.config.feature_cols] = normalized
                logger.info(f"{self.split.upper()} é›†ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨è®­ç»ƒé›†RobustScalerå‚æ•°ï¼‰")

            if self.config.normalize_targets and 'target_scaler' in self.scalers and self.scalers['target_scaler'] is not None:
                # ğŸš€ ä¼˜åŒ–ï¼šé¿å…é‡å¤å–å€¼
                runoff_data = self.data[self.config.target_col].values
                runoff_min, runoff_max = runoff_data.min(), runoff_data.max()

                # åº”ç”¨å¯¹æ•°æ­£æ€å½’ä¸€åŒ–
                normalized_runoff = self.scalers['target_scaler'].transform(runoff_data)
                self.data.loc[:, self.config.target_col] = normalized_runoff

                logger.info(f"{self.split.upper()} é›†å¾„æµç›®æ ‡å˜é‡æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨è®­ç»ƒé›†HydroLogNormalizerå‚æ•°ï¼‰")
                logger.info(f"  åŸå§‹å¾„æµèŒƒå›´: {runoff_min:.3f} ~ {runoff_max:.3f} mm/day")
                logger.info(f"  æ ‡å‡†åŒ–åèŒƒå›´: {normalized_runoff.min():.3f} ~ {normalized_runoff.max():.3f}")
    
    def _create_sequences(self):
        """åˆ›å»ºæ—¶é—´åºåˆ— - é«˜é€Ÿä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"ğŸš€ å¼€å§‹åˆ›å»ºåºåˆ— (é«˜é€Ÿä¼˜åŒ–æ¨¡å¼)")
        self.sequences = []
        
        # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šé¢„å…ˆæŒ‰ç«™ç‚¹åˆ†ç»„å¹¶æ’åºï¼Œé¿å…é‡å¤query
        logger.info("ğŸ“Š é¢„å¤„ç†ï¼šæŒ‰ç«™ç‚¹åˆ†ç»„æ•°æ®...")
        start_time = time.time()
        
        # é¦–å…ˆç¡®ä¿æ•°æ®æŒ‰station_id, dateæ’åºï¼ˆä¸€æ¬¡æ€§æ’åºï¼‰
        if not self.data.index.is_monotonic_increasing:
            self.data = self.data.sort_values(['station_id', 'date']).reset_index(drop=True)
        
        # ä½¿ç”¨groupbyä¸€æ¬¡æ€§åˆ†ç»„ï¼Œé¿å…é‡å¤query
        grouped_data = dict(list(self.data.groupby('station_id', sort=False)))
        logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
        
        # åˆ†æ‰¹å¤„ç†ç«™ç‚¹ï¼Œé¿å…å†…å­˜å³°å€¼
        batch_size = getattr(self.config, 'station_batch_size', 100)
        try:
            batch_size = int(batch_size)
            if batch_size <= 0:
                batch_size = 100
        except Exception:
            batch_size = 100
        station_batches = [self.station_list[i:i+batch_size] for i in range(0, len(self.station_list), batch_size)]
        
        for batch_idx, station_batch in enumerate(station_batches):
            batch_start_time = time.time()
            logger.info(f"å¤„ç†ç«™ç‚¹æ‰¹æ¬¡ {batch_idx+1}/{len(station_batches)} ({len(station_batch)}ä¸ªç«™ç‚¹)")
            
            for station_idx_in_batch, station_id in enumerate(station_batch):
                # è·å–ç«™ç‚¹ç´¢å¼•
                station_idx = self.station_list.index(station_id)
                
                # ğŸš€ ç›´æ¥ä»é¢„åˆ†ç»„çš„æ•°æ®ä¸­è·å–ç«™ç‚¹æ•°æ®ï¼ˆå·²æ’åºï¼‰
                if station_id not in grouped_data:
                    continue
                    
                station_data = grouped_data[station_id]
                
                # æ£€æŸ¥æ•°æ®é•¿åº¦
                if len(station_data) < self.config.sequence_length:
                    continue
                
                # ğŸš€ é«˜æ•ˆæ•°ç»„é¢„è®¡ç®—ï¼šä¸€æ¬¡æ€§æå–æ‰€æœ‰éœ€è¦çš„æ•°æ®
                features_array = station_data[self.config.feature_cols].values.astype(np.float32)
                targets_array = station_data[self.config.target_col].values.astype(np.float32)
                dates_array = station_data['date'].values
                
                # ç»çº¬åº¦ä¸ºå¯é€‰åˆ—ï¼šç¼ºå¤±æ—¶å¡«NaN
                try:
                    lons = float(station_data['lon'].iloc[0])
                except Exception:
                    lons = np.nan
                try:
                    lats = float(station_data['lat'].iloc[0])
                except Exception:
                    lats = np.nan
                
                # ğŸš€ çŸ¢é‡åŒ–æ—¶é—´ç‰¹å¾è®¡ç®—ï¼ˆä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªç«™ç‚¹çš„æ—¥æœŸï¼‰
                dates_pd = pd.to_datetime(dates_array)
                months = dates_pd.month.values
                day_of_years = dates_pd.dayofyear.values
                
                # çŸ¢é‡åŒ–å­£èŠ‚æ€§ç¼–ç 
                month_sin = np.sin(2 * np.pi * months / 12).astype(np.float32)
                month_cos = np.cos(2 * np.pi * months / 12).astype(np.float32)
                doy_sin = np.sin(2 * np.pi * day_of_years / 365).astype(np.float32)
                doy_cos = np.cos(2 * np.pi * day_of_years / 365).astype(np.float32)
                
                # ç»„åˆæ—¶é—´ç‰¹å¾çŸ©é˜µ [n_days, 4]
                all_time_features = np.column_stack([month_sin, month_cos, doy_sin, doy_cos])
                
                # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ— - çŸ¢é‡åŒ–æ‰¹é‡åˆ›å»º
                stride = 1 if self.split == "test" else self.config.sequence_stride
                seq_len = self.config.sequence_length
                
                # ğŸš€ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰åºåˆ—ç´¢å¼•
                max_start = len(station_data) - seq_len
                if max_start < 0:
                    continue
                    
                start_indices = np.arange(0, max_start + 1, stride)
                
                # ğŸš€ æ‰¹é‡åˆ›å»ºåºåˆ—ï¼ˆçŸ¢é‡åŒ–æ“ä½œï¼‰
                for start_idx in start_indices:
                    end_idx = start_idx + seq_len
                    
                    # ç›´æ¥æ•°ç»„åˆ‡ç‰‡ï¼ˆæœ€é«˜æ•ˆï¼‰
                    features = features_array[start_idx:end_idx]
                    targets = targets_array[start_idx:end_idx]
                    time_features = all_time_features[start_idx:end_idx]
                    
                    # ğŸš€ ä¼˜åŒ–ï¼šé¢„è®¡ç®—æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé¿å…åœ¨__getitem__ä¸­é‡å¤è½¬æ¢
                    start_date_str = pd.to_datetime(dates_array[start_idx]).strftime('%Y-%m-%d')
                    end_date_str = pd.to_datetime(dates_array[end_idx-1]).strftime('%Y-%m-%d')
                    
                    # æ·»åŠ åˆ°åºåˆ—åˆ—è¡¨
                    self.sequences.append({
                        'features': features,
                        'targets': targets,
                        'time_features': time_features,
                        'station_id': station_id,
                        'station_idx': station_idx,
                        'lon': lons,
                        'lat': lats,
                        'start_date': dates_array[start_idx],
                        'end_date': dates_array[end_idx-1],
                        'start_date_str': start_date_str,  # ğŸš€ é¢„ç¼“å­˜
                        'end_date_str': end_date_str  # ğŸš€ é¢„ç¼“å­˜
                    })
                
                # ğŸ”¥ æ¯å¤„ç†å®Œä¸€ä¸ªç«™ç‚¹å°±æ¸…ç†å†…å­˜
                del station_data, features_array, targets_array, dates_array
                gc.collect()
            
            # æ¯ä¸ªæ‰¹æ¬¡åæ¸…ç†å†…å­˜å¹¶æ˜¾ç¤ºè¿›åº¦
            batch_time = time.time() - batch_start_time
            sequences_per_sec = len(self.sequences) / max(batch_time, 0.1)
            logger.info(f"æ‰¹æ¬¡ {batch_idx+1}/{len(station_batches)} å®Œæˆï¼Œè€—æ—¶: {batch_time:.2f}ç§’")
            logger.info(f"  - å½“å‰æ€»åºåˆ—æ•°: {len(self.sequences):,}, åˆ›å»ºé€Ÿåº¦: {sequences_per_sec:.0f} åºåˆ—/ç§’")
            gc.collect()  # ğŸ”¥ æ¯ä¸ªæ‰¹æ¬¡éƒ½å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        logger.info(f"ä¸º {len(self.station_list)} ä¸ªç«™ç‚¹åˆ›å»ºäº† {len(self.sequences)} ä¸ªåºåˆ—")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘å†…å­˜å¤åˆ¶å’Œè®¡ç®—"""
        sequence = self.sequences[idx]
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨from_numpyé¿å…å¤åˆ¶ï¼Œç›´æ¥å…±äº«å†…å­˜
        features = torch.from_numpy(sequence['features']).float()
        targets = torch.from_numpy(sequence['targets']).float()
        time_features = torch.from_numpy(sequence.get('time_features', 
                                                      np.zeros((features.shape[0], 4), dtype=np.float32))).float()
        
        target_scalar = targets[-1]

        # ğŸš€ ä¼˜åŒ–ï¼šç®€åŒ–scaleræŸ¥æ‰¾é€»è¾‘
        raw_features_last = None
        if self.config.normalize_features:
            scaler = getattr(self, 'feature_scaler', None) or (self.scalers.get('feature_scaler') if self.scalers else None)
            if scaler is not None:
                try:
                    last_feat = sequence['features'][-1:, :]  # ä¿æŒ2Dé¿å…reshape
                    raw_last_np = scaler.inverse_transform(last_feat).astype(np.float32, copy=False).flatten()
                    raw_features_last = torch.from_numpy(raw_last_np)
                except Exception:
                    raw_features_last = features[-1]
        if raw_features_last is None:
            raw_features_last = features[-1]
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼“å­˜çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆé¿å…é‡å¤è½¬æ¢ï¼‰
        start_date_str = sequence.get('start_date_str', pd.to_datetime(sequence['start_date']).strftime('%Y-%m-%d'))
        end_date_str = sequence.get('end_date_str', pd.to_datetime(sequence['end_date']).strftime('%Y-%m-%d'))
        
        return {
            'features': features,
            'time_features': time_features,
            'targets': target_scalar,
            'targets_seq': targets,
            'station_id': sequence['station_id'],
            'lon': torch.tensor([sequence['lon']], dtype=torch.float32),
            'lat': torch.tensor([sequence['lat']], dtype=torch.float32),
            'station_idx': torch.tensor([sequence['station_idx']], dtype=torch.long),
            'raw_features_last': raw_features_last,
            'start_date': start_date_str,
            'end_date': end_date_str
        }

    def get_scalers(self):
        """è·å–æ ‡å‡†åŒ–å™¨ï¼ˆä»…è®­ç»ƒé›†æœ‰æ•ˆï¼‰"""
        if self.split == "train" and hasattr(self, 'scalers'):
            return self.scalers
        return None
    
    def _check_sequence_cache(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„åºåˆ—"""
        global _GLOBAL_DATA_CACHE
        
        # ç”Ÿæˆåºåˆ—ç¼“å­˜é”®
        cache_key = {
            'split': self.split,
            'sequence_length': self.config.sequence_length,
            'sequence_stride': self.config.sequence_stride,
            'feature_cols': tuple(self.config.feature_cols),
            'target_col': self.config.target_col,
            'csv_path': self.config.csv_path,
            'normalize_features': self.config.normalize_features,
            'normalize_targets': self.config.normalize_targets,
            'data_shape': getattr(self.data, 'shape', None),
            'station_count': len(self.station_list)
        }
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å¯ç”¨
        if (_GLOBAL_DATA_CACHE['sequence_cache_params'] == cache_key and 
            self.split in _GLOBAL_DATA_CACHE['sequences_cache']):
            
            # ä½¿ç”¨ç¼“å­˜åºåˆ—
            self.sequences = _GLOBAL_DATA_CACHE['sequences_cache'][self.split]
            
            # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œè¿˜è¦æ£€æŸ¥æ ‡å‡†åŒ–å™¨ç¼“å­˜
            if self.split == "train" and _GLOBAL_DATA_CACHE['scalers_cache'] is not None:
                self.scalers = _GLOBAL_DATA_CACHE['scalers_cache']
                # ä¸ºäº†å…¼å®¹ç°æœ‰æ¥å£ï¼Œè®¾ç½®ç›¸åº”çš„å±æ€§
                if 'feature_scaler' in self.scalers:
                    self.feature_scaler = self.scalers['feature_scaler']
                if 'target_scaler' in self.scalers:
                    self.target_scaler = self.scalers['target_scaler']
            
            return True
        
        return False
    
    def _cache_sequences(self):
        """ç¼“å­˜åˆ›å»ºçš„åºåˆ—"""
        global _GLOBAL_DATA_CACHE
        
        # ç”Ÿæˆåºåˆ—ç¼“å­˜é”®
        cache_key = {
            'split': self.split,
            'sequence_length': self.config.sequence_length,
            'sequence_stride': self.config.sequence_stride,
            'feature_cols': tuple(self.config.feature_cols),
            'target_col': self.config.target_col,
            'csv_path': self.config.csv_path,
            'normalize_features': self.config.normalize_features,
            'normalize_targets': self.config.normalize_targets,
            'data_shape': getattr(self.data, 'shape', None),
            'station_count': len(self.station_list)
        }
        
        # ç¼“å­˜åºåˆ—
        _GLOBAL_DATA_CACHE['sequences_cache'][self.split] = self.sequences
        _GLOBAL_DATA_CACHE['sequence_cache_params'] = cache_key
        
        # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œç¼“å­˜æ ‡å‡†åŒ–å™¨
        if self.split == "train" and hasattr(self, 'scalers'):
            _GLOBAL_DATA_CACHE['scalers_cache'] = self.scalers
        
        logger.info(f"âœ… {self.split}é›†åºåˆ—å·²ç¼“å­˜ ({len(self.sequences)}ä¸ª)")
    
    def clear_sequence_cache():
        """æ¸…ç†åºåˆ—ç¼“å­˜"""
        global _GLOBAL_DATA_CACHE
        _GLOBAL_DATA_CACHE['sequences_cache'].clear()
        _GLOBAL_DATA_CACHE['scalers_cache'] = None
        _GLOBAL_DATA_CACHE['grouped_data_cache'] = None
        _GLOBAL_DATA_CACHE['sequence_cache_params'] = None
        logger.info("ğŸ—‘ï¸ åºåˆ—ç¼“å­˜å·²æ¸…ç†")


def get_sequence_cache_info() -> Dict:
    """è·å–åºåˆ—ç¼“å­˜ä¿¡æ¯"""
    global _GLOBAL_DATA_CACHE
    
    cache_info = {
        'has_sequences_cache': bool(_GLOBAL_DATA_CACHE['sequences_cache']),
        'cached_splits': list(_GLOBAL_DATA_CACHE['sequences_cache'].keys()),
        'cache_params': _GLOBAL_DATA_CACHE['sequence_cache_params'],
    }
    
    # è®¡ç®—ç¼“å­˜å¤§å°
    if _GLOBAL_DATA_CACHE['sequences_cache']:
        total_sequences = sum(len(sequences) for sequences in _GLOBAL_DATA_CACHE['sequences_cache'].values())
        cache_info['total_cached_sequences'] = total_sequences
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨
        if total_sequences > 0:
            # å‡è®¾æ¯ä¸ªåºåˆ—å¤§çº¦å ç”¨ sequence_length * (feature_dims + target_dims + time_dims) * 4 bytes
            # ç²—ç•¥ä¼°ç®—ï¼š96 * (10 + 1 + 4) * 4 = 5760 bytes â‰ˆ 6KB per sequence
            estimated_mb = total_sequences * 6 / 1024
            cache_info['estimated_memory_mb'] = estimated_mb
    
    return cache_info


def warmup_data_loading(config: FixedDataConfig) -> None:
    """ğŸ”¥ æ•°æ®åŠ è½½é¢„çƒ­ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰é¢„åŠ è½½æ‰€æœ‰æ•°æ®"""
    logger.info("ğŸ”¥ æ•°æ®åŠ è½½é¢„çƒ­å¼€å§‹...")
    
    start_time = time.time()
    
    # é¢„åŠ è½½æ‰€æœ‰æ•°æ®é›†
    splits_info = preload_all_datasets(config)
    
    # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
    cache_info = get_sequence_cache_info()
    
    total_time = time.time() - start_time
    logger.info(f"ğŸ¯ æ•°æ®é¢„çƒ­å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
    logger.info(f"  - ç¼“å­˜çŠ¶æ€: {cache_info}")
    
    return splits_info


    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        sequence = self.sequences[idx]
        
        # è½¬æ¢ä¸ºtensor
        features = torch.FloatTensor(sequence['features'])  # [seq_len, n_features]
        targets = torch.FloatTensor(sequence['targets'])    # [seq_len]
        
        # ğŸ”¥ æ·»åŠ æ—¶é—´ç‰¹å¾
        import numpy as np  # ç¡®ä¿numpyå¯ç”¨
        time_features = torch.FloatTensor(sequence.get('time_features', 
                                                      np.zeros((features.shape[0], 4))))  # [seq_len, 4]
        
        # å¯¹äºåºåˆ—åˆ°ç‚¹é¢„æµ‹ï¼Œåªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡å€¼
        target_scalar = targets[-1]  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºé¢„æµ‹ç›®æ ‡

        # è¿˜åŸæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„åŸå§‹ç‰©ç†é©±åŠ¨(æœªæ ‡å‡†åŒ–)ï¼Œä¾›PBMä½¿ç”¨
        raw_features_last = None
        try:
            if self.config.normalize_features:
                scaler = getattr(self, 'feature_scaler', None)
                if scaler is None and self.scalers and 'feature_scaler' in self.scalers:
                    scaler = self.scalers['feature_scaler']
                if scaler is not None:
                    import numpy as np  # å±€éƒ¨å¯¼å…¥ä»¥é¿å…é¡¶å±‚ä¾èµ–
                    last_feat = sequence['features'][-1].reshape(1, -1)
                    raw_last_np = scaler.inverse_transform(last_feat).astype(np.float32).flatten()
                    raw_features_last = torch.from_numpy(raw_last_np)
        except Exception:
            # å›é€€ï¼šæ— æ³•åå½’ä¸€åŒ–æ—¶ï¼Œä½¿ç”¨å·²æ ‡å‡†åŒ–å€¼ï¼ˆä»å¯è¿è¡Œï¼Œä½†ç‰©ç†æ„ä¹‰è¾ƒå¼±ï¼‰
            raw_features_last = features[-1]
        if raw_features_last is None:
            raw_features_last = features[-1]
        
        # ç›´æ¥ä½¿ç”¨é¢„å­˜å‚¨çš„æ—¥æœŸï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        import pandas as pd
        start_date_str = pd.to_datetime(sequence['start_date']).strftime('%Y-%m-%d')
        end_date_str = pd.to_datetime(sequence['end_date']).strftime('%Y-%m-%d')
        
        return {
            'features': features,
            'time_features': time_features,  # ğŸ”¥ æ–°å¢æ—¶é—´ç‰¹å¾
            'targets': target_scalar,  # ä½¿ç”¨æ ‡é‡ç›®æ ‡å€¼ [1]
            'targets_seq': targets,    # æ–°å¢ï¼šå®Œæ•´ç›®æ ‡åºåˆ— [seq_len]
            'station_id': sequence['station_id'],
            'lon': torch.FloatTensor([sequence['lon']]),
            'lat': torch.FloatTensor([sequence['lat']]),
            'station_idx': torch.LongTensor([sequence['station_idx']]),
            'raw_features_last': raw_features_last,  # æä¾›æœªæ ‡å‡†åŒ–çš„æœ€åæ—¶æ­¥ç‰©ç†é©±åŠ¨
            'start_date': start_date_str,  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            'end_date': end_date_str  # é¢„æµ‹ç›®æ ‡å¯¹åº”çš„æ—¥æœŸå­—ç¬¦ä¸²
        }

    def get_scalers(self):
        """è·å–æ ‡å‡†åŒ–å™¨ï¼ˆä»…è®­ç»ƒé›†æœ‰æ•ˆï¼‰"""
        if self.split == "train" and hasattr(self, 'scalers'):
            return self.scalers
        return None


def create_fixed_data_loaders(config: FixedDataConfig, batch_size: int = 64, num_workers: int = 0, 
                             pin_memory: bool = True, prefetch_factor: int = 2) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    åˆ›å»ºä¿®å¤ç‰ˆçš„æ•°æ®åŠ è½½å™¨ - æ­£ç¡®å¤„ç†æ ‡å‡†åŒ–ï¼Œæ”¯æŒGPUä¼˜åŒ–ï¼Œä¼˜åŒ–ç¼“å­˜æœºåˆ¶
    """
    logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨å¢å¼ºç¼“å­˜ä¼˜åŒ–...")
    
    # ğŸš€ ä¼˜åŒ–ï¼šè‡ªåŠ¨æ¸…ç†æ—§çš„åºåˆ—ç¼“å­˜ï¼Œé¿å…å†…å­˜æ³„æ¼
    import gc
    gc.collect()
    
    # ğŸš€ æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®é›†éƒ½å·²ç¼“å­˜
    global _GLOBAL_DATA_CACHE
    if (_GLOBAL_DATA_CACHE['sequence_cache_params'] is not None and 
        all(split in _GLOBAL_DATA_CACHE['sequences_cache'] for split in ['train', 'val', 'test'])):
        logger.info("ğŸ¯ æ£€æµ‹åˆ°å®Œæ•´åºåˆ—ç¼“å­˜ï¼Œå¿«é€Ÿåˆ›å»ºæ•°æ®é›†...")
    
    # åˆ›å»ºè®­ç»ƒé›†
    logger.info("ğŸ“Š åˆ›å»ºè®­ç»ƒé›†...")
    train_dataset = FixedHydroDataset(config, split="train")
    
    # è·å–è®­ç»ƒé›†çš„æ ‡å‡†åŒ–å™¨
    scalers = train_dataset.get_scalers()
    
    # åˆ›å»ºéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œä¼ å…¥è®­ç»ƒé›†çš„æ ‡å‡†åŒ–å™¨
    logger.info("ğŸ“Š åˆ›å»ºéªŒè¯é›†...")
    val_dataset = FixedHydroDataset(config, split="val", scalers=scalers)
    
    logger.info("ğŸ“Š åˆ›å»ºæµ‹è¯•é›†...")
    test_dataset = FixedHydroDataset(config, split="test", scalers=scalers)
    
    # GPUä¼˜åŒ–çš„DataLoaderå‚æ•°
    loader_kwargs = {
        'num_workers': num_workers,
        'pin_memory': pin_memory,
    }
    
    # åªæœ‰åœ¨num_workers > 0æ—¶æ‰æ·»åŠ prefetch_factor
    if num_workers > 0:
        loader_kwargs['prefetch_factor'] = prefetch_factor
        loader_kwargs['persistent_workers'] = True
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        **loader_kwargs
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        **loader_kwargs
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        **loader_kwargs
    )
    
    logger.info(f"ä¿®å¤ç‰ˆæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"  - è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
    logger.info(f"  - éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
    logger.info(f"  - æµ‹è¯•é›†æ‰¹æ¬¡æ•°: {len(test_loader)}")
    
    return train_loader, val_loader, test_loader


def clear_data_cache():
    """æ¸…ç†å…¨å±€æ•°æ®ç¼“å­˜"""
    global _GLOBAL_DATA_CACHE
    _GLOBAL_DATA_CACHE.update({
        'raw_data': None,
        'filtered_data': None,
        'cache_params': None,
        'load_time': None,
        'sequences_cache': {},
        'scalers_cache': None,
        'grouped_data_cache': None,
        'sequence_cache_params': None
    })
    logger.info("ğŸ—‘ï¸ å…¨å±€æ•°æ®ç¼“å­˜å·²æ¸…ç†")


def get_cache_info() -> Dict:
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    global _GLOBAL_DATA_CACHE
    
    cache_info = {
        'has_raw_data': _GLOBAL_DATA_CACHE['raw_data'] is not None,
        'has_filtered_data': _GLOBAL_DATA_CACHE['filtered_data'] is not None,
        'cache_params': _GLOBAL_DATA_CACHE['cache_params'],
        'load_time': _GLOBAL_DATA_CACHE['load_time']
    }
    
    if cache_info['has_filtered_data']:
        cache_info['data_shape'] = _GLOBAL_DATA_CACHE['filtered_data'].shape
        cache_info['memory_usage_mb'] = _GLOBAL_DATA_CACHE['filtered_data'].memory_usage(deep=True).sum() / 1024 / 1024
    
    return cache_info


def preload_data(config: FixedDataConfig) -> None:
    """é¢„åŠ è½½æ•°æ®åˆ°ç¼“å­˜ä¸­"""
    logger.info("ğŸš€ é¢„åŠ è½½æ•°æ®ä¸­...")
    
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°æ®é›†æ¥è§¦å‘æ•°æ®åŠ è½½
    temp_dataset = FixedHydroDataset(config, split="train")
    
    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    cache_info = get_cache_info()
    if cache_info['has_filtered_data']:
        logger.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆ:")
        logger.info(f"  - æ•°æ®å½¢çŠ¶: {cache_info['data_shape']}")
        logger.info(f"  - å†…å­˜å ç”¨: {cache_info['memory_usage_mb']:.1f} MB")
        logger.info(f"  - åŠ è½½è€—æ—¶: {cache_info['load_time']:.2f} ç§’")


def preload_all_datasets(config: FixedDataConfig) -> Dict[str, int]:
    """ğŸš€ é¢„åŠ è½½æ‰€æœ‰æ•°æ®é›†åˆ°ç¼“å­˜ï¼ˆè®­ç»ƒå‰ä¸€æ¬¡æ€§å‡†å¤‡ï¼‰"""
    logger.info("ğŸš€ å¼€å§‹é¢„åŠ è½½æ‰€æœ‰æ•°æ®é›†...")
    start_time = time.time()
    
    # æŒ‰é¡ºåºåˆ›å»ºæ‰€æœ‰æ•°æ®é›†ï¼Œè§¦å‘ç¼“å­˜
    splits_info = {}
    
    # 1. è®­ç»ƒé›†ï¼ˆä¼šåˆ›å»ºæ ‡å‡†åŒ–å™¨ï¼‰
    logger.info("ğŸ“Š é¢„åŠ è½½è®­ç»ƒé›†...")
    train_dataset = FixedHydroDataset(config, split="train")
    scalers = train_dataset.get_scalers()
    splits_info['train'] = len(train_dataset)
    
    # 2. éªŒè¯é›†
    logger.info("ğŸ“Š é¢„åŠ è½½éªŒè¯é›†...")
    val_dataset = FixedHydroDataset(config, split="val", scalers=scalers)
    splits_info['val'] = len(val_dataset)
    
    # 3. æµ‹è¯•é›†
    logger.info("ğŸ“Š é¢„åŠ è½½æµ‹è¯•é›†...")
    test_dataset = FixedHydroDataset(config, split="test", scalers=scalers)
    splits_info['test'] = len(test_dataset)
    
    total_time = time.time() - start_time
    total_sequences = sum(splits_info.values())
    
    logger.info(f"âœ… æ‰€æœ‰æ•°æ®é›†é¢„åŠ è½½å®Œæˆï¼")
    logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
    logger.info(f"  - æ€»åºåˆ—æ•°: {total_sequences:,}")
    logger.info(f"  - è®­ç»ƒé›†: {splits_info['train']:,}")
    logger.info(f"  - éªŒè¯é›†: {splits_info['val']:,}")
    logger.info(f"  - æµ‹è¯•é›†: {splits_info['test']:,}")
    logger.info(f"  - åºåˆ—åˆ›å»ºé€Ÿåº¦: {total_sequences/total_time:.0f} åºåˆ—/ç§’")
    
    return splits_info


# ä¼˜åŒ–å»ºè®®ï¼šå¯¹äºé¢‘ç¹ä½¿ç”¨çš„åœºæ™¯ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥
def optimize_for_repeated_use():
    """ä¸ºé‡å¤ä½¿ç”¨ä¼˜åŒ–çš„å»ºè®®"""
    suggestions = [
        "1. ä½¿ç”¨ warmup_data_loading() åœ¨è®­ç»ƒå¼€å§‹å‰é¢„åŠ è½½æ‰€æœ‰æ•°æ®",
        "2. åœ¨ä¸åŒå®éªŒé—´ä¿æŒPythonä¼šè¯ä»¥ç»´æŒç¼“å­˜",
        "3. ä¼˜åŒ–åçš„æ•°æ®åŠ è½½å™¨ä¼šè‡ªåŠ¨ä½¿ç”¨Parquetæ ¼å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰",
        "4. åºåˆ—ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åˆ›å»ºåºåˆ—",
        "5. çŸ¢é‡åŒ–æ—¶é—´ç‰¹å¾è®¡ç®—æå‡æ€§èƒ½5-10å€"
    ]
    
    for suggestion in suggestions:
        logger.info(f" {suggestion}")
    
    return suggestions


def benchmark_data_loading(config: FixedDataConfig, runs: int = 3) -> Dict[str, float]:
    """ğŸš€ æ•°æ®åŠ è½½æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    logger.info(f"ğŸƒ å¼€å§‹æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•ï¼Œè¿è¡Œ{runs}æ¬¡...")
    
    times = []
    for run in range(runs):
        # æ¸…ç†ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡éƒ½æ˜¯å†·å¯åŠ¨
        clear_data_cache()
        
        start_time = time.time()
        
        # æµ‹è¯•å®Œæ•´çš„æ•°æ®åŠ è½½æµç¨‹
        train_loader, val_loader, test_loader = create_fixed_data_loaders(
            config, batch_size=32, num_workers=0
        )
        
        end_time = time.time()
        run_time = end_time - start_time
        times.append(run_time)
        
        logger.info(f"è¿è¡Œ {run+1}/{runs}: {run_time:.2f}ç§’")
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        total_sequences = len(train_loader.dataset) + len(val_loader.dataset) + len(test_loader.dataset)
        sequences_per_sec = total_sequences / run_time
        
        logger.info(f"  - æ€»åºåˆ—æ•°: {total_sequences:,}")
        logger.info(f"  - åˆ›å»ºé€Ÿåº¦: {sequences_per_sec:.0f} åºåˆ—/ç§’")
    
    # è®¡ç®—ç»Ÿè®¡
    avg_time = np.mean(times)
    std_time = np.std(times)
    min_time = np.min(times)
    
    logger.info(f"ğŸ¯ æ€§èƒ½æµ‹è¯•ç»“æœ:")
    logger.info(f"  - å¹³å‡æ—¶é—´: {avg_time:.2f} Â± {std_time:.2f}ç§’")
    logger.info(f"  - æœ€å¿«æ—¶é—´: {min_time:.2f}ç§’")
    
    return {
        'average_time': avg_time,
        'std_time': std_time,
        'min_time': min_time,
        'times': times
    }