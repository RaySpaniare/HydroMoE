"""
æ°´æ–‡æ¨¡å‹è¯„ä¼°æŒ‡æ ‡è®¡ç®—
åŒ…å«RÂ², KGE, RMSE, MSE, biasç­‰å®Œæ•´æŒ‡æ ‡
"""

import torch
import numpy as np
from typing import Dict, Tuple, Union, List, Any


def _to_numpy_1d(x: Union[torch.Tensor, np.ndarray, List[Any]]) -> np.ndarray:
    """å°†è¾“å…¥ç»Ÿä¸€è½¬æ¢ä¸º 1D numpy æ•°ç»„ã€‚
    æ”¯æŒ torch.Tensor / np.ndarray / list-likeã€‚
    """
    if isinstance(x, torch.Tensor):
        x = x.detach().cpu().numpy()
    elif not isinstance(x, np.ndarray):
        x = np.asarray(x)
    try:
        x = x.astype(np.float64, copy=False)
    except Exception:
        x = np.asarray(x, dtype=np.float64)
    return x.reshape(-1)


def compute_r2(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
               y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—å†³å®šç³»æ•° RÂ²
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        RÂ²å€¼
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    # è®¡ç®—RÂ²
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    
    if ss_tot == 0:
        return np.nan
    
    r2 = 1 - (ss_res / ss_tot)
    return float(r2)


def compute_kge(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> Tuple[float, Dict[str, float]]:
    """
    è®¡ç®—Kling-Guptaæ•ˆç‡ç³»æ•° (KGE)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        KGEå€¼å’Œåˆ†è§£ç»„ä»¶
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan, {}
    
    # è®¡ç®—KGEç»„ä»¶ï¼ˆæ•°å€¼ç¨³å®šï¼‰
    eps = 1e-12
    n_valid = len(y_true)
    # 1) ç›¸å…³ç³»æ•° (r) - é¿å…é›¶æ–¹å·®/é•¿åº¦ä¸è¶³
    std_true = float(np.std(y_true)) if n_valid > 0 else 0.0
    std_pred = float(np.std(y_pred)) if n_valid > 0 else 0.0
    if n_valid < 2 or std_true < eps or std_pred < eps:
        correlation = 0.0
    else:
        try:
            correlation = float(np.corrcoef(y_true, y_pred)[0, 1])
        except Exception:
            correlation = 0.0
    if np.isnan(correlation):
        correlation = 0.0
    
    # 2. åå·®æ¯”ç‡ (Î²) - å‡å€¼æ¯”ç‡
    mean_true = float(np.mean(y_true)) if n_valid > 0 else 0.0
    mean_pred = float(np.mean(y_pred)) if n_valid > 0 else 0.0
    bias_ratio = (mean_pred + eps) / (mean_true + eps)
    
    # 3. å˜å¼‚ç³»æ•°æ¯”ç‡ (Î³) - æ ‡å‡†å·®æ¯”ç‡
    # 3) å˜å¼‚ç³»æ•°æ¯” (Î³) - ç¨³å®šè®¡ç®—
    cv_true = (std_true) / (mean_true + eps)
    cv_pred = (std_pred) / (mean_pred + eps)
    variability_ratio = cv_pred / (cv_true + eps)
    
    # è®¡ç®—KGE
    kge = 1 - float(np.sqrt((correlation - 1)**2 + (bias_ratio - 1)**2 + (variability_ratio - 1)**2))
    
    components = {
        'correlation': float(correlation),
        'bias_ratio': float(bias_ratio),
        'variability_ratio': float(variability_ratio)
    }
    
    return float(kge), components


def compute_rmse(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                 y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—å‡æ–¹æ ¹è¯¯å·® (RMSE)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        RMSEå€¼
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    return float(rmse)


def compute_mse(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—å‡æ–¹è¯¯å·® (MSE)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        MSEå€¼
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    mse = np.mean((y_true - y_pred) ** 2)
    return float(mse)


def compute_bias(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                 y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—åå·® (bias)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        biaså€¼ (é¢„æµ‹å‡å€¼ - è§‚æµ‹å‡å€¼)
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    bias = np.mean(y_pred) - np.mean(y_true)
    return float(bias)


def compute_mae(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·® (MAE)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        MAEå€¼
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    mae = np.mean(np.abs(y_true - y_pred))
    return float(mae)


def compute_nse(y_true: Union[torch.Tensor, np.ndarray, List[Any]], 
                y_pred: Union[torch.Tensor, np.ndarray, List[Any]]) -> float:
    """
    è®¡ç®—Nash-Sutcliffeæ•ˆç‡ç³»æ•° (NSE)
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        NSEå€¼
    """
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return np.nan
    
    # è®¡ç®—NSE
    numerator = np.sum((y_true - y_pred) ** 2)
    denominator = np.sum((y_true - np.mean(y_true)) ** 2)
    
    if denominator == 0:
        return np.nan
    
    nse = 1 - (numerator / denominator)
    return float(nse)


def compute_all_metrics(y_true: Union[torch.Tensor, np.ndarray], 
                       y_pred: Union[torch.Tensor, np.ndarray]) -> Dict[str, float]:
    """
    è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ - ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    # ğŸš€ ä¼˜åŒ–ï¼šç»Ÿä¸€é¢„å¤„ç†ï¼Œé¿å…é‡å¤è½¬æ¢
    y_true = _to_numpy_1d(y_true)
    y_pred = _to_numpy_1d(y_pred)
    
    # ç§»é™¤NaNå€¼ï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼‰
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) == 0:
        return {'R2': np.nan, 'KGE': np.nan, 'RMSE': np.nan, 'MSE': np.nan, 
                'bias': np.nan, 'MAE': np.nan, 'NSE': np.nan}
    
    # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼Œå…±äº«ä¸­é—´è®¡ç®—
    errors = y_true - y_pred
    sq_errors = errors ** 2
    abs_errors = np.abs(errors)
    
    # åŸºç¡€ç»Ÿè®¡
    mean_true = np.mean(y_true)
    mean_pred = np.mean(y_pred)
    std_true = np.std(y_true)
    std_pred = np.std(y_pred)
    
    # MSE, RMSE, MAE
    mse = np.mean(sq_errors)
    rmse = np.sqrt(mse)
    mae = np.mean(abs_errors)
    bias = mean_pred - mean_true
    
    # RÂ² å’Œ NSEï¼ˆå…¬å¼ç›¸åŒï¼‰
    ss_res = np.sum(sq_errors)
    ss_tot = np.sum((y_true - mean_true) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else np.nan
    nse = r2  # NSEå’ŒRÂ²è®¡ç®—ç›¸åŒ
    
    # KGE
    correlation = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 and std_true > 0 and std_pred > 0 else 0.0
    bias_ratio = (mean_pred + 1e-12) / (mean_true + 1e-12)
    cv_true = std_true / (mean_true + 1e-12)
    cv_pred = std_pred / (mean_pred + 1e-12)
    variability_ratio = cv_pred / (cv_true + 1e-12)
    kge = 1 - float(np.sqrt((correlation - 1)**2 + (bias_ratio - 1)**2 + (variability_ratio - 1)**2))
    
    # æ±‡æ€»æ‰€æœ‰æŒ‡æ ‡
    metrics = {
        'R2': float(r2),
        'KGE': float(kge),
        'RMSE': float(rmse),
        'MSE': float(mse),
        'bias': float(bias),
        'MAE': float(mae),
        'NSE': float(nse),
        'KGE_correlation': float(correlation),
        'KGE_bias_ratio': float(bias_ratio),
        'KGE_variability_ratio': float(variability_ratio)
    }
    
    return metrics


def format_metrics_string(metrics: Dict[str, float], precision: int = 4) -> str:
    """
    æ ¼å¼åŒ–æŒ‡æ ‡ä¸ºå­—ç¬¦ä¸²
    
    Args:
        metrics: æŒ‡æ ‡å­—å…¸
        precision: å°æ•°ä½ç²¾åº¦
        
    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    formatted_parts = []
    
    # ä¸»è¦æŒ‡æ ‡
    main_metrics = ['R2', 'KGE', 'RMSE', 'MSE', 'bias']
    for metric in main_metrics:
        if metric in metrics:
            value = metrics[metric]
            if np.isnan(value):
                formatted_parts.append(f"{metric}: NaN")
            else:
                formatted_parts.append(f"{metric}: {value:.{precision}f}")
    
    return ", ".join(formatted_parts)


if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°æŒ‡æ ‡
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—...")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    y_true = np.random.randn(1000) * 2 + 5
    y_pred = y_true + np.random.randn(1000) * 0.5  # æ·»åŠ ä¸€äº›å™ªå£°
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    metrics = compute_all_metrics(y_true, y_pred)
    
    print("ğŸ“Š è¯„ä¼°æŒ‡æ ‡ç»“æœ:")
    for metric, value in metrics.items():
        if np.isnan(value):
            print(f"  {metric}: NaN")
        else:
            print(f"  {metric}: {value:.6f}")
    
    print(f"\nğŸ“ æ ¼å¼åŒ–è¾“å‡º: {format_metrics_string(metrics)}")
    print(" è¯„ä¼°æŒ‡æ ‡æµ‹è¯•å®Œæˆï¼")


def compute_stratified_metrics(y_true: Union[torch.Tensor, np.ndarray],
                              y_pred: Union[torch.Tensor, np.ndarray],
                              quantiles: List[float] = [0.33, 0.67]) -> Dict[str, Dict[str, float]]:
    """
    è®¡ç®—åˆ†å±‚è¯„ä¼°æŒ‡æ ‡ï¼ˆä½ã€ä¸­ã€é«˜å¾„æµåˆ†åˆ«è¯„ä¼°ï¼‰

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        quantiles: åˆ†å±‚çš„åˆ†ä½æ•°é˜ˆå€¼

    Returns:
        åˆ†å±‚æŒ‡æ ‡å­—å…¸
    """
    if isinstance(y_true, torch.Tensor):
        y_true = y_true.cpu().numpy()
    if isinstance(y_pred, torch.Tensor):
        y_pred = y_pred.cpu().numpy()

    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]

    if len(y_true) == 0:
        return {}

    # è®¡ç®—åˆ†ä½æ•°é˜ˆå€¼
    thresholds = np.quantile(y_true, quantiles)

    # åˆ†å±‚
    low_mask = y_true <= thresholds[0]
    high_mask = y_true > thresholds[-1]
    mid_mask = ~(low_mask | high_mask)

    stratified_metrics = {}

    # ä½å¾„æµ
    if np.sum(low_mask) > 0:
        stratified_metrics['low_flow'] = compute_all_metrics(y_true[low_mask], y_pred[low_mask])
        stratified_metrics['low_flow']['sample_count'] = int(np.sum(low_mask))
        stratified_metrics['low_flow']['flow_range'] = f"{y_true[low_mask].min():.3f}-{y_true[low_mask].max():.3f}"

    # ä¸­å¾„æµ
    if np.sum(mid_mask) > 0:
        stratified_metrics['mid_flow'] = compute_all_metrics(y_true[mid_mask], y_pred[mid_mask])
        stratified_metrics['mid_flow']['sample_count'] = int(np.sum(mid_mask))
        stratified_metrics['mid_flow']['flow_range'] = f"{y_true[mid_mask].min():.3f}-{y_true[mid_mask].max():.3f}"

    # é«˜å¾„æµ
    if np.sum(high_mask) > 0:
        stratified_metrics['high_flow'] = compute_all_metrics(y_true[high_mask], y_pred[high_mask])
        stratified_metrics['high_flow']['sample_count'] = int(np.sum(high_mask))
        stratified_metrics['high_flow']['flow_range'] = f"{y_true[high_mask].min():.3f}-{y_true[high_mask].max():.3f}"

    return stratified_metrics


def compute_peak_flow_metrics(y_true: Union[torch.Tensor, np.ndarray],
                             y_pred: Union[torch.Tensor, np.ndarray],
                             peak_threshold: float = 0.9) -> Dict[str, float]:
    """
    è®¡ç®—å³°å€¼å¾„æµé¢„æµ‹å‡†ç¡®æ€§æŒ‡æ ‡

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        peak_threshold: å³°å€¼é˜ˆå€¼ï¼ˆåˆ†ä½æ•°ï¼‰

    Returns:
        å³°å€¼é¢„æµ‹æŒ‡æ ‡
    """
    if isinstance(y_true, torch.Tensor):
        y_true = y_true.cpu().numpy()
    if isinstance(y_pred, torch.Tensor):
        y_pred = y_pred.cpu().numpy()

    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]

    if len(y_true) == 0:
        return {}

    # ç¡®å®šå³°å€¼é˜ˆå€¼
    threshold_value = np.quantile(y_true, peak_threshold)
    peak_mask = y_true >= threshold_value

    if np.sum(peak_mask) == 0:
        return {'peak_count': 0}

    # å³°å€¼é¢„æµ‹æŒ‡æ ‡
    peak_true = y_true[peak_mask]
    peak_pred = y_pred[peak_mask]

    # å³°å€¼æ£€æµ‹å‡†ç¡®æ€§
    pred_peaks = y_pred >= threshold_value
    true_positives = np.sum(peak_mask & pred_peaks)
    false_positives = np.sum(~peak_mask & pred_peaks)
    false_negatives = np.sum(peak_mask & ~pred_peaks)

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    # å³°å€¼é‡çº§é¢„æµ‹å‡†ç¡®æ€§
    peak_r2 = compute_r2(peak_true, peak_pred)
    peak_rmse = compute_rmse(peak_true, peak_pred)
    peak_bias = compute_bias(peak_true, peak_pred)

    return {
        'peak_count': int(np.sum(peak_mask)),
        'peak_threshold': float(threshold_value),
        'peak_precision': float(precision),
        'peak_recall': float(recall),
        'peak_f1_score': float(f1_score),
        'peak_r2': float(peak_r2),
        'peak_rmse': float(peak_rmse),
        'peak_bias': float(peak_bias),
        'peak_mean_true': float(np.mean(peak_true)),
        'peak_mean_pred': float(np.mean(peak_pred))
    }


def compute_comprehensive_metrics(y_true: Union[torch.Tensor, np.ndarray],
                                 y_pred: Union[torch.Tensor, np.ndarray]) -> Dict[str, Any]:
    """
    è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ•´ä½“ã€åˆ†å±‚å’Œå³°å€¼æŒ‡æ ‡

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼

    Returns:
        ç»¼åˆæŒ‡æ ‡å­—å…¸
    """
    from typing import Any

    # æ•´ä½“æŒ‡æ ‡
    overall_metrics = compute_all_metrics(y_true, y_pred)

    # åˆ†å±‚æŒ‡æ ‡
    stratified_metrics = compute_stratified_metrics(y_true, y_pred)

    # å³°å€¼æŒ‡æ ‡
    peak_metrics = compute_peak_flow_metrics(y_true, y_pred)

    return {
        'overall': overall_metrics,
        'stratified': stratified_metrics,
        'peak_flow': peak_metrics
    }