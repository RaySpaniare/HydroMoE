"""
ä¸“å®¶ç½‘ç»œæ¨¡å— - ä¸åŒç±»åž‹çš„ä¸“å®¶ç½‘ç»œå®žçŽ°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from MoE_attention import RMSNorm


class BaseExpert(nn.Module, ABC):
    """ä¸“å®¶ç½‘ç»œåŸºç±»"""
    
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        pass


class MLPExpert(BaseExpert):
    """å¤šå±‚æ„ŸçŸ¥æœºä¸“å®¶"""
    
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 hidden_dim: int = 256,
                 num_layers: int = 2,
                 dropout: float = 0.1,
                 activation: str = 'relu'):
        super().__init__(input_dim, output_dim)
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # ðŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨inplaceæ¿€æ´»å‡½æ•°ï¼ŒèŠ‚çœæ˜¾å­˜
        if activation == 'relu':
            self.activation = nn.ReLU(inplace=True)
        elif activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'swish':
            self.activation = nn.SiLU(inplace=True)
        else:
            self.activation = nn.ReLU(inplace=True)
        
        # æž„å»ºç½‘ç»œå±‚
        layers = []
        
        # è¾“å…¥å±‚
        layers.extend([
            nn.Linear(input_dim, hidden_dim),
            self.activation,
            nn.Dropout(dropout)
        ])
        
        # éšè—å±‚
        for _ in range(num_layers - 1):
            layers.extend([
                nn.Linear(hidden_dim, hidden_dim),
                self.activation,
                nn.Dropout(dropout)
            ])
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(hidden_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class ConvExpert(BaseExpert):
    """å·ç§¯ä¸“å®¶ - é€‚ç”¨äºŽæ—¶é—´åºåˆ—æ¨¡å¼è¯†åˆ«"""
    
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 hidden_channels: int = 64,
                 kernel_sizes: list = [3, 5, 7],
                 dropout: float = 0.1):
        super().__init__(input_dim, output_dim)
        
        self.hidden_channels = hidden_channels
        self.kernel_sizes = kernel_sizes
        
        # å¤šå°ºåº¦å·ç§¯åˆ†æ”¯
        self.conv_branches = nn.ModuleList()
        for kernel_size in kernel_sizes:
            branch = nn.Sequential(
                nn.Conv1d(input_dim, hidden_channels, kernel_size, padding=kernel_size//2),
                nn.BatchNorm1d(hidden_channels),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Conv1d(hidden_channels, hidden_channels//2, 1),
                nn.BatchNorm1d(hidden_channels//2),
                nn.ReLU()
            )
            self.conv_branches.append(branch)
        
        # ç‰¹å¾èžåˆ
        total_channels = len(kernel_sizes) * (hidden_channels // 2)
        self.feature_fusion = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(total_channels, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, output_dim)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Conv1d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, input_dim] æˆ– [batch_size, seq_len, input_dim]
            
        Returns:
            output: [batch_size, output_dim]
        """
        # å¤„ç†è¾“å…¥ç»´åº¦
        if len(x.shape) == 2:
            # å¦‚æžœæ˜¯2Dï¼Œæ·»åŠ åºåˆ—ç»´åº¦
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
        
        # è½¬ç½®ä¸ºå·ç§¯æ ¼å¼ [batch_size, input_dim, seq_len]
        x = x.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯
        branch_outputs = []
        for branch in self.conv_branches:
            branch_out = branch(x)  # [batch_size, hidden_channels//2, seq_len]
            branch_outputs.append(branch_out)
        
        # æ‹¼æŽ¥æ‰€æœ‰åˆ†æ”¯
        concat_features = torch.cat(branch_outputs, dim=1)  # [batch_size, total_channels, seq_len]
        
        # ç‰¹å¾èžåˆå’Œè¾“å‡º
        output = self.feature_fusion(concat_features)
        
        return output


class AttentionExpert(BaseExpert):
    """æ³¨æ„åŠ›ä¸“å®¶ - ä¸“æ³¨äºŽæ—¶é—´åºåˆ—çš„å…³é”®ç‰¹å¾"""
    
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 hidden_dim: int = 128,
                 num_heads: int = 4,
                 dropout: float = 0.1):
        super().__init__(input_dim, output_dim)
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)

        # å½’ä¸€åŒ–ï¼ˆPre-RMSNormï¼‰
        self.norm1 = RMSNorm(hidden_dim)
        self.norm2 = RMSNorm(hidden_dim)

        # è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # è¾“å‡ºå±‚ï¼ˆä¸å†é¢å¤–åšLayerNormï¼‰
        self.output_layer = nn.Linear(hidden_dim, output_dim)

        self.dropout = nn.Dropout(dropout)
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, input_dim] æˆ– [batch_size, seq_len, input_dim]
            
        Returns:
            output: [batch_size, output_dim]
        """
        # å¤„ç†è¾“å…¥ç»´åº¦
        if len(x.shape) == 2:
            # å¦‚æžœæ˜¯2Dï¼Œæ·»åŠ åºåˆ—ç»´åº¦
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
        
        batch_size, seq_len, _ = x.shape
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆPre-Normï¼‰
        x_proj = self.input_projection(x)  # [batch, seq, hidden]
        attn_input = self.norm1(x_proj)
        attn_output, _ = self.self_attention(attn_input, attn_input, attn_input)
        x = x_proj + self.dropout(attn_output)

        # å‰é¦ˆç½‘ç»œï¼ˆPre-Normï¼‰
        ffn_input = self.norm2(x)
        ffn_output = self.ffn(ffn_input)
        x = x + self.dropout(ffn_output)
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼ˆå¦‚æžœæœ‰å¤šä¸ªæ—¶é—´æ­¥ï¼‰
        if seq_len > 1:
            x = x.mean(dim=1)  # [batch_size, hidden_dim]
        else:
            x = x.squeeze(1)  # [batch_size, hidden_dim]
        
        # è¾“å‡º
        output = self.output_layer(x)
        return output


class HydrologySpecificExpert(BaseExpert):
    """æ°´æ–‡å­¦ä¸“ç”¨ä¸“å®¶ - é›†æˆæ°´æ–‡å­¦å…ˆéªŒçŸ¥è¯†"""
    
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 expert_type: str = 'runoff',
                 hidden_dim: int = 128,
                 dropout: float = 0.1):
        super().__init__(input_dim, output_dim)
        
        self.expert_type = expert_type
        self.hidden_dim = hidden_dim
        
        # æ ¹æ®ä¸“å®¶ç±»åž‹è®¾è®¡ä¸åŒçš„ç½‘ç»œç»“æž„
        if expert_type == 'runoff':
            # å¾„æµä¸“å®¶ï¼šå…³æ³¨é™æ°´ã€åœŸå£¤æ¹¿åº¦ã€åœ°å½¢ç‰¹å¾
            self.feature_extractor = self._build_runoff_extractor()
        elif expert_type == 'evapotranspiration':
            # è’¸æ•£å‘ä¸“å®¶ï¼šå…³æ³¨æ¸©åº¦ã€æ¹¿åº¦ã€å¤ªé˜³è¾å°„
            self.feature_extractor = self._build_et_extractor()
        elif expert_type == 'snowmelt':
            # èžé›ªä¸“å®¶ï¼šå…³æ³¨æ¸©åº¦æ¢¯åº¦ã€é›ªæ·±ã€èƒ½é‡å¹³è¡¡
            self.feature_extractor = self._build_snow_extractor()
        elif expert_type == 'baseflow':
            # åŸºæµä¸“å®¶ï¼šå…³æ³¨åœ°ä¸‹æ°´ã€é•¿æœŸè¶‹åŠ¿
            self.feature_extractor = self._build_baseflow_extractor()
        else:
            # é€šç”¨ä¸“å®¶
            self.feature_extractor = self._build_general_extractor()
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        self._init_weights()
    
    def _build_runoff_extractor(self):
        """æž„å»ºå¾„æµç‰¹å¾æå–å™¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            # éžçº¿æ€§å˜æ¢æ¨¡æ‹Ÿé™æ°´-å¾„æµå…³ç³»
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.Tanh(),  # é™åˆ¶è¾“å‡ºèŒƒå›´
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
    
    def _build_et_extractor(self):
        """æž„å»ºè’¸æ•£å‘ç‰¹å¾æå–å™¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            # æ¨¡æ‹Ÿå½­æ›¼æ–¹ç¨‹çš„éžçº¿æ€§å…³ç³»
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.Sigmoid(),  # è’¸æ•£å‘æ€»æ˜¯æ­£å€¼
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
    
    def _build_snow_extractor(self):
        """æž„å»ºèžé›ªç‰¹å¾æå–å™¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            # åº¦æ—¥å› å­æ¨¡åž‹çš„éžçº¿æ€§æ‰©å±•
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),  # èžé›ªé€ŸçŽ‡éžè´Ÿ
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
    
    def _build_baseflow_extractor(self):
        """æž„å»ºåŸºæµç‰¹å¾æå–å™¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            # æŒ‡æ•°è¡°å‡ç‰¹å¾æ¨¡æ‹Ÿåœ°ä¸‹æ°´é‡Šæ”¾
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ELU(),  # å¹³æ»‘çš„æŒ‡æ•°ç‰¹å¾
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
    
    def _build_general_extractor(self):
        """æž„å»ºé€šç”¨ç‰¹å¾æå–å™¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ç‰¹å¾æå–
        features = self.feature_extractor(x)
        
        # è¾“å‡ºé¢„æµ‹
        output = self.output_layer(features)
        
        return output


def create_expert(expert_type: str,
                 input_dim: int,
                 output_dim: int,
                 config: Dict[str, Any]) -> BaseExpert:
    """
    ä¸“å®¶å·¥åŽ‚å‡½æ•°

    Args:
        expert_type: ä¸“å®¶ç±»åž‹ ('mlp', 'conv', 'attention', 'hydrology', 'flow_regime', 'seasonal')
        input_dim: è¾“å…¥ç»´åº¦
        output_dim: è¾“å‡ºç»´åº¦
        config: é…ç½®å‚æ•°

    Returns:
        åˆ›å»ºçš„ä¸“å®¶ç½‘ç»œ
    """
    if expert_type == 'mlp':
        return MLPExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            hidden_dim=config.get('hidden_dim', 256),
            num_layers=config.get('num_layers', 2),
            dropout=config.get('dropout', 0.1),
            activation=config.get('activation', 'relu')
        )

    elif expert_type == 'conv':
        return ConvExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            hidden_channels=config.get('hidden_channels', 64),
            kernel_sizes=config.get('kernel_sizes', [3, 5, 7]),
            dropout=config.get('dropout', 0.1)
        )

    elif expert_type == 'attention':
        return AttentionExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            hidden_dim=config.get('hidden_dim', 128),
            num_heads=config.get('num_heads', 4),
            dropout=config.get('dropout', 0.1)
        )

    elif expert_type == 'hydrology':
        return HydrologySpecificExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            expert_type=config.get('hydrology_type', 'runoff'),
            hidden_dim=config.get('hidden_dim', 128),
            dropout=config.get('dropout', 0.1)
        )

    elif expert_type == 'flow_regime':
        return FlowRegimeExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            regime_type=config.get('regime_type', 'medium'),
            hidden_dim=config.get('hidden_dim', 128),
            dropout=config.get('dropout', 0.1)
        )

    elif expert_type == 'seasonal':
        return SeasonalExpert(
            input_dim=input_dim,
            output_dim=output_dim,
            season_type=config.get('season_type', 'spring'),
            hidden_dim=config.get('hidden_dim', 128),
            dropout=config.get('dropout', 0.1)
        )

    else:
        raise ValueError(f"Unknown expert type: {expert_type}")


class FlowRegimeExpert(BaseExpert):
    """æµé‡åˆ†çº§ä¸“å®¶ - ä¸“é—¨å¤„ç†ä¸åŒæµé‡çº§åˆ«"""

    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 regime_type: str = 'low',  # 'low', 'medium', 'high', 'extreme'
                 hidden_dim: int = 128,
                 dropout: float = 0.1):
        super().__init__(input_dim, output_dim)

        self.regime_type = regime_type
        self.hidden_dim = hidden_dim

        # æ ¹æ®æµé‡çº§åˆ«è®¾è®¡ä¸åŒçš„ç½‘ç»œç»“æž„
        if regime_type == 'low':
            # ä½Žæµé‡ä¸“å®¶ï¼šå…³æ³¨åŸºæµã€è’¸æ•£å‘
            self.network = self._build_low_flow_network()
        elif regime_type == 'medium':
            # ä¸­ç­‰æµé‡ä¸“å®¶ï¼šå…³æ³¨å¸¸è§„é™æ°´-å¾„æµå…³ç³»
            self.network = self._build_medium_flow_network()
        elif regime_type == 'high':
            # é«˜æµé‡ä¸“å®¶ï¼šå…³æ³¨æ´ªå³°ã€å¿«é€Ÿå“åº”
            self.network = self._build_high_flow_network()
        elif regime_type == 'extreme':
            # æžç«¯æµé‡ä¸“å®¶ï¼šå…³æ³¨æžç«¯äº‹ä»¶
            self.network = self._build_extreme_flow_network()
        else:
            raise ValueError(f"Unknown regime type: {regime_type}")

        self.dropout = nn.Dropout(dropout)
        self._init_weights()

    def _build_low_flow_network(self):
        """æž„å»ºä½Žæµé‡ç½‘ç»œ - å¹³æ»‘ã€ç¨³å®šçš„å“åº”"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.Tanh(),  # å¹³æ»‘æ¿€æ´»
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.ELU(),   # å¹³æ»‘çš„æŒ‡æ•°ç‰¹å¾ï¼Œé€‚åˆåŸºæµ
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim // 2, self.output_dim),
            nn.Softplus()  # ç¡®ä¿æ­£å€¼è¾“å‡º
        )

    def _build_medium_flow_network(self):
        """æž„å»ºä¸­ç­‰æµé‡ç½‘ç»œ - æ ‡å‡†çš„éžçº¿æ€§å“åº”"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.hidden_dim // 2, self.output_dim)
        )

    def _build_high_flow_network(self):
        """æž„å»ºé«˜æµé‡ç½‘ç»œ - å¿«é€Ÿå“åº”ã€éžçº¿æ€§å¢žå¼º"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.GELU(),  # æ›´å¼ºçš„éžçº¿æ€§
            nn.Dropout(0.15),
            nn.Linear(self.hidden_dim, self.hidden_dim * 2),  # æ›´å¤§å®¹é‡
            nn.GELU(),
            nn.Dropout(0.15),
            nn.Linear(self.hidden_dim * 2, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.output_dim)
        )

    def _build_extreme_flow_network(self):
        """æž„å»ºæžç«¯æµé‡ç½‘ç»œ - å¤„ç†å¼‚å¸¸å€¼å’Œæžç«¯å“åº”"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.LeakyReLU(0.2),  # å…è®¸è´Ÿå€¼ä¼ æ’­
            nn.Dropout(0.2),
            nn.Linear(self.hidden_dim, self.hidden_dim * 2),
            nn.Swish(),  # å¹³æ»‘ä½†æœ‰ç•Œçš„æ¿€æ´»
            nn.Dropout(0.2),
            nn.Linear(self.hidden_dim * 2, self.hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(self.hidden_dim, self.output_dim),
            nn.ReLU()  # ç¡®ä¿éžè´Ÿè¾“å‡º
        )

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.regime_type == 'low':
                    # ä½Žæµé‡ï¼šå°æƒé‡ï¼Œç¨³å®šåˆå§‹åŒ–
                    nn.init.xavier_uniform_(module.weight, gain=0.3)
                elif self.regime_type == 'extreme':
                    # æžç«¯æµé‡ï¼šè¾ƒå¤§æƒé‡ï¼Œå¢žå¼ºè¡¨è¾¾èƒ½åŠ›
                    nn.init.xavier_uniform_(module.weight, gain=1.0)
                else:
                    # ä¸­ç­‰å’Œé«˜æµé‡ï¼šæ ‡å‡†åˆå§‹åŒ–
                    nn.init.xavier_uniform_(module.weight, gain=0.5)

                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class SeasonalExpert(BaseExpert):
    """å­£èŠ‚æ€§ä¸“å®¶ - å¤„ç†å­£èŠ‚æ€§æ°´æ–‡è¿‡ç¨‹"""

    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 season_type: str = 'spring',  # 'spring', 'summer', 'autumn', 'winter'
                 hidden_dim: int = 128,
                 dropout: float = 0.1):
        super().__init__(input_dim, output_dim)

        self.season_type = season_type
        self.hidden_dim = hidden_dim

        # å­£èŠ‚ç‰¹å®šçš„ç½‘ç»œç»“æž„
        if season_type == 'spring':
            # æ˜¥å­£ï¼šèžé›ªã€é™æ°´å¢žåŠ 
            self.network = self._build_spring_network()
        elif season_type == 'summer':
            # å¤å­£ï¼šé«˜è’¸æ•£å‘ã€é›·æš´
            self.network = self._build_summer_network()
        elif season_type == 'autumn':
            # ç§‹å­£ï¼šç¨³å®šé™æ°´ã€è’¸æ•£å‘å‡å°‘
            self.network = self._build_autumn_network()
        elif season_type == 'winter':
            # å†¬å­£ï¼šé›ªç§¯ç´¯ã€ä½Žè’¸æ•£å‘
            self.network = self._build_winter_network()
        else:
            raise ValueError(f"Unknown season type: {season_type}")

        self.dropout = nn.Dropout(dropout)
        self._init_weights()

    def _build_spring_network(self):
        """æ˜¥å­£ç½‘ç»œ - å¤„ç†èžé›ªå’Œé™æ°´"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            # æ¸©åº¦æ•æ„Ÿå±‚ï¼ˆèžé›ªï¼‰
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.Sigmoid(),  # æ¸©åº¦é˜ˆå€¼æ•ˆåº”
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.output_dim)
        )

    def _build_summer_network(self):
        """å¤å­£ç½‘ç»œ - å¤„ç†é«˜è’¸æ•£å‘å’Œé›·æš´"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.GELU(),  # éžçº¿æ€§è’¸æ•£å‘å…³ç³»
            nn.Dropout(0.15),
            # è’¸æ•£å‘æŠ‘åˆ¶å±‚
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.Tanh(),  # æœ‰ç•Œæ¿€æ´»ï¼Œæ¨¡æ‹Ÿè’¸æ•£å‘ä¸Šé™
            nn.Dropout(0.15),
            nn.Linear(self.hidden_dim, self.output_dim)
        )

    def _build_autumn_network(self):
        """ç§‹å­£ç½‘ç»œ - ç¨³å®šçš„æ°´æ–‡è¿‡ç¨‹"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.hidden_dim // 2, self.output_dim)
        )

    def _build_winter_network(self):
        """å†¬å­£ç½‘ç»œ - å¤„ç†é›ªç§¯ç´¯å’Œä½Žæ´»åŠ¨"""
        return nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.LeakyReLU(0.1),  # ä½Žæ´»åŠ¨æœŸçš„å°æ¢¯åº¦
            nn.Dropout(0.05),   # è¾ƒå°‘dropoutï¼Œä¿æŒç¨³å®š
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ELU(),  # å¹³æ»‘æ¿€æ´»
            nn.Dropout(0.05),
            nn.Linear(self.hidden_dim, self.output_dim),
            nn.Softplus()  # ç¡®ä¿éžè´Ÿ
        )

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.season_type in ['winter', 'autumn']:
                    # å†¬ç§‹å­£ï¼šè¾ƒå°æƒé‡ï¼Œç¨³å®šå“åº”
                    nn.init.xavier_uniform_(module.weight, gain=0.3)
                else:
                    # æ˜¥å¤å­£ï¼šæ ‡å‡†æƒé‡
                    nn.init.xavier_uniform_(module.weight, gain=0.5)

                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)