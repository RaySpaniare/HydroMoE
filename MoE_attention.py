"""
æ³¨æ„åŠ›æœºåˆ¶æ¨¡å— - æ°´æ–‡æ—¶é—´åºåˆ—çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


class RMSNorm(nn.Module):
    """RMSNormå®ç°ï¼Œæ¯”LayerNormæ›´ç¨³å®šä¸”è®¡ç®—æ›´é«˜æ•ˆ"""
    def __init__(self, dim: int, eps: float = 1e-8):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # å‡æ–¹æ ¹å½’ä¸€åŒ–ï¼šx / rms(x)
        norm = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()
        return self.weight * (x * norm)


def _make_norm(norm_type: str, dim: int):
    if norm_type.lower() == 'rms':
        return RMSNorm(dim)
    else:
        return nn.LayerNorm(dim)


class MultiHeadAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ - ä¸“ä¸ºæ—¶é—´åºåˆ—è®¾è®¡"""
    
    def __init__(self, 
                 d_model: int,
                 n_heads: int = 8,
                 dropout: float = 0.1,
                 temperature: float = 1.0,
                 pre_norm: bool = True,
                 norm_type: str = 'rms'):
        super().__init__()
        
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.temperature = temperature
        
        # çº¿æ€§å˜æ¢å±‚
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.pre_norm = pre_norm
        self.norm = _make_norm(norm_type, d_model)
        
        # æ•°å€¼ç¨³å®šæ€§åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–"""
        for module in [self.w_q, self.w_k, self.w_v, self.w_o]:
            nn.init.xavier_uniform_(module.weight, gain=0.5)
            if hasattr(module, 'bias') and module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len] å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
            
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.shape
        
        # æ®‹ä½™è¿æ¥è¾“å…¥
        residual = x

        # Pre-Normï¼šå…ˆå½’ä¸€åŒ–å†è¿›å…¥æ³¨æ„åŠ›
        x_in = self.norm(x) if self.pre_norm else x
        
        # 1. çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V
        Q = self.w_q(x_in)  # [batch, seq_len, d_model]
        K = self.w_k(x_in)
        V = self.w_v(x_in)
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # [batch, n_heads, seq_len, d_k]
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›
        attention_output = self._scaled_dot_product_attention(Q, K, V, mask)
        
        # 4. åˆå¹¶å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # 5. è¾“å‡ºçº¿æ€§å˜æ¢
        output = self.w_o(attention_output)
        output = self.dropout(output)
        
        # æ®‹å·®
        output = residual + output
        
        # Post-Normï¼ˆå¦‚é€‰æ‹©åå½’ä¸€åŒ–ï¼‰
        if not self.pre_norm:
            output = self.norm(output)
        
        return output
    
    def _scaled_dot_product_attention(self, 
                                    Q: torch.Tensor, 
                                    K: torch.Tensor, 
                                    V: torch.Tensor,
                                    mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› - ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨PyTorch 2.0+çš„scaled_dot_product_attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(F, 'scaled_dot_product_attention') and mask is None:
            # ä½¿ç”¨åŸç”Ÿå®ç°ï¼Œæ›´é«˜æ•ˆä¸”èŠ‚çœæ˜¾å­˜
            context = F.scaled_dot_product_attention(
                Q, K, V,
                dropout_p=self.dropout.p if self.training else 0.0,
                is_causal=False
            )
            return context
        
        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (math.sqrt(self.d_k) * self.temperature)
        
        # 2. åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            mask = mask.unsqueeze(1).expand(-1, self.n_heads, -1, -1)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 3. Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        
        # ğŸš€ ä¼˜åŒ–ï¼šç®€åŒ–clampæ“ä½œ
        attention_weights = self.dropout(attention_weights)
        
        # 4. åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_weights, V)
        
        return context


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - ä¸ºæ—¶é—´åºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯"""
    
    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            x + positional encoding
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]


class HydroAttentionBlock(nn.Module):
    """æ°´æ–‡æ³¨æ„åŠ›å— - é›†æˆä½ç½®ç¼–ç å’Œå¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self,
                 d_model: int,
                 n_heads: int = 8,
                 dropout: float = 0.1,
                 max_seq_len: int = 512,
                 pre_norm: bool = True,
                 norm_type: str = 'rms'):
        super().__init__()
        
        self.d_model = d_model
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, n_heads, dropout, pre_norm=pre_norm, norm_type=norm_type)
        
        # å‰é¦ˆç½‘ç»œï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼‰
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, d_model),
            nn.Dropout(dropout)
        )
        
        self.pre_norm = pre_norm
        self.norm2 = _make_norm(norm_type, d_model)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥ç‰¹å¾ â†’ ä½ç½®ç¼–ç  â†’ è‡ªæ³¨æ„åŠ› â†’ å‰é¦ˆç½‘ç»œ
        
        Args:
            x: [batch_size, seq_len, input_dim]
            mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
            
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        # 1. ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        
        # 2. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ï¼ˆå½’ä¸€åŒ–åœ¨æ³¨æ„åŠ›å†…éƒ¨å¤„ç†ï¼‰
        attn_output = self.attention(x, mask)
        
        # 3. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        residual = attn_output
        x_ffn_in = self.norm2(attn_output) if self.pre_norm else attn_output
        ffn_output = self.ffn(x_ffn_in)
        output = residual + ffn_output
        if not self.pre_norm:
            output = self.norm2(output)
        
        return output


def create_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹"""
    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
    return mask


def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:
    """åˆ›å»ºå¡«å……æ©ç """
    batch_size = lengths.size(0)
    mask = torch.arange(max_len, device=lengths.device).expand(
        batch_size, max_len
    ) < lengths.unsqueeze(1)
    return mask