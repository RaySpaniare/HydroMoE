"""
æ··åˆHydroMoEæ¨¡å‹ - é›†æˆPBMç‰©ç†æ¨¡å—å’Œç¥ç»ç½‘ç»œä¸“å®¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Any, Tuple
import math
import sys
import os

from MoE_attention import HydroAttentionBlock
from MoE_gate import MoEGate, ExpertDispatcher, ExpertCombiner
from MoE_experts import MLPExpert, BaseExpert
from MoE_pbm import OptimizedPBM
from MoE_cmaes_loader import CMAESParamLoader


class PBMExpert(BaseExpert):
    """PBMä¸“å®¶ - åŒ…è£…ç‰©ç†æ¨¡å‹"""
    
    def __init__(self, input_dim: int, output_dim: int, module_type: str = 'runoff'):
        super().__init__(input_dim, output_dim)
        
        self.module_type = module_type
        
        # å¼ºåˆ¶ä½¿ç”¨CMA-ESå‚æ•°ï¼Œç¡®ä¿PBMä¸“å®¶æ­£ç¡®åˆå§‹åŒ–
        try:
            # ä½¿ç”¨CMA-ESå‚æ•°ç›´æ¥è®¡ç®—PBM
            self.pbm = OptimizedPBM(
                config={'use_precomputed_pbm': False},  # ç›´æ¥è®¡ç®—ï¼Œä¸ä¾èµ–ç»“æœæ–‡ä»¶
                cmaes_loader=CMAESParamLoader()
            )
            self.use_simple_nn = False
            
            # éªŒè¯PBMæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            self._validate_pbm_initialization()
            print(f"âœ… {module_type}æ¨¡å—PBMä¸“å®¶åˆå§‹åŒ–æˆåŠŸï¼ŒåŒ…å«{len(self.pbm.cmaes_loader.params_data)}ä¸ªç«™ç‚¹å‚æ•°")
            
        except Exception as e:
            print(f"  âš ï¸ {module_type}æ¨¡å—PBMåˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"  ğŸ”§ ä½¿ç”¨ä¿®å¤ç­–ç•¥é‡æ–°åˆå§‹åŒ–...")
            
            # ä¿®å¤ç­–ç•¥ï¼šç¡®ä¿CMA-ESå‚æ•°æ­£ç¡®åŠ è½½
            try:
                cmaes_loader = CMAESParamLoader()
                if cmaes_loader.params_data and len(cmaes_loader.params_data) > 0:
                    self.pbm = OptimizedPBM(
                        config={'use_precomputed_pbm': False},
                        cmaes_loader=cmaes_loader
                    )
                    self.use_simple_nn = False
                    print(f"  âœ… {module_type}æ¨¡å—PBMä¸“å®¶ä¿®å¤æˆåŠŸ")
                else:
                    raise Exception("CMA-ESå‚æ•°ä¸ºç©º")
            except:
                print(f"  âŒ {module_type}æ¨¡å—å½»åº•å¤±è´¥ï¼Œä½¿ç”¨NNæ›¿ä»£")
                # ä½¿ç”¨ç®€å•çš„ç¥ç»ç½‘ç»œæ›¿ä»£PBM
                self.pbm = nn.Sequential(
                    nn.Linear(input_dim, 64),
                    nn.ReLU(),
                    nn.Linear(64, 32),
                    nn.ReLU(),
                    nn.Linear(32, 1)
                )
                self.use_simple_nn = True
        
        # è¾“å‡ºæ˜ å°„å±‚ï¼ˆå°†PBMè¾“å‡ºæ˜ å°„åˆ°æ ‡å‡†è¾“å‡ºç»´åº¦ï¼‰
        self.output_projection = nn.Linear(1, output_dim)
        
    def _validate_pbm_initialization(self):
        """éªŒè¯PBMæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–"""
        if not hasattr(self.pbm, 'cmaes_loader'):
            raise Exception("PBMç¼ºå°‘CMA-ESåŠ è½½å™¨")
        
        if not self.pbm.cmaes_loader.params_data:
            raise Exception("CMA-ESå‚æ•°æ•°æ®ä¸ºç©º")
        
        # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬ç«™ç‚¹çš„å‚æ•°
        sample_station = list(self.pbm.cmaes_loader.params_data.keys())[0]
        sample_params = self.pbm.cmaes_loader.params_data[sample_station]
        
        # éªŒè¯ç«™ç‚¹æ•°æ®ç»“æ„
        if 'best_params' not in sample_params:
            raise Exception(f"ç«™ç‚¹ {sample_station} ç¼ºå°‘best_paramså­—æ®µ")
        
        best_params = sample_params['best_params']
        
        # éªŒè¯æ ¸å¿ƒCMA-ESå‚æ•°å­˜åœ¨ï¼ˆç›´æ¥æ¥è‡ªä¼˜åŒ–ç»“æœï¼‰
        required_cmaes_params = ['wmin', 'wmax', 'beta', 'baseflow_threshold']
        missing_params = [p for p in required_cmaes_params if p not in best_params]
        if missing_params:
            raise Exception(f"ç¼ºå°‘å…³é”®CMA-ESå‚æ•°: {missing_params}")
        
        print(f"    âœ… PBMå‚æ•°éªŒè¯é€šè¿‡ï¼ŒåŒ…å«{len(self.pbm.cmaes_loader.params_data)}ä¸ªç«™ç‚¹å‚æ•°")
        print(f"    ğŸ“Š æ ·æœ¬ç«™ç‚¹ {sample_station} å‚æ•°æ•°é‡: {len(best_params)}")
        
        # æµ‹è¯•å‚æ•°è½¬æ¢
        converted_params = self.pbm.cmaes_loader.get_station_params(sample_station)
        if not converted_params:
            raise Exception("å‚æ•°è½¬æ¢å¤±è´¥")
            
        print(f"    ğŸ”„ å‚æ•°è½¬æ¢æˆåŠŸï¼ŒåŒ…å« {len(converted_params)} ä¸ªå‚æ•°ç»„")
        
    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            **kwargs: å¯èƒ½åŒ…å«station_idsç­‰ä¿¡æ¯
        """
        batch_size = x.shape[0]
        device = x.device
        
        if self.use_simple_nn:
            # ä½¿ç”¨ç®€å•ç¥ç»ç½‘ç»œ
            output = self.pbm(x)
            output = self.output_projection(output)
        else:
            # ä½¿ç”¨å®Œæ•´PBM
            # ä¼˜å…ˆä»kwargsä¸­è·å–æœªæ ‡å‡†åŒ–çš„ç‰©ç†é©±åŠ¨(raw_features_last: [batch, 3])
            raw_feats = kwargs.get('raw_features_last', None)
            if raw_feats is not None:
                # rawé¡ºåºä¸æ•°æ®é›†feature_colsä¸€è‡´: [pet, precip, temp]
                pet = raw_feats[:, 0]
                precip = raw_feats[:, 1]
                temp = raw_feats[:, 2]
            else:
                # å›é€€ï¼šä»æ¨¡å—è¾“å…¥ä¸­å–å‰ä¸‰ç»´ï¼ˆå¯èƒ½æ˜¯ç¼–ç ç‰¹å¾ï¼Œç‰©ç†æ„ä¹‰è¾ƒå¼±ï¼‰
                pet = x[:, 0] if x.shape[1] > 0 else torch.zeros(batch_size, device=device)
                precip = x[:, 1] if x.shape[1] > 1 else torch.zeros(batch_size, device=device)
                temp = x[:, 2] if x.shape[1] > 2 else torch.zeros(batch_size, device=device)

            # æ„å»ºPBMè¾“å…¥ï¼ˆç¡®ä¿ä¸ºè®¾å¤‡ä¸Šçš„ä¸€ç»´å¼ é‡ï¼‰
            pbm_inputs = {
                'precip': precip,
                'temp': temp,
                'pet': pet,
                'time_step': torch.zeros(batch_size, dtype=torch.long, device=device)
            }
            
            # ğŸš€ ä¿®å¤ï¼šè·å–å®é™…çš„ç«™ç‚¹IDå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ç´¢å¼•
            station_ids_str = kwargs.get('station_ids_str', None)
            station_ids_idx = kwargs.get('station_ids', torch.zeros(batch_size, dtype=torch.long, device=device))
            
            # å¦‚æœæœ‰ç«™ç‚¹IDå­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤ç«™ç‚¹
            if station_ids_str is not None and isinstance(station_ids_str, (list, tuple)):
                # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²IDåˆ—è¡¨
                pass
            else:
                # å¦‚æœæ²¡æœ‰å­—ç¬¦ä¸²IDï¼Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„æµ‹è¯•ç«™ç‚¹
                station_ids_str = ["camels_09378630"] * batch_size
            
            # è¿è¡ŒPBM
            with torch.no_grad():  # PBMä¸éœ€è¦æ¢¯åº¦
                pbm_outputs = self.pbm(pbm_inputs, station_ids_idx, station_ids_str)
            
            # æ ¹æ®æ¨¡å—ç±»å‹é€‰æ‹©è¾“å‡º
            if self.module_type == 'snow':
                output = pbm_outputs['snow_output']
            elif self.module_type == 'runoff':
                output = pbm_outputs['runoff_output']
            elif self.module_type == 'et':
                output = pbm_outputs['et_output']
            elif self.module_type == 'drainage':
                output = pbm_outputs['groundwater_output']
            else:
                output = pbm_outputs['runoff_output']  # é»˜è®¤
            
            # æŠ•å½±åˆ°æ ‡å‡†è¾“å‡ºç»´åº¦
            output = self.output_projection(output.unsqueeze(-1))
        
        return output


class ModuleGate(nn.Module):
    """æ¨¡å—é—¨æ§ - åœ¨PBMå’ŒNNä¸“å®¶ä¹‹é—´é€‰æ‹©"""
    
    def __init__(self, input_dim: int, num_experts: int = 2, dropout: float = 0.1, 
                 pbm_min_weight: float = 0.0, top_k: int = 1, temperature: float = 0.7):
        super().__init__()
        
        self.input_dim = input_dim
        self.num_experts = num_experts
        self.pbm_min_weight = pbm_min_weight  # PBMä¸“å®¶æœ€å°æƒé‡çº¦æŸï¼ˆé»˜è®¤ä¸å¯ç”¨ï¼‰
        self.top_k = max(1, min(int(top_k), num_experts))
        self.temperature = max(1e-6, float(temperature))
        
        # ğŸš€ æ”¹è¿›çš„é—¨æ§ç½‘ç»œ - å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œå…è®¸ä¸“å®¶å·®å¼‚åŒ–
        hidden_dim = max(input_dim // 2, 32)
        self.gate = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),  # æ·»åŠ å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ
            nn.GELU(),  # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°ï¼Œæ›´å¥½çš„æ¢¯åº¦ç‰¹æ€§
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_dim // 2, num_experts)
        )
        
        # ğŸš€ ä¸“å®¶åå¥½å¼•å¯¼æœºåˆ¶
        self.expert_bias = nn.Parameter(torch.zeros(num_experts))
        
        # ğŸš€ ä¸“å®¶è´¨é‡è¯„ä¼°æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
        self.enable_quality_guidance = True
        if self.enable_quality_guidance:
            self.quality_tracker = nn.Parameter(torch.ones(num_experts), requires_grad=False)
            self.quality_momentum = 0.95  # åŠ¨é‡ç³»æ•°
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ– - å…è®¸ä¸“å®¶å·®å¼‚åŒ–çš„åˆå§‹åŒ–"""
        for i, module in enumerate(self.modules()):
            if isinstance(module, nn.Linear):
                # ğŸš€ å¢åŠ åˆå§‹åŒ–å¼ºåº¦ï¼Œå…è®¸æ›´å¤§çš„åˆå§‹logitså·®å¼‚
                nn.init.xavier_uniform_(module.weight, gain=0.5)  # ä»0.1å¢åŠ åˆ°0.5
                if module.bias is not None:
                    # ğŸš€ æ·»åŠ å°çš„éšæœºåç½®ï¼Œæ‰“ç ´å¯¹ç§°æ€§
                    nn.init.uniform_(module.bias, -0.1, 0.1)  # è€Œä¸æ˜¯å…¨é›¶åˆå§‹åŒ–
    
    def forward(self, features: torch.Tensor, pbm_output: torch.Tensor, 
                nn_output: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            pbm_output: [batch_size, output_dim] PBMä¸“å®¶è¾“å‡º
            nn_output: [batch_size, output_dim] NNä¸“å®¶è¾“å‡º
            
        Returns:
            æ··åˆè¾“å‡ºå’Œé—¨æ§ä¿¡æ¯
        """
        # è®¡ç®—é—¨æ§æƒé‡
        gate_logits = self.gate(features)  # [batch_size, num_experts]
        
        # ğŸš€ æ·»åŠ ä¸“å®¶åå¥½åç½®ï¼Œé¼“åŠ±å·®å¼‚åŒ–
        gate_logits = gate_logits + self.expert_bias.unsqueeze(0)
        
        # ğŸš€ å¯é€‰ï¼šæ ¹æ®ä¸“å®¶å†å²è´¨é‡è°ƒæ•´logits
        if self.enable_quality_guidance and hasattr(self, 'quality_tracker'):
            # è´¨é‡è¶Šé«˜çš„ä¸“å®¶è·å¾—æ›´é«˜çš„logitsåç½®
            quality_bias = (self.quality_tracker - self.quality_tracker.mean()) * 0.5
            gate_logits = gate_logits + quality_bias.unsqueeze(0)
        
        # ğŸš€ é™ä½æ¸©åº¦ï¼Œå¢å¼ºé€‰æ‹©æ€§ï¼ˆä»0.7é™åˆ°0.3ï¼‰
        effective_temperature = max(0.3, self.temperature)
        gate_weights = F.softmax(gate_logits / effective_temperature, dim=-1)  # [batch_size, num_experts]
        # å½¢çŠ¶ç¨³å¥ï¼šè‹¥é—¨æ§è¾“å‡ºç»´åº¦å¼‚å¸¸ï¼Œå– Top-2 å¹¶å½’ä¸€åŒ–
        if gate_weights.size(-1) != self.num_experts:
            top_k = min(self.num_experts, gate_weights.size(-1))
            top_w, _ = torch.topk(gate_weights, k=top_k, dim=-1)
            gate_weights = top_w / (top_w.sum(dim=-1, keepdim=True) + 1e-8)
        
        # åº”ç”¨PBMæœ€å°æƒé‡çº¦æŸï¼ˆå¦‚å¯ç”¨ï¼‰
        if self.pbm_min_weight > 0:
            pbm_weights = gate_weights[:, 0]
            pbm_weights_constrained = torch.clamp(pbm_weights, min=self.pbm_min_weight)
            rest = torch.clamp(1.0 - pbm_weights_constrained, min=0.0)
            # å°†ä½™é‡å¹³å‡åˆ†é…ç»™å…¶ä½™ä¸“å®¶ï¼ˆå½“å‰ä¸º2ä¸“å®¶æ—¶å³å¦ä¸€ä¸ªï¼‰
            if self.num_experts > 1:
                others = gate_weights[:, 1:]
                others_sum = others.sum(dim=1, keepdim=True) + 1e-8
                others_norm = others / others_sum
                others_new = others_norm * rest
                gate_weights = torch.cat([pbm_weights_constrained.unsqueeze(1), others_new], dim=1)

        # Top-k ç­–ç•¥ï¼šk=1 ä½¿ç”¨ç¡¬é€‰æ‹©ï¼ˆone-hotï¼‰ï¼Œk>1 ä½¿ç”¨è½¯åŠ æƒ
        if self.top_k == 1:
            # Straight-Through Gumbel-Softmaxï¼ˆæ¸©åº¦å¯æ§ï¼‰
            # è®­ç»ƒæ—¶è¿‘ä¼¼ one-hotï¼Œåå‘ç”¨softæ¢¯åº¦ï¼Œé¿å…æ—©æœŸå†»ç»“
            gumbel = -torch.log(-torch.log(torch.rand_like(gate_logits).clamp_(1e-9, 1 - 1e-9)))
            y_soft = F.softmax((gate_logits + gumbel) / self.temperature, dim=-1)
            idx = torch.argmax(y_soft, dim=-1)
            y_hard = torch.zeros_like(y_soft)
            y_hard.scatter_(1, idx.unsqueeze(1), 1.0)
            gate_weights = (y_hard - y_soft).detach() + y_soft
        
        # ä¸“å®¶è¾“å‡ºå †å 
        # ç¡®ä¿ä¸“å®¶è¾“å‡ºä¸º [B, output_dim]
        if pbm_output.dim() == 1:
            pbm_output = pbm_output.unsqueeze(-1)
        if nn_output.dim() == 1:
            nn_output = nn_output.unsqueeze(-1)
        expert_outputs = torch.stack([pbm_output, nn_output], dim=1)  # æœŸæœ› [B, 2, output_dim]
        # è‹¥ä¸“å®¶ç»´åº¦ä¸è¾“å‡ºç»´åº¦è¢«è¯¯ç½®æ¢ï¼Œè‡ªåŠ¨çº æ­£
        if expert_outputs.size(1) != self.num_experts and expert_outputs.size(-1) == self.num_experts:
            expert_outputs = expert_outputs.transpose(1, 2)
        
        # åŠ æƒç»„åˆï¼ˆè‡ªé€‚åº”è¯†åˆ«ä¸“å®¶ç»´åº¦ä½ç½®ï¼Œé¿å…ç»´åº¦é”™ç½®ï¼‰
        if expert_outputs.dim() != 3:
            raise RuntimeError(f"expert_outputs ç»´åº¦å¼‚å¸¸: {expert_outputs.shape}")
        if expert_outputs.size(1) == self.num_experts:
            outputs_expert_first = expert_outputs  # [B, K, D]
        elif expert_outputs.size(2) == self.num_experts:
            outputs_expert_first = expert_outputs.transpose(1, 2)  # [B, K, D]
        else:
            # æ— æ³•è¯†åˆ«ï¼Œå¼ºåˆ¶å°†æœ€åä¸€ç»´è§†ä½œç‰¹å¾ï¼Œç¬¬äºŒç»´èšåˆä¸ºä¸“å®¶æ•°
            if expert_outputs.size(1) != self.num_experts:
                # å°è¯•åˆ‡åˆ°å‰K
                k = min(self.num_experts, expert_outputs.size(1))
                outputs_expert_first = expert_outputs[:, :k, :]
                gate_weights = gate_weights[:, :k]
            else:
                outputs_expert_first = expert_outputs
        # å¯¹é½ gate_weights çš„ä¸“å®¶ç»´åº¦åˆ° outputs_expert_first çš„ K ç»´
        K_out = outputs_expert_first.size(1)
        if gate_weights.size(1) != K_out:
            if gate_weights.size(1) > K_out:
                # å–å‰K_outå¤§çš„æƒé‡å¹¶å½’ä¸€åŒ–
                top_w, _ = torch.topk(gate_weights, k=K_out, dim=-1)
                gate_weights = top_w / (top_w.sum(dim=-1, keepdim=True) + 1e-8)
            else:
                # å¡«å……åˆ°K_outå¹¶å½’ä¸€åŒ–
                B = gate_weights.size(0)
                pad = torch.zeros(B, K_out - gate_weights.size(1), device=gate_weights.device, dtype=gate_weights.dtype)
                gate_weights = torch.cat([gate_weights, pad], dim=-1)
                gate_weights = gate_weights / (gate_weights.sum(dim=-1, keepdim=True) + 1e-8)
        # einsum åšåŠ æƒæ±‚å’Œï¼Œé¿å…æ˜¾å¼ expand
        mixed_output = torch.einsum('bkd,bk->bd', outputs_expert_first, gate_weights)  # [B, D]
        
        # é—¨æ§ä¿¡æ¯
        gate_info = {
            'gate_weights': gate_weights,
            'pbm_weight': gate_weights[:, 0].mean().item(),
            'nn_weight': gate_weights[:, 1].mean().item(),
            'effective_gate': gate_weights
        }
        
        return mixed_output, gate_info


class HybridHydroMoEModel(nn.Module):
    """
    æ··åˆæ°´æ–‡MoEæ¨¡å‹ - é›†æˆPBMç‰©ç†æ¨¡å—å’Œç¥ç»ç½‘ç»œä¸“å®¶
    
    æ¶æ„ï¼šç‰¹å¾ç¼–ç  â†’ è‡ªæ³¨æ„åŠ› â†’ å››ä¸ªæ°´æ–‡æ¨¡å—ï¼ˆPBM+NNï¼‰ â†’ æœ€ç»ˆç»„åˆ
    """
    
    def __init__(self, config):
        super().__init__()
        
        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡é…ç½®
        if isinstance(config, dict):
            self.config = config
            self.model_config = config.get('model', config)
            self.pbm_config = config.get('pbm', {})
        else:
            self.config = config
            self.model_config = getattr(config, 'model', config)
            self.pbm_config = getattr(config, 'pbm', {})
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–å¼€å…³
        self.use_gradient_checkpointing = os.getenv('USE_GRAD_CHECKPOINT', '1').lower() in ['1', 'true', 'yes']
        
        # åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶
        self._initialize_model()
    
    def _get_config_value(self, key, default=None):
        """è¾…åŠ©å‡½æ•°ï¼šä»é…ç½®ä¸­è·å–å€¼ï¼Œå…¼å®¹å­—å…¸å’Œå¯¹è±¡"""
        if isinstance(self.model_config, dict):
            return self.model_config.get(key, default)
        else:
            return getattr(self.model_config, key, default)
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶"""
        # æ¨¡å‹ç»´åº¦å‚æ•°
        self.input_dim = self._get_config_value('input_size', 20)
        self.d_model = self._get_config_value('hidden_size', 128)
        self.output_dim = 1
        
        # è·å–å…¶ä»–é…ç½®å‚æ•°
        dropout = self._get_config_value('dropout', 0.1)
        num_heads = self._get_config_value('num_heads', 8)
        num_attention_layers = self._get_config_value('num_attention_layers', 2)
        max_sequence_length = self._get_config_value('max_sequence_length', 256)
        
        # 1. è¾“å…¥ç‰¹å¾ç¼–ç å™¨
        self.feature_encoder = nn.Sequential(
            nn.Linear(self.input_dim, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 2. è‡ªæ³¨æ„åŠ›æœºåˆ¶å±‚
        self.attention_blocks = nn.ModuleList([
            HydroAttentionBlock(
                d_model=self.d_model,
                n_heads=num_heads,
                dropout=dropout,
                max_seq_len=max_sequence_length
            ) for _ in range(num_attention_layers)
        ])
        
        # 3. å››ä¸ªæ°´æ–‡æ¨¡å—ï¼ˆPBM + NN ä¸“å®¶ï¼‰
        pbm_min_weight = self._get_config_value('pbm_min_weight', 0.0)
        module_gate_top_k = self._get_config_value('module_gate_top_k', 2)  # ğŸš€ ä¿®å¤ï¼šé»˜è®¤å€¼åº”è¯¥æ˜¯2
        
        self.snow_pbm_expert = PBMExpert(self.d_model, self.output_dim, 'snow')
        self.snow_nn_expert = MLPExpert(self.d_model, self.output_dim, 
                                       hidden_dim=self.d_model//2, num_layers=2)
        # ğŸš€ ç»Ÿä¸€ä½¿ç”¨æ–°çš„é—¨æ§é…ç½®
        module_gate_temperature = self._get_config_value('module_gate_temperature', 0.3)
        
        self.snow_gate = ModuleGate(self.d_model, num_experts=2, pbm_min_weight=pbm_min_weight, top_k=module_gate_top_k, temperature=module_gate_temperature)
        
        self.runoff_pbm_expert = PBMExpert(self.d_model, self.output_dim, 'runoff')
        self.runoff_nn_expert = MLPExpert(self.d_model, self.output_dim,
                                         hidden_dim=self.d_model//2, num_layers=2)
        self.runoff_gate = ModuleGate(self.d_model, num_experts=2, pbm_min_weight=pbm_min_weight, top_k=module_gate_top_k, temperature=module_gate_temperature)
        
        self.et_pbm_expert = PBMExpert(self.d_model, self.output_dim, 'et')
        self.et_nn_expert = MLPExpert(self.d_model, self.output_dim,
                                     hidden_dim=self.d_model//2, num_layers=2)
        self.et_gate = ModuleGate(self.d_model, num_experts=2, pbm_min_weight=pbm_min_weight, top_k=module_gate_top_k, temperature=module_gate_temperature)
        
        self.drainage_pbm_expert = PBMExpert(self.d_model, self.output_dim, 'drainage')
        self.drainage_nn_expert = MLPExpert(self.d_model, self.output_dim,
                                           hidden_dim=self.d_model//2, num_layers=2)
        self.drainage_gate = ModuleGate(self.d_model, num_experts=2, pbm_min_weight=pbm_min_weight, top_k=module_gate_top_k, temperature=module_gate_temperature)
        
        # 4. å¢å¼ºæœ€ç»ˆç»„åˆå™¨ - æ‰©å¤§é¢„æµ‹åŠ¨æ€èŒƒå›´
        # ä¿ç•™åŸ MLP å®šä¹‰ä»¥å…¼å®¹ï¼ˆä½†ä¸å†ä½¿ç”¨ä½œä¸ºæœ€ç»ˆç»„åˆï¼‰
        self.final_combiner = nn.Sequential(
            nn.Linear(4 * self.output_dim, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(self.d_model, self.d_model // 2),
            nn.LayerNorm(self.d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(self.d_model // 2, self.d_model // 4),
            nn.LayerNorm(self.d_model // 4),
            nn.GELU(),
            nn.Dropout(dropout * 0.25),
            nn.Linear(self.d_model // 4, self.output_dim)
        )

        # æ–°å¢ï¼šäºŒè·¯å‡¸ç»„åˆæƒé‡å¤´ï¼ˆç”Ÿæˆå¿«æµ/åŸºæµçš„åˆ†é…ç³»æ•° Î±ï¼‰
        alpha_hidden = max(self.d_model // 4, 32)
        # è¾“å…¥ä¸º [module_input(d_model), snow_out, et_out] â†’ 2
        self.alpha_head = nn.Sequential(
            nn.Linear(self.d_model + 2, alpha_hidden),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(alpha_hidden, 2)
        )

        # æ–°å¢ï¼šå¯ç”¨æ°´æ˜ å°„å¤´ï¼ˆå°†åŸå§‹ç‰©ç†é©±åŠ¨æ˜ å°„åˆ°é¢„æµ‹ç©ºé—´çš„ä¸Šé™æ ‡é‡ï¼Œä¿è¯éè´Ÿï¼‰
        self.avail_head = nn.Sequential(
            nn.Linear(3, max(16, self.d_model // 8)),
            nn.GELU(),
            nn.Linear(max(16, self.d_model // 8), 1)
        )

        # 4.1 è¾“å‡ºæ¿€æ´»å±‚ï¼ˆä¿ç•™ä½†åœ¨å‡¸ç»„åˆè·¯å¾„ä¸­ä¸ç›´æ¥ä½¿ç”¨ï¼‰
        self.output_activation = nn.Softplus(beta=0.1)
        
        # åºåˆ—èšåˆæ–¹å¼
        self.sequence_aggregation = self._get_config_value('sequence_aggregation', 'last')

        # 5. å¾„æµåˆ†æœŸ Regime-MoE å¤´ï¼ˆä½/å¹³/æ´ªï¼‰ï¼Œä»¥æ®‹å·®æ–¹å¼ç»†åŒ– base runoffï¼ˆè‡ªç”±é€‰æ‹©ï¼‰
        self.use_regime_moe = True
        regime_hidden = max(self.d_model // 4, 32)
        # ä½¿ç”¨å°å‹ Transformer ç¼–ç å™¨ä»æ³¨æ„åŠ›åºåˆ—ä¸­æå–ä¸Šä¸‹æ–‡
        nheads = min(8, max(1, self.d_model // 64))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=nheads,
            dim_feedforward=max(self.d_model * 2, 64),
            dropout=dropout,
            batch_first=True,
            activation="gelu",
            norm_first=True
        )
        self.regime_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)
        # å°†ç¼–ç åçš„åºåˆ—æ± åŒ–å¹¶æŠ•å½±åˆ°åŸå…ˆçš„ regime_hidden ç»´åº¦ï¼Œä¿æŒä¸‹æ¸¸ç»“æ„ä¸å˜
        self.regime_proj = nn.Linear(self.d_model, regime_hidden)
        # é—¨æ§ç”± Transformer ä¸Šä¸‹æ–‡äº§ç”Ÿ
        self.regime_gate = nn.Linear(regime_hidden, 3)
        # ä¸‰ä¸ªæ°´æœŸä¸“å®¶ï¼šNNï¼ˆåŸºäº LSTM éšçŠ¶æ€ + base_runoffï¼‰
        self.regime_experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(1 + regime_hidden, regime_hidden),
                nn.ReLU(),
                nn.Linear(regime_hidden, self.output_dim)
            ) for _ in range(3)
        ])
        # æ®‹å·®å°ºåº¦ï¼ˆå¯è®­ç»ƒï¼Œå°å¹…åº¦ï¼‰
        self.regime_residual_scale = nn.Parameter(torch.tensor(0.05))
        # é—¨æ§æ¸©åº¦ï¼ˆè¶Šå°è¶Šå°–é”ï¼‰ä¸top-kï¼ˆ1=ç¡¬è·¯ç”±ï¼‰
        self.regime_temperature = float(self._get_config_value('regime_temperature', 0.8))
        self.regime_top_k = int(self._get_config_value('regime_top_k', 1))
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.bias, 0)
                nn.init.constant_(module.weight, 1.0)
    
    def forward(self, batch: Dict[str, torch.Tensor], return_gate_info: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            batch: åŒ…å«'features'ç­‰é”®çš„æ‰¹æ¬¡æ•°æ®
            return_gate_info: æ˜¯å¦è¿”å›é—¨æ§ä¿¡æ¯
            
        Returns:
            åŒ…å«'runoff'å’Œå¯é€‰'gate_info'çš„è¾“å‡ºå­—å…¸
        """
        features = batch['features']  # [batch_size, seq_len, input_dim]
        batch_size, seq_len, input_dim = features.shape
        
        # ğŸš€ ä¿®å¤ï¼šè·å–å®é™…çš„ç«™ç‚¹IDå­—ç¬¦ä¸²
        station_ids_str = batch.get('station_id', None)  # å­—ç¬¦ä¸²åˆ—è¡¨
        station_ids = batch.get('station_idx', torch.zeros(batch_size, dtype=torch.long, device=features.device))
        if isinstance(station_ids, torch.Tensor):
            station_ids = station_ids.view(-1).to(dtype=torch.long, device=features.device)
        
        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if input_dim != self.input_dim:
            # åŠ¨æ€è°ƒæ•´è¾“å…¥ç¼–ç å™¨
            self.feature_encoder[0] = nn.Linear(input_dim, self.d_model).to(features.device)
            self.input_dim = input_dim
        
        # 1. ç‰¹å¾ç¼–ç 
        encoded_features = self.feature_encoder(features)  # [batch_size, seq_len, d_model]
        
        # 2. è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤„ç†
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨gradient checkpointingå‡å°‘æ˜¾å­˜å ç”¨
        attention_output = encoded_features
        if self.training and self.use_gradient_checkpointing:
            for attention_block in self.attention_blocks:
                attention_output = torch.utils.checkpoint.checkpoint(attention_block, attention_output, use_reentrant=False)
        else:
            for attention_block in self.attention_blocks:
                attention_output = attention_block(attention_output)
        
        # 3. åºåˆ—èšåˆï¼ˆè·å–å•ä¸ªæ—¶é—´æ­¥çš„è¡¨ç¤ºï¼‰
        if self.sequence_aggregation == 'last':
            module_input = attention_output[:, -1, :]  # [batch_size, d_model]
        elif self.sequence_aggregation == 'mean':
            module_input = attention_output.mean(dim=1)  # [batch_size, d_model]
        else:
            module_input = attention_output[:, -1, :]  # é»˜è®¤ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        
        # 4. å››ä¸ªæ°´æ–‡æ¨¡å—å¤„ç†
        gate_infos = {}
        
        # Snowæ¨¡å—
        raw_features_last = batch.get('raw_features_last', None)
        if isinstance(raw_features_last, torch.Tensor):
            if raw_features_last.dim() == 1:
                raw_features_last = raw_features_last.unsqueeze(0).repeat(batch_size, 1)
            elif raw_features_last.dim() == 2 and raw_features_last.shape[0] == 1 and batch_size > 1:
                raw_features_last = raw_features_last.repeat(batch_size, 1)
        # ğŸš€ ä¼ å…¥å®é™…çš„ç«™ç‚¹IDå­—ç¬¦ä¸²ç»™PBMä¸“å®¶
        pbm_kwargs = {
            'station_ids': station_ids, 
            'station_ids_str': station_ids_str,
            'raw_features_last': raw_features_last
        }
        
        snow_pbm_out = self.snow_pbm_expert(module_input, **pbm_kwargs)
        snow_nn_out = self.snow_nn_expert(module_input)
        snow_output, snow_gate_info = self.snow_gate(module_input, snow_pbm_out, snow_nn_out)
        gate_infos['snow'] = snow_gate_info
        
        # Runoffæ¨¡å—
        runoff_pbm_out = self.runoff_pbm_expert(module_input, **pbm_kwargs)
        runoff_nn_out = self.runoff_nn_expert(module_input)
        runoff_output, runoff_gate_info = self.runoff_gate(module_input, runoff_pbm_out, runoff_nn_out)
        gate_infos['runoff'] = runoff_gate_info
        
        # ETæ¨¡å—
        et_pbm_out = self.et_pbm_expert(module_input, **pbm_kwargs)
        et_nn_out = self.et_nn_expert(module_input)
        et_output, et_gate_info = self.et_gate(module_input, et_pbm_out, et_nn_out)
        gate_infos['et'] = et_gate_info
        
        # Drainageæ¨¡å—
        drainage_pbm_out = self.drainage_pbm_expert(module_input, **pbm_kwargs)
        drainage_nn_out = self.drainage_nn_expert(module_input)
        drainage_output, drainage_gate_info = self.drainage_gate(module_input, drainage_pbm_out, drainage_nn_out)
        gate_infos['drainage'] = drainage_gate_info
        
        # 5. æœ€ç»ˆç»„åˆï¼ˆç‰©ç†åŒ–ï¼‰ï¼šäºŒè·¯å‡¸ç»„åˆï¼ˆrunoff/drainageï¼‰+ å¯ç”¨æ°´ä¸Šé™ A
        # 5.1 ç”Ÿæˆåˆ†é…ç³»æ•° Î±ï¼ˆéè´Ÿä¸”å’Œä¸º1ï¼‰ï¼Œèé›ª/è’¸æ•£ä½œä¸ºè°ƒåˆ¶å› å­
        alpha_in = torch.cat([module_input, snow_output, et_output], dim=-1)  # [B, d_model+2]
        alpha_logits = self.alpha_head(alpha_in)  # [B,2]
        alpha = torch.softmax(alpha_logits, dim=-1)  # [B,2]

        # 5.2 å‡¸ç»„åˆçš„åŸºç¡€å¾„æµ
        # å¯¹åˆ†é‡æ–½åŠ éè´Ÿæ€§ï¼Œé¿å…è´Ÿå€¼è¢«å‡¸ç»„åˆæ”¾å¤§
        q_quick = F.softplus(runoff_output)  # [B,1]
        q_base = F.softplus(drainage_output)  # [B,1]
        q_comb = alpha[:, 0:1] * q_quick + alpha[:, 1:2] * q_base  # [B,1]

        # 5.3 å¯ç”¨æ°´ä¸Šé™ Aï¼ˆå¹³æ»‘å®ˆæ’ï¼‰ï¼šA â‰ˆ ReLU(precip + snow - pet)
        # å°½é‡ä½¿ç”¨æœªæ ‡å‡†åŒ–ç‰©ç†é©±åŠ¨
        precip_raw = None
        pet_raw = None
        if raw_features_last is not None and isinstance(raw_features_last, torch.Tensor):
            try:
                pet_raw = raw_features_last[:, 0].reshape(-1, 1)  # [B,1]
                precip_raw = raw_features_last[:, 1].reshape(-1, 1)  # [B,1]
            except Exception:
                pass
        if precip_raw is None:
            precip_raw = torch.zeros_like(q_comb)
        if pet_raw is None:
            pet_raw = torch.zeros_like(q_comb)
        snow_pos = torch.relu(snow_output)  # [B,1]
        A_raw = torch.relu(precip_raw + snow_pos - pet_raw)  # [B,1]
        # å°†åŸå§‹Aæ˜ å°„åˆ°é¢„æµ‹ç©ºé—´ï¼Œä¿è¯éè´Ÿ
        A_in = torch.cat([precip_raw, snow_pos, pet_raw], dim=-1)  # [B,3]
        A_mapped = F.softplus(self.avail_head(A_in))  # [B,1]

        # 5.4 è½¯ä¸Šé™ï¼šfinal = A_mapped - Softplus(A_mapped - q_comb)
        final_output = A_mapped - F.softplus(A_mapped - q_comb)

        # 5.1 Regime-MoE è¾“å‡ºç»†åŒ–ï¼ˆæ®‹å·®åˆ° base runoffï¼‰
        regime_debug = None
        weights = None
        if self.use_regime_moe:
            # ä½¿ç”¨ Transformer ç¼–ç å™¨æå–æ—¶åºä¸Šä¸‹æ–‡
            enc_out = self.regime_encoder(attention_output)  # [batch, seq, d_model]
            # å…¨å±€å¹³å‡æ± åŒ–å¾—åˆ°ä¸Šä¸‹æ–‡ï¼Œå†æŠ•å½±åˆ°åŸå…ˆ hidden ç»´åº¦
            enc_ctx = enc_out.mean(dim=1)  # [batch, d_model]
            regime_ctx = self.regime_proj(enc_ctx)  # [batch, regime_hidden]

            base_runoff = final_output  # [batch, 1]

            # é—¨æ§æƒé‡
            logits = self.regime_gate(regime_ctx)  # [batch, 3]
            weights = torch.softmax(logits / max(self.regime_temperature, 1e-6), dim=-1)  # [batch, 3]

            # Regime top-k: 1 => ç¡¬è·¯ç”±ï¼›>1 => è½¯åŠ æƒ
            if self.regime_top_k == 1:
                with torch.no_grad():
                    idx = torch.argmax(weights, dim=-1)
                one_hot = torch.zeros_like(weights)
                one_hot.scatter_(1, idx.unsqueeze(1), 1.0)
                weights = one_hot

            # ä¸‰ä¸ªä¸“å®¶åŸºäº [base_runoff, regime_ctx]
            expert_outs = []
            regime_input = torch.cat([base_runoff, regime_ctx], dim=-1)  # [batch, 1+hidden]
            for i in range(3):
                expert_out = self.regime_experts[i](regime_input)  # [batch, 1]
                # å¯¹ä¸“å®¶è¾“å‡ºä¹Ÿåº”ç”¨æ¿€æ´»å‡½æ•°
                expert_outs.append(self.output_activation(expert_out))
            experts_stack = torch.stack(expert_outs, dim=-1)  # [batch, 1, 3]

            # åŠ æƒæ±‚å’Œå¾—åˆ°æ®‹å·®
            weights_exp = weights.unsqueeze(1)  # [batch, 1, 3]
            regime_residual = (experts_stack * weights_exp).sum(dim=-1)  # [batch, 1]
            final_output = base_runoff + self.regime_residual_scale * regime_residual

            # æœ€ç»ˆç¡®ä¿è¾“å‡ºéè´Ÿ
            final_output = torch.clamp(final_output, min=0.0)

            # æ”¶é›†è°ƒè¯•ä¿¡æ¯
            regime_debug = {
                'weights': weights,
                'weights_mean': weights.mean(dim=0),
                'residual_scale': self.regime_residual_scale.detach().clone()
            }
        
        # ç¡®ä¿è¾“å‡ºä¸ºæ ‡é‡ï¼ˆå¦‚æœoutput_dim=1ï¼‰
        if self.output_dim == 1:
            final_output = final_output.squeeze(-1)  # [batch_size]
        
        # æ„å»ºè¾“å‡ºå­—å…¸
        output = {
            'runoff': final_output,
            'regime_weights': weights,
            'alpha_weights': alpha,
            'available_water': A_mapped
        }
        
        if return_gate_info:
            # å¤„ç†é—¨æ§ä¿¡æ¯
            output['gate_info'] = {
                'module_gates': gate_infos,
                'expert_usage': self._compute_expert_usage(gate_infos),
                'load_balancing_loss': self._compute_load_balancing_loss(gate_infos)
            }
            if regime_debug is not None:
                output['gate_info']['regime'] = regime_debug
        
        return output
    
    def _compute_expert_usage(self, gate_infos: Dict) -> Dict[str, float]:
        """è®¡ç®—ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡"""
        expert_usage = {}
        for module_name, gate_info in gate_infos.items():
            pbm_usage = gate_info['pbm_weight']
            nn_usage = gate_info['nn_weight']
            expert_usage[f'{module_name}_pbm'] = pbm_usage
            expert_usage[f'{module_name}_nn'] = nn_usage
        return expert_usage
    
    def _compute_load_balancing_loss(self, gate_infos: Dict) -> torch.Tensor:
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        total_loss = 0.0
        for module_name, gate_info in gate_infos.items():
            gate_weights = gate_info['gate_weights']  # [batch_size, num_experts]
            # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨ç‡
            expert_usage = gate_weights.mean(dim=0)  # [num_experts]
            # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨ç‡åº”è¯¥æ˜¯ 0.5
            ideal_usage = 0.5
            # è®¡ç®—æ–¹å·®ä½œä¸ºè´Ÿè½½å‡è¡¡æŸå¤±
            loss = torch.var(expert_usage) / (ideal_usage ** 2)
            total_loss += loss
        
        return total_loss / len(gate_infos)


def create_hybrid_hydro_moe_model(config) -> HybridHydroMoEModel:
    """åˆ›å»ºæ··åˆHydroMoEæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    return HybridHydroMoEModel(config)


if __name__ == "__main__":
    # æµ‹è¯•æ··åˆæ¨¡å‹
    print("ğŸ§ª æµ‹è¯•æ··åˆHydroMoEæ¨¡å‹...")
    
    # ç®€åŒ–é…ç½®
    class TestConfig:
        def __init__(self):
            self.model = TestModelConfig()
    
    class TestModelConfig:
        def __init__(self):
            self.input_size = 3
            self.d_model = 64
            self.num_heads = 4
            self.num_attention_layers = 2
            self.dropout = 0.1
            self.max_sequence_length = 100
            self.sequence_aggregation = 'last'
    
    config = TestConfig()
    model = HybridHydroMoEModel(config)
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 10
    batch = {
        'features': torch.randn(batch_size, seq_len, 3),
        'station_idx': torch.tensor([1, 2, 3, 4])
    }
    
    # å‰å‘ä¼ æ’­
    output = model(batch, return_gate_info=True)
    
    print(f" è¾“å‡ºå½¢çŠ¶: {output['runoff'].shape}")
    print(f" ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡: {output['gate_info']['expert_usage']}")
    print(" æ··åˆHydroMoEæ¨¡å‹æµ‹è¯•æˆåŠŸï¼")