"""
HydroMoE v2.0 Enhanced Main Program
é›†æˆé«˜çº§å½’ä¸€åŒ–å’Œæ¢¯åº¦ç¨³å®šæŠ€æœ¯çš„ä¸»ç¨‹åº
"""

import os
import sys
import logging
import glob
import pandas as pd

from pandas.core.nanops import F
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import warnings

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from MoE_config import get_default_config
from MoE_data_loader import FixedHydroDataset, FixedDataConfig, clear_data_cache, warmup_data_loading
from MoE_hybrid_model import HybridHydroMoEModel

# å¯¼å…¥å¢å¼ºåŠŸèƒ½
from MoE_advanced_normalization import create_gradient_stable_normalizer

# å¯¼å…¥è®­ç»ƒå™¨å’Œè¯„ä¼°å™¨
from MoE_trainer_enhanced import enhanced_training_loop
from MoE_evaluator_simple import (
    evaluate_enhanced_model,
    load_best_model_if_exists,
)
from MoE_station_regime_calibration import wrap_with_calibration
from MoE_lowflow_augment import run_pipeline
from MoE_risk_refiner import run_risk_refine, finetune_on_risk_stations

# å¯¼å…¥æ”¹è¿›åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
try:
    from MoE_feature_engineering import HydroFeatureEngineer, AdaptiveFeatureSelector
    FEATURE_ENGINEERING_AVAILABLE = True
except ImportError:
    FEATURE_ENGINEERING_AVAILABLE = False
    print(" ç‰¹å¾å·¥ç¨‹æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")

try:
    from MoE_multiscale_attention import MultiScaleTemporalAttention
    MULTISCALE_ATTENTION_AVAILABLE = True
except ImportError:
    MULTISCALE_ATTENTION_AVAILABLE = False
    print(" å¤šå°ºåº¦æ³¨æ„åŠ›æ¨¡å—ä¸å¯ç”¨")

# è®¾ç½®è­¦å‘Šè¿‡æ»¤
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

logger = logging.getLogger(__name__)


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

def setup_enhanced_training():
    """è®¾ç½®å¢å¼ºè®­ç»ƒç¯å¢ƒ"""
    
    # 1. åŸºç¡€é…ç½®
    config = get_default_config()
    
    # 2. æ•°æ®é…ç½® - æµ‹è¯•æ¨¡å¼50ä¸ªç«™ç‚¹
    data_config = FixedDataConfig(
        normalize_features=True,  # å¯ç”¨æ ‡å‡†åŒ–ï¼Œé…åˆåå½’ä¸€åŒ–è¾“å‡ºçœŸå®å€¼
        normalize_targets=True,   # å¯ç”¨ç›®æ ‡æ ‡å‡†åŒ–ï¼Œé¿å…é‡çº²ä¸åŒ¹é…
        use_all_stations=True,   # ä½¿ç”¨å…¨éƒ¨ç«™ç‚¹
        quick_test=False,          # å…³é—­å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        quick_test_stations=50,   # ğŸ”¥ ä¿æŒ50ä¸ªç«™ç‚¹ç”¨äºæµ‹è¯•
        # å…¶ä»–å‚æ•°éƒ½ä½¿ç”¨æ­£å¸¸æ¨¡å¼çš„é»˜è®¤å€¼ï¼ˆsequence_length=64, stride=16ç­‰ï¼‰
    )
    
    # 2.1 å¯åŠ¨æ—¶è‡ªåŠ¨ç¦»çº¿å¢å¼ºï¼š
    # - è¯»å– cmaes_optimal_params.jsonï¼Œå¯¼å‡º RÂ²<0.2 çš„ä½å€¼ç«™ç‚¹æ¸…å•
    # - åŸºäºåŸå§‹CSVç”Ÿæˆâ€œåªç”¨å†å²ä¿¡æ¯â€çš„å¾„æµæ»å/æ»šåŠ¨ç‰¹å¾å¢å¼ºç‰ˆCSV
    try:
        meta = run_pipeline(
            src_csv=data_config.csv_path,
            cmaes_json='cmaes_optimal_params.json',
            out_dir='./outputs/augmented',
            r2_threshold=0.2
        )
        # ç”¨å¢å¼ºåçš„CSVä¸æ¨èç‰¹å¾åˆ—æ›¿æ¢
        if isinstance(meta, dict):
            if 'augmented_csv' in meta and meta['augmented_csv']:
                data_config.csv_path = meta['augmented_csv']
                print(f"  ğŸ”— ä½¿ç”¨å¢å¼ºCSV: {data_config.csv_path}")
            rec_cols = meta.get('recommended_feature_cols', '')
            if rec_cols:
                data_config.feature_cols = [c for c in rec_cols.split(',') if c]
                print(f"  ğŸ§© ä½¿ç”¨æ¨èç‰¹å¾åˆ—: {data_config.feature_cols}")
            if 'low_r2_list' in meta and meta['low_r2_list']:
                print(f"  ğŸ“ ä½RÂ²ç«™ç‚¹æ¸…å•: {meta['low_r2_list']}")
                print(f"  ğŸ“‰ ä½RÂ²ç«™ç‚¹æ•°é‡: {meta.get('low_r2_count', 'NA')}")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®å¢å¼ºæ­¥éª¤å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹CSV: {e}")
    
    # 3. åˆ›å»ºé«˜çº§å½’ä¸€åŒ–å™¨
    normalizer = create_gradient_stable_normalizer(strategy="station_wise")
    
    print("âœ… å¢å¼ºè®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")
    print(f"  ğŸ“Š å½’ä¸€åŒ–ç­–ç•¥: ç«™ç‚¹çº§å½’ä¸€åŒ–")
    print(f"  ğŸ›ï¸ æ¢¯åº¦æ§åˆ¶: é€‚ä¸­ç­–ç•¥")
    print(f"  ğŸ è®­ç»ƒæ¨¡å¼: æµ‹è¯•æ¨¡å¼ï¼ˆä»…è°ƒæ•´ç«™ç‚¹æ•°=10ï¼Œå…¶ä»–ä¸ºæ­£å¸¸å‚æ•°ï¼‰")
    try:
        seq_len = data_config.sequence_length
        stride = data_config.sequence_stride
    except Exception:
        seq_len = 96
        stride = 16
    print(f"  âš™ï¸ æ­£å¸¸å‚æ•°: åºåˆ—é•¿åº¦{seq_len}ï¼Œstride={stride}ï¼Œbatch=32")
    
    return config, data_config, normalizer


def create_enhanced_datasets(data_config, normalizer):
    """åˆ›å»ºå¢å¼ºæ•°æ®é›†"""
    
    print("\nğŸ”„ åˆ›å»ºå¢å¼ºæ•°æ®é›†...")
    # ç¡®ä¿ä¸å—ä¸Šä¸€æ¬¡å¿«é€Ÿæµ‹è¯•ç¼“å­˜å½±å“
    try:
        clear_data_cache()
    except Exception:
        pass
    
    # 1. åˆ›å»ºè®­ç»ƒé›†å¹¶è·å–æ ‡å‡†åŒ–å™¨
    train_dataset = FixedHydroDataset(data_config, split="train", scalers=None)
    scalers = train_dataset.get_scalers()
    # 2. éªŒè¯/æµ‹è¯•é›†å…±äº«è®­ç»ƒé›†æ ‡å‡†åŒ–å‚æ•°ï¼Œç¡®ä¿è¯„ä¼°/åå½’ä¸€åŒ–ä¸€è‡´
    val_dataset = FixedHydroDataset(data_config, split="val", scalers=scalers)
    test_dataset = FixedHydroDataset(data_config, split="test", scalers=scalers)
    
    print(f"  ğŸ“ˆ è®­ç»ƒé›†: {len(train_dataset)} åºåˆ—")
    print(f"  ğŸ“Š éªŒè¯é›†: {len(val_dataset)} åºåˆ—")
    print(f"  ğŸ“‹ æµ‹è¯•é›†: {len(test_dataset)} åºåˆ—")
    
    # 2. è·å–æ•°æ®ç”¨äºå½’ä¸€åŒ–ï¼ˆè¿™é‡Œéœ€è¦ä»æ•°æ®é›†ä¸­æå–åŸå§‹æ•°æ®ï¼‰
    # æ³¨æ„ï¼šå®é™…å®ç°ä¸­éœ€è¦ä¿®æ”¹æ•°æ®é›†ç±»ä»¥æ”¯æŒé«˜çº§å½’ä¸€åŒ–
    print("  ğŸ”§ åº”ç”¨é«˜çº§å½’ä¸€åŒ–...")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æ­£å¸¸æ¨¡å¼çš„æ‰¹æ¬¡å¤§å°
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,  # ğŸ”¥ ä½¿ç”¨é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°ï¼ˆè€ƒè™‘åˆ°åªæœ‰10ä¸ªç«™ç‚¹ï¼Œ128å¤ªå¤§ï¼‰
        shuffle=True,
        num_workers=0,  # Windowså…¼å®¹æ€§
        pin_memory=False,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=32,  # ä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=32,  # ä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )
    
    print("âœ… å¢å¼ºæ•°æ®é›†åˆ›å»ºå®Œæˆ")
    
    return train_loader, val_loader, test_loader, normalizer


def create_enhanced_model(config):
    """åˆ›å»ºå¢å¼ºæ¨¡å‹"""
    
    print("\nğŸ—ï¸ åˆ›å»ºå¢å¼ºæ¨¡å‹...")
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆç›´æ¥ä½¿ç”¨ Hybrid æ¨¡å‹ï¼Œå†…éƒ¨å·²é›†æˆ LSTM æ°´æœŸä¸“å®¶ï¼‰
    model = HybridHydroMoEModel(config).to(device)

    # è¯¦ç»†ç»Ÿè®¡æ¨¡å‹å‚æ•°
    def analyze_model_complexity(model):
        """åˆ†ææ¨¡å‹å¤æ‚æ€§"""
        total_params = 0
        trainable_params = 0
        module_stats = {}

        # åˆ†æ¨¡å—ç»Ÿè®¡
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                module_params = sum(p.numel() for p in module.parameters())
                module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)

                if module_params > 0:
                    module_stats[name] = {
                        'total': module_params,
                        'trainable': module_trainable
                    }

        # æ€»è®¡
        for p in model.parameters():
            total_params += p.numel()
            if p.requires_grad:
                trainable_params += p.numel()

        return total_params, trainable_params, module_stats

    total_params, trainable_params, module_stats = analyze_model_complexity(model)

    print(f"  ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  ğŸ”’ å›ºå®šå‚æ•°: {total_params - trainable_params:,}")

    # åˆ†ææ¨¡å‹æ¶æ„å¤æ‚æ€§
    print(f"\nğŸ’¡ MoEæ¶æ„å¤æ‚æ€§åˆ†æ:")
    print(f"   ğŸ“¥ è¾“å…¥ç‰¹å¾: ä»…3ä¸ª (é™æ°´ã€æ¸©åº¦ã€è’¸æ•£å‘)")
    print(f"   ğŸ§  ç¥ç»ç½‘ç»œä¸“å®¶: 4ä¸ªä¸“é—¨åŒ–ä¸“å®¶")
    print(f"      â„ï¸ é›ªæ¨¡å—ä¸“å®¶ (å­¦ä¹ é›ªç§¯ç´¯/èåŒ–æœºç†)")
    print(f"      ğŸŒŠ å¾„æµä¸“å®¶ (å­¦ä¹ åœ°è¡¨å¾„æµæœºç†)")
    print(f"      ğŸŒ¿ è’¸æ•£å‘ä¸“å®¶ (å­¦ä¹ æ¤è¢«è’¸è…¾æœºç†)")
    print(f"      ğŸ’§ æ’æ°´ä¸“å®¶ (å­¦ä¹ åœ°ä¸‹æ’æ°´æœºç†)")
    print(f"   ğŸ¯ æ™ºèƒ½é—¨æ§: 4ä¸ªé—¨æ§ç½‘ç»œ (åŠ¨æ€é€‰æ‹©ä¸“å®¶æƒé‡)")
    print(f"   ğŸ”„ æ³¨æ„åŠ›æœºåˆ¶: å¤šå¤´è‡ªæ³¨æ„åŠ› (æ•è·æ—¶åºä¾èµ–)")
    print(f"   âš™ï¸ ç‰©ç†æœºç†: PBMæ¨¡å— (åŒ…å«å¯å­¦ä¹ æ°´æ–‡å‚æ•°)")
    print(f"   ğŸ“ˆ å‚æ•°å­¦ä¹ : {trainable_params:,} ä¸ªå¯å­¦ä¹ å‚æ•°!")

    # æ˜¾ç¤ºå…³é”®æ¨¡å—å‚æ•°åˆ†å¸ƒ
    expert_params = 0
    gate_params = 0
    attention_params = 0

    for name, stats in module_stats.items():
        if any(x in name for x in ['nn_expert', 'mlp']):
            expert_params += stats['trainable']
        elif 'gate' in name:
            gate_params += stats['trainable']
        elif 'attention' in name:
            attention_params += stats['trainable']

    print(f"\n å‚æ•°åˆ†å¸ƒ:")
    print(f"    ä¸“å®¶ç½‘ç»œ: {expert_params:,} å‚æ•°")
    print(f"    é—¨æ§ç½‘ç»œ: {gate_params:,} å‚æ•°")
    print(f"    æ³¨æ„åŠ›æœºåˆ¶: {attention_params:,} å‚æ•°")
    print(f"    å…¶ä»–æ¨¡å—: {trainable_params - expert_params - gate_params - attention_params:,} å‚æ•°")
    # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ â€” å…ˆåŠ è½½åˆ°åŸºç¡€æ¨¡å‹
    load_best_model_if_exists(model)

    # åŒ…è£…ç«™ç‚¹Ã—æ°´æœŸæ ¡å‡†ï¼ˆä¸æ”¹å˜åŸæ¨¡å‹ç»“æ„ï¼Œç«¯åˆ°ç«¯è®­ç»ƒï¼‰
    try:
        model = wrap_with_calibration(model)
        print("  ğŸ”§ å·²å¯ç”¨ç«™ç‚¹Ã—æ°´æœŸæ ¡å‡†åŒ…è£…å™¨ (CalibratedHybridModel)")
    except Exception as e:
        print(f"  âš ï¸ ç«™ç‚¹Ã—æ°´æœŸæ ¡å‡†åŒ…è£…å™¨å¯ç”¨å¤±è´¥: {e}")
    return model, device


# è®­ç»ƒå¾ªç¯å·²ç§»è‡³ MoE_trainer_enhanced.py

def check_r2_consistency(predictions, targets, station_ids):
    """æ£€æŸ¥RÂ²ä¸€è‡´æ€§"""
    from sklearn.metrics import r2_score
    
    # æ•´ä½“RÂ²
    overall_r2 = r2_score(targets, predictions)
    
    # ç«™ç‚¹çº§RÂ²
    station_r2s = []
    unique_stations = np.unique(station_ids)
    
    print(f"  ğŸ¢ ç«™ç‚¹çº§åˆ†æ ({len(unique_stations)}ä¸ªç«™ç‚¹):")
    
    for station in unique_stations:
        mask = station_ids == station
        if np.sum(mask) > 10:  # è‡³å°‘10ä¸ªæ ·æœ¬
            station_r2 = r2_score(targets[mask], predictions[mask])
            station_r2s.append(station_r2)
    
    avg_station_r2 = np.mean(station_r2s)
    consistency_gap = abs(overall_r2 - avg_station_r2)
    
    print(f"    æ•´ä½“RÂ²: {overall_r2:.4f}")
    print(f"    ç«™ç‚¹å‡å€¼RÂ²: {avg_station_r2:.4f}")
    print(f"    ä¸€è‡´æ€§å·®è·: {consistency_gap:.4f}")
    
    if consistency_gap < 0.05:
        print("     RÂ²ä¸€è‡´æ€§è‰¯å¥½")
    else:
        print("     RÂ²ä¸€è‡´æ€§éœ€è¦æ”¹è¿›")
    
    return consistency_gap < 0.05


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸŒŠ HydroMoE v2.0 Enhanced Training")
    print("=" * 60)
    print("é›†æˆé«˜çº§å½’ä¸€åŒ–å’Œæ¢¯åº¦ç¨³å®šæŠ€æœ¯")
    print()
    
    # ğŸš€ æ€§èƒ½ä¼˜åŒ–æç¤º
    print("âš¡ æ€§èƒ½ä¼˜åŒ–é…ç½® (å·²è‡ªåŠ¨åº”ç”¨):")
    print(f"  ğŸ“Š æ•°æ®ä¼˜åŒ–:")
    print(f"     - from_numpy()é›¶æ‹·è´åŠ è½½: å¯ç”¨")
    print(f"     - æ—¥æœŸå­—ç¬¦ä¸²é¢„ç¼“å­˜: å¯ç”¨")
    print(f"     - åºåˆ—strideä¼˜åŒ–: 32 (å‡å°‘åºåˆ—æ•°)")
    print(f"  ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–:")
    print(f"     - Gradient Checkpointing: {os.getenv('USE_GRAD_CHECKPOINT', '1')}")
    print(f"     - æ‰¹æ¬¡å¤§å°: 64")
    print(f"     - æ¢¯åº¦ç´¯ç§¯: 2æ­¥ (ç­‰æ•ˆbatch=128)")
    print(f"     - Inplaceæ¿€æ´»å‡½æ•°: å¯ç”¨")
    print(f"  âš¡ è®¡ç®—ä¼˜åŒ–:")
    print(f"     - PBMæ‰¹é‡è®¡ç®—: å¯ç”¨ (10-50xåŠ é€Ÿ)")
    print(f"     - PyTorch 2.0 Flash Attention: è‡ªåŠ¨")
    print(f"     - æ··åˆç²¾åº¦è®­ç»ƒ: False (ç¨³å®šæ€§ä¼˜å…ˆ)")
    print(f"  ğŸ§¹ å†…å­˜ç®¡ç†:")
    print(f"     - å‘¨æœŸæ€§æ˜¾å­˜æ¸…ç†: æ¯100 batch")
    print(f"     - æ•°æ®Worker: 0 (Windowså…¼å®¹)")
    print()
    
    try:
        # 1. è®¾ç½®ç¯å¢ƒ
        setup_logging()
        
        # 2. è®¾ç½®å¢å¼ºè®­ç»ƒ
        config, data_config, normalizer = setup_enhanced_training()

          # 2.1 è‹¥å­˜åœ¨ä¸Šä¸€è½®æŒ‡æ ‡ï¼šå¼ºåˆ¶ä»…åŠ è½½ä½RÂ²ç«™ç‚¹çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆéå¯é€‰ï¼‰
        try:
            metrics_csv = os.path.join('outputs', 'enhanced_real_runoff_predictions', 'station_daily_metrics.csv')
            if os.path.exists(metrics_csv):
                dfm = pd.read_csv(metrics_csv)
                if 'station_id' in dfm.columns and 'R2' in dfm.columns:
                    r2_th = getattr(config.training, 'risk_refine_r2_threshold', 0.2)
                    low_df = dfm[pd.to_numeric(dfm['R2'], errors='coerce') < float(r2_th)]
                    low_sids = [str(s) for s in low_df['station_id'].tolist()]
                    if low_sids:
                        data_config.filter_station_ids = low_sids
                        print(f"ğŸ§­ ä»…åŠ è½½ä½RÂ²ç«™ç‚¹æ•°æ®è¿›è¡Œè®­ç»ƒ: {len(low_sids)} ä¸ª (<{r2_th})")
        except Exception as _:
            pass
        
        # 2.5 ğŸš€ æ•°æ®é¢„çƒ­ï¼ˆå¯é€‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
        if os.getenv("WARMUP_DATA", "1").lower() in ["1", "true", "yes"]:
            logger.info("ğŸ”¥ å¯åŠ¨æ•°æ®é¢„çƒ­...")
            warmup_data_loading(data_config)
        
        # 3. åˆ›å»ºæ•°æ®é›†ï¼ˆé™ä½é»˜è®¤æ‰¹æ¬¡ï¼Œç¼“è§£æ˜¾å­˜ï¼‰
        train_loader, val_loader, test_loader, normalizer = create_enhanced_datasets(
            data_config, normalizer
        )
        
        # 4. åˆ›å»ºæ¨¡å‹
        model, device = create_enhanced_model(config)
        # æ³¨ï¼šæ­¤æ—¶ DataLoader å·²æŒ‰ä½RÂ²é›†åˆæ„å»ºï¼Œç›´æ¥è¿›å…¥å¸¸è§„è®­ç»ƒï¼ˆå³å¯¹ä½RÂ²ç«™ç‚¹è¿›è¡Œæ­£å¼è®­ç»ƒï¼‰
        
        # 5. è®­ç»ƒæ¨¡å‹ï¼ˆæˆ–è·³è¿‡ï¼Œä»…è¯„ä¼°ï¼‰
        eval_only = os.getenv("EVAL_ONLY", "0").lower() in ["1", "true", "yes"]
        if not eval_only:
            # ä½¿ç”¨é…ç½®ä¸­çš„epochsä¸ºä¸»ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
            try:
                epochs = int(os.getenv("EPOCHS", str(getattr(config.training, 'epochs', 50))))
            except Exception:
                epochs = 50
            enhanced_training_loop(
                model, train_loader, val_loader, device,
                epochs=epochs,
                patience=getattr(config.training, 'early_stopping_patience', 5)
            )
            # ç›´æ¥ä¾æ®è®­ç»ƒé…ç½®å¼€å…³å¯ç”¨é«˜é£é™©ç«™ç‚¹å†è®­ç»ƒï¼ˆé»˜è®¤å¼€å¯ï¼‰
            try:
                if getattr(config.training, 'risk_refine_enable', True):
                    run_risk_refine(
                        model, train_loader, val_loader, device,
                        r2_threshold=getattr(config.training, 'risk_refine_r2_threshold', 0.2),
                        epochs=getattr(config.training, 'risk_refine_epochs', 8),
                        lr=getattr(config.training, 'risk_refine_lr', 5e-5),
                        patience=getattr(config.training, 'risk_refine_patience', 3)
                    )
            except Exception as e:
                print(f"âš ï¸ é£é™©ç«™ç‚¹å†è®­ç»ƒå¤±è´¥: {e}")
        else:
            print("\nâ­ï¸ è·³è¿‡è®­ç»ƒ (EVAL_ONLY=1)ï¼Œç›´æ¥ä½¿ç”¨å½“å‰/æœ€ä½³æƒé‡è¿›è¡Œè¯„ä¼°ã€‚")
        
        # 6. åœ¨è¯„ä¼°å‰ï¼Œé‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡ï¼ˆç¨³å®šæ–‡ä»¶æˆ–æœ€æ–°æ—¶é—´æˆ³ï¼‰
        print("\nğŸ” è¯„ä¼°å‰åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡...")
        load_best_model_if_exists(model, path="outputs/enhanced_hydromoe_best.pth")

        # 7. è¯„ä¼°æ¨¡å‹
        evaluate_enhanced_model(model, test_loader, device)
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\n ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
    else:
        print("\n ç¨‹åºæ‰§è¡Œå¤±è´¥")
        sys.exit(1)