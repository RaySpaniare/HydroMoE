"""
Advanced Normalization Strategies for HydroMoE v2.0
è§£å†³å½’ä¸€åŒ–ä¸è¿˜åŸä¸ä¸€è‡´æ€§é—®é¢˜çš„é«˜çº§ç­–ç•¥
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler, RobustScaler
from typing import Dict, List, Tuple, Optional, Union
import logging
from dataclasses import dataclass
import warnings

logger = logging.getLogger(__name__)


@dataclass
class NormalizationConfig:
    """å½’ä¸€åŒ–é…ç½®"""
    # ç­–ç•¥é€‰æ‹©
    strategy: str = "time_window"  # "time_window", "sliding_window", "station_wise", "robust", "none"
    
    # æ—¶é—´çª—å£å½’ä¸€åŒ–å‚æ•°
    window_size: int = 365  # æ—¶é—´çª—å£å¤§å°ï¼ˆå¤©ï¼‰
    window_stride: int = 30  # çª—å£æ»‘åŠ¨æ­¥é•¿ï¼ˆå¤©ï¼‰
    min_window_data: int = 100  # çª—å£å†…æœ€å°æ•°æ®ç‚¹æ•°
    
    # æ»‘åŠ¨çª—å£å‚æ•°
    lookback_days: int = 730  # å›æœ›å¤©æ•°ï¼ˆ2å¹´ï¼‰
    update_frequency: int = 90  # æ›´æ–°é¢‘ç‡ï¼ˆå¤©ï¼‰
    
    # ç«™ç‚¹çº§å½’ä¸€åŒ–
    use_station_stats: bool = True  # æ˜¯å¦ä½¿ç”¨ç«™ç‚¹çº§ç»Ÿè®¡
    station_min_samples: int = 1000  # ç«™ç‚¹æœ€å°æ ·æœ¬æ•°
    
    # é²æ£’æ€§å‚æ•°
    outlier_threshold: float = 3.0  # å¼‚å¸¸å€¼é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰
    use_robust_scaler: bool = False  # æ˜¯å¦ä½¿ç”¨é²æ£’æ ‡å‡†åŒ–
    
    # ç‰¹å¾ç‰¹å®šé…ç½®
    feature_specific: Dict[str, Dict] = None
    
    def __post_init__(self):
        if self.feature_specific is None:
            self.feature_specific = {
                'precip': {'log_transform': True, 'add_constant': 0.1},
                'temp': {'log_transform': False, 'seasonal_adjust': True},
                'pet': {'log_transform': True, 'add_constant': 0.01},
                'runoff': {'log_transform': True, 'add_constant': 0.001}
            }


class TimeWindowNormalizer:
    """æ—¶é—´çª—å£å½’ä¸€åŒ–å™¨"""
    
    def __init__(self, config: NormalizationConfig):
        self.config = config
        self.window_stats = {}  # å­˜å‚¨æ¯ä¸ªçª—å£çš„ç»Ÿè®¡ä¿¡æ¯
        self.fitted = False
    
    def fit(self, data: pd.DataFrame, feature_cols: List[str], 
            target_col: str, time_col: str = 'date') -> 'TimeWindowNormalizer':
        """
        æ‹Ÿåˆæ—¶é—´çª—å£å½’ä¸€åŒ–å™¨
        
        Args:
            data: åŒ…å«æ—¶é—´åˆ—çš„æ•°æ®æ¡†
            feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
            target_col: ç›®æ ‡åˆ—å
            time_col: æ—¶é—´åˆ—å
        """
        logger.info("ğŸ”„ å¼€å§‹æ‹Ÿåˆæ—¶é—´çª—å£å½’ä¸€åŒ–å™¨...")
        
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(data[time_col]):
            data[time_col] = pd.to_datetime(data[time_col])
        
        # æŒ‰æ—¶é—´æ’åº
        data = data.sort_values(time_col)
        
        # åˆ›å»ºæ—¶é—´çª—å£
        start_date = data[time_col].min()
        end_date = data[time_col].max()
        
        window_starts = pd.date_range(
            start=start_date, 
            end=end_date - pd.Timedelta(days=self.config.window_size),
            freq=pd.Timedelta(days=self.config.window_stride)
        )
        
        logger.info(f"åˆ›å»ºäº† {len(window_starts)} ä¸ªæ—¶é—´çª—å£")
        
        # è®¡ç®—æ¯ä¸ªçª—å£çš„ç»Ÿè®¡ä¿¡æ¯
        for i, window_start in enumerate(window_starts):
            window_end = window_start + pd.Timedelta(days=self.config.window_size)
            
            # è·å–çª—å£å†…æ•°æ®
            window_mask = (data[time_col] >= window_start) & (data[time_col] < window_end)
            window_data = data[window_mask]
            
            if len(window_data) < self.config.min_window_data:
                continue
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            window_stats = {
                'start_date': window_start,
                'end_date': window_end,
                'feature_stats': {},
                'target_stats': {}
            }
            
            # ç‰¹å¾ç»Ÿè®¡
            for col in feature_cols:
                window_stats['feature_stats'][col] = {
                    'mean': window_data[col].mean(),
                    'std': window_data[col].std(),
                    'median': window_data[col].median(),
                    'q25': window_data[col].quantile(0.25),
                    'q75': window_data[col].quantile(0.75)
                }
            
            # ç›®æ ‡å˜é‡ç»Ÿè®¡
            window_stats['target_stats'] = {
                'mean': window_data[target_col].mean(),
                'std': window_data[target_col].std(),
                'median': window_data[target_col].median(),
                'q25': window_data[target_col].quantile(0.25),
                'q75': window_data[target_col].quantile(0.75)
            }
            
            self.window_stats[i] = window_stats
        
        self.fitted = True
        logger.info(f"âœ… æ—¶é—´çª—å£å½’ä¸€åŒ–å™¨æ‹Ÿåˆå®Œæˆï¼Œå…± {len(self.window_stats)} ä¸ªæœ‰æ•ˆçª—å£")
        return self
    
    def transform(self, data: pd.DataFrame, feature_cols: List[str], 
                  target_col: str, time_col: str = 'date') -> pd.DataFrame:
        """
        åº”ç”¨æ—¶é—´çª—å£å½’ä¸€åŒ–
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        data = data.copy()
        
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(data[time_col]):
            data[time_col] = pd.to_datetime(data[time_col])
        
        # ä¸ºæ¯è¡Œæ•°æ®æ‰¾åˆ°å¯¹åº”çš„æ—¶é—´çª—å£
        for idx, row in data.iterrows():
            current_time = row[time_col]
            
            # æ‰¾åˆ°æœ€åˆé€‚çš„æ—¶é—´çª—å£
            best_window = self._find_best_window(current_time)
            
            if best_window is None:
                continue
            
            # åº”ç”¨å½’ä¸€åŒ–
            for col in feature_cols:
                if col in best_window['feature_stats']:
                    stats = best_window['feature_stats'][col]
                    if stats['std'] > 1e-8:  # é¿å…é™¤é›¶
                        data.loc[idx, col] = (row[col] - stats['mean']) / stats['std']
            
            # ç›®æ ‡å˜é‡å½’ä¸€åŒ–
            if target_col in data.columns:
                stats = best_window['target_stats']
                if stats['std'] > 1e-8:
                    data.loc[idx, target_col] = (row[target_col] - stats['mean']) / stats['std']
        
        return data
    
    def inverse_transform(self, data: pd.DataFrame, target_col: str, 
                         time_col: str = 'date') -> pd.DataFrame:
        """
        åå½’ä¸€åŒ–ç›®æ ‡å˜é‡
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        data = data.copy()
        
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(data[time_col]):
            data[time_col] = pd.to_datetime(data[time_col])
        
        # ä¸ºæ¯è¡Œæ•°æ®æ‰¾åˆ°å¯¹åº”çš„æ—¶é—´çª—å£å¹¶åå½’ä¸€åŒ–
        for idx, row in data.iterrows():
            current_time = row[time_col]
            
            # æ‰¾åˆ°æœ€åˆé€‚çš„æ—¶é—´çª—å£
            best_window = self._find_best_window(current_time)
            
            if best_window is None:
                continue
            
            # åå½’ä¸€åŒ–ç›®æ ‡å˜é‡
            stats = best_window['target_stats']
            if stats['std'] > 1e-8:
                data.loc[idx, target_col] = row[target_col] * stats['std'] + stats['mean']
        
        return data
    
    def _find_best_window(self, target_time) -> Optional[Dict]:
        """æ‰¾åˆ°æœ€é€‚åˆçš„æ—¶é—´çª—å£"""
        best_window = None
        min_distance = float('inf')
        
        for window in self.window_stats.values():
            window_center = window['start_date'] + (window['end_date'] - window['start_date']) / 2
            distance = abs((target_time - window_center).total_seconds())
            
            if distance < min_distance:
                min_distance = distance
                best_window = window
        
        return best_window


class StationWiseNormalizer:
    """ç«™ç‚¹çº§å½’ä¸€åŒ–å™¨"""
    
    def __init__(self, config: NormalizationConfig):
        self.config = config
        self.station_scalers = {}  # æ¯ä¸ªç«™ç‚¹çš„æ ‡å‡†åŒ–å™¨
        self.fitted = False
    
    def fit(self, data: pd.DataFrame, feature_cols: List[str], 
            target_col: str, station_col: str = 'station_id') -> 'StationWiseNormalizer':
        """
        æ‹Ÿåˆç«™ç‚¹çº§å½’ä¸€åŒ–å™¨
        """
        logger.info("ğŸ”„ å¼€å§‹æ‹Ÿåˆç«™ç‚¹çº§å½’ä¸€åŒ–å™¨...")
        
        unique_stations = data[station_col].unique()
        logger.info(f"å‘ç° {len(unique_stations)} ä¸ªç‹¬ç‰¹ç«™ç‚¹")
        
        for station in unique_stations:
            station_data = data[data[station_col] == station]
            
            if len(station_data) < self.config.station_min_samples:
                logger.warning(f"ç«™ç‚¹ {station} æ ·æœ¬æ•° ({len(station_data)}) å°‘äºæœ€å°è¦æ±‚ ({self.config.station_min_samples})")
                continue
            
            # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºæ ‡å‡†åŒ–å™¨
            station_scalers = {}
            
            # ç‰¹å¾æ ‡å‡†åŒ–å™¨
            if self.config.use_robust_scaler:
                feature_scaler = RobustScaler()
                target_scaler = RobustScaler()
            else:
                feature_scaler = StandardScaler()
                target_scaler = StandardScaler()
            
            # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
            feature_scaler.fit(station_data[feature_cols])
            target_scaler.fit(station_data[[target_col]])
            
            station_scalers['feature_scaler'] = feature_scaler
            station_scalers['target_scaler'] = target_scaler
            
            self.station_scalers[station] = station_scalers
        
        self.fitted = True
        logger.info(f"âœ… ç«™ç‚¹çº§å½’ä¸€åŒ–å™¨æ‹Ÿåˆå®Œæˆï¼Œè¦†ç›– {len(self.station_scalers)} ä¸ªç«™ç‚¹")
        return self
    
    def transform(self, data: pd.DataFrame, feature_cols: List[str], 
                  target_col: str, station_col: str = 'station_id') -> pd.DataFrame:
        """
        åº”ç”¨ç«™ç‚¹çº§å½’ä¸€åŒ–
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        data = data.copy()
        
        for station in data[station_col].unique():
            if station not in self.station_scalers:
                logger.warning(f"ç«™ç‚¹ {station} æ²¡æœ‰å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œè·³è¿‡å½’ä¸€åŒ–")
                continue
            
            station_mask = data[station_col] == station
            station_scalers = self.station_scalers[station]
            
            # å½’ä¸€åŒ–ç‰¹å¾
            data.loc[station_mask, feature_cols] = station_scalers['feature_scaler'].transform(
                data.loc[station_mask, feature_cols]
            )
            
            # å½’ä¸€åŒ–ç›®æ ‡å˜é‡
            if target_col in data.columns:
                data.loc[station_mask, [target_col]] = station_scalers['target_scaler'].transform(
                    data.loc[station_mask, [target_col]]
                )
        
        return data
    
    def inverse_transform(self, data: pd.DataFrame, target_col: str, 
                         station_col: str = 'station_id') -> pd.DataFrame:
        """
        åå½’ä¸€åŒ–ç›®æ ‡å˜é‡
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        data = data.copy()
        
        for station in data[station_col].unique():
            if station not in self.station_scalers:
                logger.warning(f"ç«™ç‚¹ {station} æ²¡æœ‰å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œè·³è¿‡åå½’ä¸€åŒ–")
                continue
            
            station_mask = data[station_col] == station
            target_scaler = self.station_scalers[station]['target_scaler']
            
            # åå½’ä¸€åŒ–ç›®æ ‡å˜é‡
            data.loc[station_mask, [target_col]] = target_scaler.inverse_transform(
                data.loc[station_mask, [target_col]]
            )
        
        return data


class AdvancedNormalizer:
    """é«˜çº§å½’ä¸€åŒ–å™¨ - ç»Ÿä¸€æ¥å£"""
    
    def __init__(self, config: NormalizationConfig):
        self.config = config
        self.normalizer = None
        self.global_scaler = None  # å¤‡ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨
        self._initialize_normalizer()
    
    def _initialize_normalizer(self):
        """åˆå§‹åŒ–å…·ä½“çš„å½’ä¸€åŒ–å™¨"""
        if self.config.strategy == "time_window":
            self.normalizer = TimeWindowNormalizer(self.config)
        elif self.config.strategy == "station_wise":
            self.normalizer = StationWiseNormalizer(self.config)
        elif self.config.strategy == "robust":
            self.global_scaler = RobustScaler()
        elif self.config.strategy == "none":
            self.normalizer = None
        else:
            logger.warning(f"æœªçŸ¥çš„å½’ä¸€åŒ–ç­–ç•¥: {self.config.strategy}ï¼Œä½¿ç”¨æ ‡å‡†å½’ä¸€åŒ–")
            self.global_scaler = StandardScaler()
    
    def fit_transform(self, train_data: pd.DataFrame, feature_cols: List[str], 
                      target_col: str, **kwargs) -> pd.DataFrame:
        """
        æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒæ•°æ®
        """
        if self.config.strategy == "none":
            logger.info("ğŸš« è·³è¿‡å½’ä¸€åŒ–")
            return train_data.copy()
        
        if self.normalizer is not None:
            # ä½¿ç”¨é«˜çº§å½’ä¸€åŒ–å™¨
            self.normalizer.fit(train_data, feature_cols, target_col, **kwargs)
            return self.normalizer.transform(train_data, feature_cols, target_col, **kwargs)
        
        elif self.global_scaler is not None:
            # ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨
            data = train_data.copy()
            
            # ç‰¹å¾å½’ä¸€åŒ–
            data[feature_cols] = self.global_scaler.fit_transform(data[feature_cols])
            
            # ç›®æ ‡å˜é‡å½’ä¸€åŒ–
            if hasattr(self, 'target_scaler'):
                self.target_scaler = StandardScaler() if not self.config.use_robust_scaler else RobustScaler()
            else:
                self.target_scaler = StandardScaler() if not self.config.use_robust_scaler else RobustScaler()
            
            data[[target_col]] = self.target_scaler.fit_transform(data[[target_col]])
            
            return data
        
        return train_data.copy()
    
    def transform(self, data: pd.DataFrame, feature_cols: List[str], 
                  target_col: str, **kwargs) -> pd.DataFrame:
        """
        è½¬æ¢éªŒè¯/æµ‹è¯•æ•°æ®
        """
        if self.config.strategy == "none":
            return data.copy()
        
        if self.normalizer is not None:
            return self.normalizer.transform(data, feature_cols, target_col, **kwargs)
        
        elif self.global_scaler is not None:
            data = data.copy()
            
            # ç‰¹å¾å½’ä¸€åŒ–
            data[feature_cols] = self.global_scaler.transform(data[feature_cols])
            
            # ç›®æ ‡å˜é‡å½’ä¸€åŒ–
            if target_col in data.columns and hasattr(self, 'target_scaler'):
                data[[target_col]] = self.target_scaler.transform(data[[target_col]])
            
            return data
        
        return data.copy()
    
    def inverse_transform_targets(self, data: pd.DataFrame, target_col: str, 
                                  **kwargs) -> pd.DataFrame:
        """
        åå½’ä¸€åŒ–ç›®æ ‡å˜é‡
        """
        if self.config.strategy == "none":
            return data.copy()
        
        if self.normalizer is not None and hasattr(self.normalizer, 'inverse_transform'):
            return self.normalizer.inverse_transform(data, target_col, **kwargs)
        
        elif hasattr(self, 'target_scaler') and self.target_scaler is not None:
            data = data.copy()
            data[[target_col]] = self.target_scaler.inverse_transform(data[[target_col]])
            return data
        
        return data.copy()


def create_gradient_stable_normalizer(strategy: str = "station_wise") -> AdvancedNormalizer:
    """
    åˆ›å»ºæ¢¯åº¦ç¨³å®šçš„å½’ä¸€åŒ–å™¨ - é’ˆå¯¹å¾„æµæ•°æ®ä¼˜åŒ–

    Args:
        strategy: å½’ä¸€åŒ–ç­–ç•¥ï¼Œæ¨è "station_wise" æˆ– "time_window"

    Returns:
        é…ç½®å¥½çš„é«˜çº§å½’ä¸€åŒ–å™¨
    """
    config = NormalizationConfig(
        strategy=strategy,
        use_robust_scaler=True,  # ä½¿ç”¨é²æ£’æ ‡å‡†åŒ–ï¼Œå‡å°‘å¼‚å¸¸å€¼å½±å“
        outlier_threshold=3.0,   # æ”¾å®½å¼‚å¸¸å€¼é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šæå€¼ä¿¡æ¯
        window_size=730,         # 2å¹´çª—å£ï¼Œæ•æ‰å­£èŠ‚æ€§
        station_min_samples=300, # è¿›ä¸€æ­¥é™ä½ç«™ç‚¹æœ€å°æ ·æœ¬è¦æ±‚
        feature_specific={
            'precip': {'log_transform': True, 'add_constant': 0.1},
            'temp': {'log_transform': False, 'seasonal_adjust': False},
            'pet': {'log_transform': True, 'add_constant': 0.01},
            'runoff': {'log_transform': True, 'add_constant': 0.01}  # å¢åŠ å¸¸æ•°ï¼Œæ›´å¥½å¤„ç†å°å¾„æµå€¼
        }
    )

    return AdvancedNormalizer(config)


class HydroLogNormalizer:
    """
    ä¸“é—¨é’ˆå¯¹å¾„æµæ•°æ®çš„å¯¹æ•°æ­£æ€åˆ†å¸ƒå½’ä¸€åŒ–å™¨
    """

    def __init__(self, add_constant: float = 0.1, use_robust: bool = True):
        """
        åˆå§‹åŒ–å¾„æµä¸“ç”¨å½’ä¸€åŒ–å™¨ - æ•°å€¼ç¨³å®šç‰ˆæœ¬

        Args:
            add_constant: å¯¹æ•°å˜æ¢å‰æ·»åŠ çš„å¸¸æ•°ï¼Œé¿å…log(0)ï¼Œå¢å¤§ä»¥æé«˜ç¨³å®šæ€§
            use_robust: æ˜¯å¦ä½¿ç”¨é²æ£’æ ‡å‡†åŒ–ï¼ˆä¸­ä½æ•°+MADï¼‰
        """
        self.add_constant = add_constant  # å¢å¤§å¸¸æ•°ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
        self.use_robust = use_robust
        self.fitted = False

        # å­˜å‚¨ç»Ÿè®¡å‚æ•°
        self.log_mean = None
        self.log_std = None
        self.log_median = None
        self.log_mad = None  # Median Absolute Deviation

    def fit(self, runoff_data: np.ndarray) -> 'HydroLogNormalizer':
        """
        æ‹Ÿåˆå½’ä¸€åŒ–å‚æ•°

        Args:
            runoff_data: å¾„æµæ•°æ®ï¼Œå½¢çŠ¶ä¸º (n_samples,)
        """
        # ç¡®ä¿æ•°æ®ä¸ºæ­£å€¼
        runoff_data = np.maximum(runoff_data, 0.0)

        # å¯¹æ•°å˜æ¢
        log_data = np.log1p(runoff_data + self.add_constant)

        if self.use_robust:
            # ä½¿ç”¨é²æ£’ç»Ÿè®¡é‡
            self.log_median = np.median(log_data)
            self.log_mad = np.median(np.abs(log_data - self.log_median))
            # é¿å…MADä¸º0çš„æƒ…å†µ
            if self.log_mad < 1e-8:
                self.log_mad = np.std(log_data)
        else:
            # ä½¿ç”¨æ ‡å‡†ç»Ÿè®¡é‡
            self.log_mean = np.mean(log_data)
            self.log_std = np.std(log_data)
            # é¿å…æ ‡å‡†å·®ä¸º0çš„æƒ…å†µ
            if self.log_std < 1e-8:
                self.log_std = 1.0

        self.fitted = True
        return self

    def transform(self, runoff_data: np.ndarray) -> np.ndarray:
        """
        åº”ç”¨å½’ä¸€åŒ–å˜æ¢

        Args:
            runoff_data: å¾„æµæ•°æ®

        Returns:
            å½’ä¸€åŒ–åçš„æ•°æ®
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")

        # ç¡®ä¿æ•°æ®ä¸ºæ­£å€¼
        runoff_data = np.maximum(runoff_data, 0.0)

        # å¯¹æ•°å˜æ¢
        log_data = np.log1p(runoff_data + self.add_constant)

        if self.use_robust:
            # é²æ£’æ ‡å‡†åŒ–
            normalized = (log_data - self.log_median) / self.log_mad
        else:
            # æ ‡å‡†æ ‡å‡†åŒ–
            normalized = (log_data - self.log_mean) / self.log_std

        return normalized

    def inverse_transform(self, normalized_data: np.ndarray) -> np.ndarray:
        """
        åå½’ä¸€åŒ–å˜æ¢

        Args:
            normalized_data: å½’ä¸€åŒ–åçš„æ•°æ®

        Returns:
            åŸå§‹å°ºåº¦çš„å¾„æµæ•°æ®
        """
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")

        if self.use_robust:
            # åé²æ£’æ ‡å‡†åŒ–
            log_data = normalized_data * self.log_mad + self.log_median
        else:
            # åæ ‡å‡†æ ‡å‡†åŒ–
            log_data = normalized_data * self.log_std + self.log_mean

        # åå¯¹æ•°å˜æ¢
        runoff_data = np.expm1(log_data) - self.add_constant

        # ç¡®ä¿ç»“æœä¸ºéè´Ÿ
        return np.maximum(runoff_data, 0.0)

    def fit_transform(self, runoff_data: np.ndarray) -> np.ndarray:
        """
        æ‹Ÿåˆå¹¶å˜æ¢æ•°æ®
        """
        return self.fit(runoff_data).transform(runoff_data)


# æ¢¯åº¦ç¨³å®šå·¥å…·å‡½æ•°
def apply_gradient_clipping(model: nn.Module, max_norm: float = 1.0) -> float:
    """
    åº”ç”¨æ¢¯åº¦è£å‰ª
    
    Args:
        model: PyTorchæ¨¡å‹
        max_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°
    
    Returns:
        è£å‰ªå‰çš„æ¢¯åº¦èŒƒæ•°
    """
    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
    return grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm


def check_gradient_health(model: nn.Module) -> Dict[str, float]:
    """
    æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶å†µ
    
    Args:
        model: PyTorchæ¨¡å‹
    
    Returns:
        æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯
    """
    total_norm = 0
    param_count = 0
    max_grad = 0
    min_grad = float('inf')
    
    for param in model.parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += param.numel()
            
            max_grad = max(max_grad, param.grad.data.abs().max().item())
            min_grad = min(min_grad, param.grad.data.abs().min().item())
    
    total_norm = total_norm ** (1. / 2)
    
    return {
        'total_norm': total_norm,
        'average_norm': total_norm / max(param_count, 1),
        'max_gradient': max_grad,
        'min_gradient': min_grad if min_grad != float('inf') else 0,
        'param_count': param_count
    }